[{"title":"【策略模式】最佳实践——Spring IoC实现策略模式全流程深度解析","date":"2024-11-17T01:21:16.000Z","url":"/2024/11/17/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["策略模式","/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"简介 策略模式是一种行为型设计模式，它定义了一系列算法，并将每一个算法封装起来，使它们可以互相替换，并且使算法的变化不会影响使用算法的客户端。策略模式通过将具体的业务逻辑从上下文（Context）中剥离出来，独立为策略类，动态地将所需的行为注入上下文对象中，从而避免代码中充斥条件判断逻辑。 策略模式的核心由以下三部分组成： 策略接口（Strategy Interface）：定义所有具体策略类的公共接口，用于约束具体策略的实现。 具体策略类（Concrete Strategy）：实现策略接口，在内部封装具体的算法和业务逻辑。 上下文类（Context）：持有一个具体策略对象的引用，用来调用具体策略类的方法。也可以使用工厂模式，将创建具体策略类的动作交给工厂类执行，实现业务逻辑、创建逻辑与应用程序的解耦合。 在下面的文章中，我将给策略模式的实现分为三步，从业务逻辑解耦、创建逻辑解耦到满足开闭原则，一步步实现策略模式的最佳实践。 1.策略模式（业务逻辑解耦） 设计一个支付系统，支持多种支付方式，包括支付宝（Alipay）、微信支付（WeChatPay）和银行卡支付（BankCardPay）。不同支付方式的逻辑独立，同时系统可以根据需求动态切换支付方式。 定义策略接口 定义具体策略类 不同的具体策略类都统一继承策略接口，在类中重写策略接口定义的方法，编写具体的业务逻辑。 定义上下文类 在上下文中定义策略类的引用，使用 set 方法注入具体策略类，通过方法调用执行具体策略类的业务逻辑。 使用上下文类 在用户选用具体的支付方式时，需要给上下文 context 传入具体策略的对象，才可使用该策略的支付方法。（这种方式需要应用程序自己手动 new 出具体策略的对象，并且通过 set 方法传入给上下文类） 虽然通过这种方式，可以使得具体的支付业务逻辑与应用程序解耦合。但是依然存在两个问题： 具体策略类的创建逻辑仍然由应用程序执行，即策略类创建逻辑仍然与应用程序耦合。 如果需要添加新的策略，就需要再添加一个新的 if-else 分支，因此不符合开闭原则。（开闭原则通俗点说就是在添加新的功能时，不能修改现有的代码，这里添加一个新的分支就属于修改了现有的代码） 执行结果如下： 开闭原则： 软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。 对扩展开放（Open for extension）：软件实体应该允许在不改变其现有代码的情况下，通过增加新功能来对其进行扩展。也就是说，当软件的需求发生变化时，我们应该能够通过添加新代码来满足这些需求，而不需要修改已有的代码。 对修改关闭（Closed for modification）：一旦软件实体被开发完成并投入使用，其源代码就不应该再被修改。这可以防止对现有功能的破坏，减少引入新的错误的风险，并使软件更加稳定和可维护。 2.引入工厂模式（创建逻辑解耦） 通过引入工厂模式，可以将创建具体策略类的动作转移给工厂类执行，将策略类的创建逻辑与应用程序解耦合。 定义工厂类 由工厂类负责策略类的创建逻辑，根据传入的不同参数，创建不同的具体策略类。这一步骤也就实现了策略类的创建逻辑与应用程序的解耦。 改造工厂类 可以发现，如果需要使用大量的具体策略类的话，应用程序就需要编写大量的 if-else、switch 分支。我们可以使用 Map 存储具体策略类，从而取消分支判断的设计。 通过上述的改造，虽然解决了创建逻辑与应用程序耦合的问题，但是不符合开闭原则的问题仍然没有解决（如果需要添加新的具体策略类，还是需要修改工厂类的代码，在 Map 集合中 put 一个新的键值对）。 比如，如果要添加一个苹果支付的业务逻辑，就需要在 static 中添加这个代码，从而打破了开闭原则。 下文我们将使用 Spring IoC 解决这个问题。 使用工厂类 直接使用 PaymentStrategyFactory 工厂类的静态方法 getStrategy() 获取策略对象，调用策略对象的方法即可。 执行结果如下： 3.引入Spring IoC（满足开闭原则） 改造策略接口 需要在策略接口 PaymentStrategy 添加一个新的 mark 方法，用于标识每个接口。 改造具体策略类 具体策略类由于实现了策略接口，因此需要重写标识方法，这里直接返回对应策略的标识即可。（这一步骤很重要，应用程序通过工厂类获取策略类，就是通过这个标识获取的） 此外，我们还需要给每个具体策略类添加 @Component 注解，方便 Spring 容器管理具体策略类的生命周期。 通过 @Component 注解，实际上是通过 Spring 容器来执行 new 的步骤，也就是将具体策略类的创建逻辑由工厂类交给了 Spring 容器。 @Component：可以标记任意类为 Spring Bean，Spring 容器会自动扫描和管理使用 @Component 注解标注的类。 支付宝支付策略： 微信支付策略： 银行卡支付策略： 改造工厂类 下面开始重头戏，我们将对工厂类进行改造，通过 Spring 容器帮我们管理策略类。 添加注解：首先需要给工厂类添加 @Component 注解，后续我们将通过依赖注入的方式使用该工厂类。 实现 InitializingBean 接口：通过重写该接口的 afterPropertiesSet 方法，可以在 Bean 的初始化阶段将 Spring 容器创建好的具体策略类写入到工厂类的 strategyMap 集合中。 Spring Bean 的生命周期：整体上可以简单分为四步：实例化 —&gt; 属性赋值 —&gt; 初始化 —&gt; 销毁。 注入 IoC 容器：使用 @Autowired 注解注入 ApplicationContext，即 IoC 容器。方便后续从容器中获取 Bean 名称和策略接口实现类的 Map 集合。 定义 Map 集合：这个 Map 是工厂类存放策略类的集合，应用程序将从这个集合中获取对应的策略类。 编写 afterPropertiesSet 方法逻辑：这里我们需要将 Bean 名称和策略类的键值对转化为标识和策略类的键值对，方便我们根据传入的标识直接获取策略类。 第一步先从注入的 Spring 容器中获取容器创建好策略类的 Map 集合，该集合存储的是 Bean 名称和策略类的键值对，如下图： 第二步使用了 lambda 表达式（如果不熟练的话使用普通 for 循环也行），将Spring容器中的策略类转移到工厂类的Map集合中，其实这一步也就是将 key 值简化，方便根据标识字符串获取策略类。如下图： 编写 getStrategy 方法逻辑：最后一步，可以从 strategyMap 集合中获取对应的策略类了。 工厂类到这里就彻底改造完毕了，我们通过将每个具体策略类都添加了 @Component 注解，在 Spring 启动时就会将这些使用了 @Component 注解的类创建出来并添加到 Spring IoC 容器中。这样，在添加新的策略类时就无需修改原有的工厂类，满足了开闭原则。 使用工厂类 执行结果如下： 总结 最佳实践小结 通过上文的三个步骤，我们一步步将策略模式改造为了最佳实践，实现了业务逻辑、创建逻辑与应用程序的解耦以及满足了开闭原则。在Spring IoC 的策略模式实现中，主要做了这三件事情： 业务逻辑与应用程序解耦：通过定义策略接口，规范不同具体策略类的统一实现，将算法的业务逻辑交给了策略类实现。 创建逻辑与应用程序解耦：通过给策略类添加 @Component 注解，由 Spring 扫描并注册具体策略类的对象到 Spring 容器中，从而将策略类的创建逻辑转移到了 Spring 容器。 满足开闭原则： 由于添加了 @Component 注解的策略类在 Spring 启动后都被注册到了 Spring 容器中，无需开发者手动硬编码到工厂类。在需要添加新的策略类时，Spring 容器可以帮助开发者自动注入。 对比其他模式 策略模式 定义：定义一系列算法，将每种算法封装在独立的策略类中，使得这些算法可以互相替换，且算法的变化不会影响使用算法的上下文对象。 核心思想：动态选择算法，运行时可以替换策略对象。 模板方法模式 定义：定义一个算法的框架，将算法的某些步骤延迟到子类中实现，使得子类可以在不改变算法结构的情况下重新定义算法的某些步骤。 核心思想：固定流程，子类负责填充或改写部分步骤。 工厂模式 定义：定义一个用于创建对象的接口，让子类决定实例化哪个具体类。工厂模式将对象的创建过程与具体类的逻辑解耦。 核心思想：对象的创建，由工厂负责生产特定对象。 "},{"title":"【分布式】万字图文解析——深入七大分布式事务解决方案","date":"2024-11-14T02:46:02.000Z","url":"/2024/11/14/%E3%80%90%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%91%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","tags":[["TCC","/tags/TCC/"],["Saga","/tags/Saga/"],["2PC/3PC","/tags/2PC-3PC/"],["MQ事务消息","/tags/MQ%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF/"],["本地消息表","/tags/%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8/"],["最大努力通知","/tags/%E6%9C%80%E5%A4%A7%E5%8A%AA%E5%8A%9B%E9%80%9A%E7%9F%A5/"],["分布式事务","/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"]],"categories":[["分布式","/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"]],"content":"分布式事务 分布式事务是指跨多个独立服务或系统的事务管理，以确保这些服务中的数据变更要么全部成功，要么全部回滚，从而保证数据的一致性。在微服务架构和分布式系统中，由于业务逻辑往往会跨多个服务，传统的单体事务无法覆盖，因此需要通过分布式事务来保障一致性。 三种数据一致性模型： 强一致性：系统保证每个读操作都将返回最近的写操作的结果，即任何时间点，客户端都将看到相同的数据视图。 弱一致性：放宽了一致性保证，允许在不同节点之间的数据访问之间存在一定程度的不一致性，以换取更高的性能和可用性。 最终一致性：允许在系统发生分区或网络故障后，经过一段时间，系统将最终达到一致状态。 刚性事务 刚性事务（Rigid Transaction）也称为强一致性事务，遵循严格的ACID特性（原子性、一致性、隔离性、持久性），确保所有操作都严格按照要求执行，要么全部成功，要么全部回滚。刚性事务常用于金融系统、银行转账等场景，要求数据高度一致，不能有任何误差。 在分布式系统中，实现刚性事务通常采用2PC或3PC协议，但这些协议会带来较高的性能开销，同时存在同步阻塞等问题。因此，刚性事务适合对数据一致性要求极高、但允许较高事务延迟的场景。 2PC或3PC协议根据 XA 规范衍生而来，XA 规范将分布式事务处理模型中涉及到了角色主要分为了应用程序（AP）、事务管理器（TM）和资源管理器（RM） AP(Application Program)：应用程序本身。 RM(Resource Manager)：资源管理器，也就是事务的参与者，绝大部分情况下就是指数据库，一个分布式事务往往涉及到多个资源管理器。 TM(Transaction Manager)：事务管理器，负责管理全局事务，分配事务唯一标识，监控事务的执行进度，并负责事务的提交、回滚、失败恢复等。 两阶段提交 2PC（Two-Phase Commit Protocol，两阶段提交协议）是分布式系统中广泛使用的一种协议，用于保证事务跨多个节点（参与者）一致地提交或回滚。 两阶段提交协议分为两个阶段：准备阶段和提交阶段。 准备阶段 事务管理器（TM）向所有事务参与者（资源管理器，RM）发送准备提交（prepare）的请求。 每个事务参与者在收到请求后执行以下操作： 执行本地数据库事务预操作，比如写 redo log / undo log 日志，保留修改（如将数据写入暂存区域或进行日志记录），但不提交。 若该操作能够成功完成并保证后续提交，则返回同意消息。 若遇到错误或不能确保事务提交成功，则返回拒绝消息。 在准备阶段，事务参与者只是准备提交，尚未实际提交事务。这使得系统仍然处于一致性状态。 提交阶段 如果所有事务参与者都返回了同意消息，事务管理器向所有参与者发送提交（Commit）请求。 各参与者收到提交消息后，正式提交事务操作，释放锁，清除临时数据，并通知事务管理器提交成功。 如果任何一个事务参与者返回拒绝消息，事务管理器向所有参与者发送回滚（Rollback）请求。 各参与者收到回滚消息后，撤销预操作，将状态恢复到事务开始之前。 在提交阶段，如果某个参与者出现故障，事务管理器会根据超时或故障检测机制来决定事务状态，尽量使所有节点最终保持一致。 存在的问题 同步阻塞：在事务提交的过程中，如果事务管理者出现故障，所有的事务参与者会进入阻塞状态，等待事务管理者的进一步指令，导致整个系统的事务处理停滞，不能继续执行其他事务。 单点故障：事务管理器是单点故障，如果事务管理器在提交阶段崩溃，系统将无法知道事务的最终状态，容易导致数据不一致。 数据不一致风险：在提交阶段，若事务管理器在发送提交或回滚请求后崩溃，可能导致部分参与者收到提交请求，而部分未收到，从而造成数据不一致。 三阶段提交 3PC（Three-Phase Commit Protocol，三阶段提交协议）是在两阶段提交协议的基础上进一步改进的分布式事务一致性协议，通过增加预提交阶段，增强了系统的容错能力和处理网络分区的能力，以解决2PC的同步阻塞问题和单点故障风险。 准备阶段 事务管理器（TM）向所有事务参与者（资源管理器，RM）发送「准备请求」，询问它们是否能够完成事务提交操作。 每个事务参与者收到「准备请求」后，如果认为自己可以完成事务，则返回同意消息；如果认为无法完成，则返回拒绝消息。 事务管理器收集所有参与者的响应： 若所有事务参与者都返回同意消息，则进入预提交阶段。 若任何一个事务参与者返回拒绝消息，则进入终止阶段并通知所有参与者回滚事务。 预提交阶段 如果所有事务参与者在第一阶段都返回了同意消息，事务管理器向所有参与者发送「预提交请求」。 每个事务参与者收到「预提交请求」后，执行本地数据库事务预操作，记录日志，保存即将提交的事务信息，以确保即使事务管理器故障也能恢复状态。如果事务参与者成功执行了事务预操作，发送预提交成功给事务管理器；如果执行失败，返回预提交失败给事务管理器。 预提交阶段通过提前记录事务信息，保证即使事务管理器故障，事务参与者也能够通过日志记录自我恢复。 事务管理器在等待一段时间后收集所有参与者的响应： 若所有参与者都返回预提交成功，事务管理器进入提交阶段。 若任意一个参与者返回预提交失败，则事务管理器进入终止阶段并通知所有参与者回滚事务。 提交阶段 若事务管理器在预提交阶段收到所有参与者的预提交成功，则向所有参与者发送「事务提交请求」。 各参与者收到提交请求后，正式提交事务操作，释放锁，清除临时数据，并返回提交完成。 若事务管理器在预提交阶段未收到所有参与者的响应，或检测到参与者或事务管理器自身出现故障，则事务管理器向所有参与者发送「事务回滚请求」。 各参与者收到回滚请求后，撤销事务，并返回回滚完成。 优缺点 虽然 3PC 通过引入超时机制和预提交阶段一定程度上解决了 2PC 的同步阻塞和单点故障的问题，但是依然存在数据不一致的问题，同时也引入了一些新问题比如性能糟糕。 解决同步阻塞：在3PC中，通过预提交阶段参与者在实际提交前达成共识状态，具备了超时控制和独立决策的条件。即使协调者故障，参与者也能在超时后依据自身状态决定提交或回滚，从而有效避免了因协调者故障导致的长时间同步阻塞问题。 解决单点故障：在3PC中，事务参与者引入了超时机制，当事务管理者挂掉后，事务参与者在等待超时后可以根据自身的状态来进行决策（如提交或回滚），不必依赖事务管理者的指令，从而降低了事务管理者的单点故障风险。 存在的问题： 网络分区风险：3PC试图通过在预提交阶段前引入准备阶段来减少阻塞问题，但如果在该阶段发生网络分区，事务管理器和参与者之间的通信可能会断开。此时，部分参与者可能会继续提交事务，而其他未收到指令的参与者则可能回滚，从而导致数据不一致。 性能降低：3PC虽然解决了一些2PC的阻塞问题，但增加的确认阶段和处理步骤使得事务处理的整体延迟变高。在高并发环境下，系统性能和响应速度可能会受到影响，导致事务处理效率较低。 数据不一致风险：虽然3PC引入了超时机制，但当事务管理器在预提交阶段（准备提交）崩溃且没有及时恢复时，参与者可能会依据超时策略决定提交或回滚，导致不同参与者采取了不同的事务状态，从而产生数据不一致。 柔性事务 柔性事务（Flexible Transaction）也称为最终一致性事务，放宽了对事务的严格性要求，允许系统在短期内出现不一致，只要在一段时间后达成最终一致性即可。柔性事务通常不完全遵循ACID特性，而采用BASE理论（基本可用、软状态、最终一致），在保证系统可用性的同时降低了对一致性的强制要求。 柔性事务广泛应用于电商订单处理、社交应用等对实时性和高可用性要求高，但数据一致性可延迟的场景。 柔性事务常见的实现方式包括： TCC（Try-Confirm-Cancel）：将事务分解为三个步骤，先尝试（Try），再确认（Confirm）或取消（Cancel）。 可靠消息事务：利用消息队列确保最终一致性，通过消息重试机制来保证事务操作的最终完成。 补偿事务：通过补偿机制在出现错误时对事务进行回滚。 TCC TCC（Try-Confirm-Cancel）是一种分布式事务解决方案，特别适用于柔性事务场景。TCC通过将事务分解为三个步骤来确保最终一致性，提供对事务操作的更加灵活的控制。 执行流程 Try（尝试）阶段：事务参与者尝试执行本地事务，并对全局事务预留业务资源。如果 try 阶段执行成功，参与者会返回一个成功标识，否则会返回一个失败标识。 Confirm（确认）阶段：如果所有参与者的 try 阶段都执行成功，则协调者通知所有参与者提交事务，执行 confirm 阶段。这时参与者将在本地提交事务，并处理全局事务预留的资源。 Cancel（取消）阶段：如果任意参与者在 try 阶段或 Confirm 阶段执行失败，则协调者通知所有参与者回滚事务，执行 cancel 阶段。这时参与者将释放预留的业务资源。 如果Confirm或者Cancel执行失败，可以采用重试的方式减少因为网络问题导致TM没有接收到消息的情况，此外，还可以采用异常监控、日志记录和人工干预的方式来保证事务的一致性。 对于Confirm执行失败，还可以采用执行Cancel操作的方式，撤销在Try阶段预留的事务资源。 典型示例 假设我们有一个转账服务，需要从 A 账户转账 100 元到 B 账户，同时从 A 账户转账 200 元到 C 账户，总计从 A 账户中扣减 300 元。整个事务流程分为 Try、Confirm 和 Cancel 三个阶段： Try 阶段：转账服务首先冻结 A 账户的 300 元，以确保后续转账的可用性和一致性。 Confirm 阶段：如果所有 Try 操作都成功执行，转账服务进入 Confirm 阶段，实际执行转账操作。此时，A 账户的冻结金额解冻，系统将 100 元转入 B 账户，200 元转入 C 账户，完成资金划转。 Cancel 阶段：如果在 Try 阶段某一步失败，则系统进入 Cancel 阶段，对 A 账户的冻结金额进行解冻处理，保证资金不受影响；如果 Confirm 阶段中某步转账失败，例如 A 到 B 的转账失败，而 A 到 C 的转账已完成，则需要通过 C 到 A 的逆向转账操作将金额退回，保证 A 账户资金最终一致。 通过这种方式，TCC 模式确保了分布式事务的最终一致性，即使在部分失败的情况下，系统也能通过回滚或补偿机制恢复到一致状态。 存在的问题 空回滚：在TCC协议中，如果某些参与者在Try阶段成功，而其他参与者失败，此时所有参与者需要执行Cancel操作。对于那些在Try阶段没有成功的参与者来说，执行Cancel操作就成为一次空回滚。如果业务没有有效地识别和处理空回滚，可能会导致异常错误，甚至可能导致Cancel一直失败，最终使整个分布式事务失败。 悬挂事务：在TCC执行一阶段的Try操作阶段时，可能出现网络拥堵导致超时，事务协调器会触发二阶段的Cancel操作。而下游的节点由于网络延迟先接收到了Cancel，网络恢复后会在Cancel后再次接收到Try请求，这就导致该节点Try操作占用的资源无法释放，造成事务悬挂。 解决方案 通过引入分布式事务记录表，可以解决上述两个问题。该表中两个关键字段，一个是 tx_id 字段用于保存本次处理的事务ID，另一个是 state 字段用于记录本次事务的执行状态。 当后续在进行Try、Confirm和Cancel操作时，都需要在本地事务中创建或修改这条记录。 解决空回滚：当一个事务参与者接受到Cancel请求时，先去事务记录表中查询是否存在当前事务 trx_id 的Try操作的记录，如果记录不存在，则说明Try操作并未成功，此时可以直接跳过Cancel，避免执行空回滚。 解决悬挂事务：当一个事务参与者接收到Try请求时，先去事务记录表中查询是否存在当前事务 trx_id 的Cancel操作的记录，如果存在，说明执行Try操作会导致事务悬挂，无法释放资源，拒绝本次Try请求。 MQ事务消息 RocketMQ 事务消息是一种可靠的消息传输机制，用于保证分布式事务的最终一致性。事务消息允许将消息的发送与本地事务绑定在一起，使得即使在网络异常或系统故障情况下，消息也能最终达到一致状态。 执行流程 下图为MQ事务消息的总体执行流程： RocketMQ 的事务消息通过 TransactionListener 接口实现，其执行流程总体上可以分为四个步骤： 发送半消息：消息发送方给 MQ Broker 发送一条 Half 消息，即半消息。半消息会存储在 Broker 的事务消息日志中，且该半消息暂时无法被消费。 执行本地事务：半消息发送成功后，Broker 会通知消息发送方执行本地事务，消息发送方可以根据本地事务的执行情况，判断是否需要提交该事务消息。 发送提交或回滚消息：如果本地事务执行成功，发送方会通知 Broker 提交该事务消息，使得消息被消费者消费；如果本地事务执行失败，发送方会通知 Broker 回滚该事务消息，并将该半消息从事务消息日志中删除，不会被消费者消费。 回查事务状态：如果 Broker 在规定时间内没有收到 COMMIT 和 ROLLBACK 消息，会向消息发送方发送一个回查请求，根据请求的回调结果判断是否需要提交或回滚事务消息。 相关问题 半消息发送失败了，如何处理？ 在 RocketMQ 事务消息的一致性解决方案中，应用程序（即消息发送方）是先发送半消息，后执行的本地事务。因此如果半消息发送失败了，直接进行消息重发即可，不会造成数据不一致的问题。 为什么需要先发送半消息？ 主要原因是：本地事务执行完成后再发送消息，如果消息发送失败，那么 Broker 将无法感知本地事务的执行状态。 如果先执行完本地事务消息，再发送消息，当发送消息失败时，消费者没有半消息提供的本地事务的任何信息，因此 Broker 无法去回查本地事务的状态。 通过先发送半消息，Broker 可以在日志中记录该半消息，进而获知该事务的“初步状态”。如果半消息在超时内没有完成提交或回滚，Broker 就可以发起回查，以此来确保事务消息的最终一致性。因此，半消息为 Broker 提供了识别和查询特定事务状态的依据。 本地消息表 本地消息表（Local Message Table）同样也是一种分布式系统中常用的可靠消息传递方案。其核心思想是在发送消息的业务系统中，在消息发送方创建一个本地消息表，用于记录每一条需要发送的消息，并通过定时任务确保消息的可靠传递。 执行流程 一个典型的本地消息表结构如下： 字段 类型 说明 id VARCHAR(36) 消息ID，唯一标识 content TEXT 消息内容 status INT 消息状态（0: 待发送，1: 已发送，2: 已完成） retry_count INT 重试次数 create_time TIMESTAMP 创建时间 update_time TIMESTAMP 最后更新时间 该方案主要将分布式事务拆分成了本地事务和消息事务两个部分，具体执行流程如下： 执行本地事务：首先由消息发送方开启本地事务，在本地事务中执行具体的业务操作。业务操作执行成功后，发送方将需要通知到其他服务的消息记录在本地消息表中，并设置消息状态为「待发送」。 定时任务发送消息：在本地消息表中，消息初始状态为「待发送」。业务系统通过定时任务周期性地扫描本地消息表，找到所有「待发送」状态的消息，将其发送到消息队列，并将状态设置为「已发送」。定时任务的时间间隔可以根据业务需求设置。 保证消息一致性：消息发送到消息队列后，可以通过消息重试机制，防止因网络问题导致传递的消息丢失问题；消息传递到消费者后，可以通过消费者确认机制、消息幂等处理来保证消息的一致性。 返回消费结果：消费者消费完毕后，返回消费结果给生产者。生产者接收到消费者的消费确认后，根据消息的唯一ID，将本地消息表中对应消息的状态更新为「已完成」。 相关问题 如果步骤1和2失败，如何处理？ 当本地事务执行失败时，由于此阶段将业务操作和消息表写入放在了同一个事务中，即使步骤1和2执行失败，事务也可以通过回滚恢复数据，保证业务操作和消息表写入的一致性。 如果步骤3中的消息发送失败，如何处理？ 需要在消息发送方开启一个定时任务，不断的扫描本地消息表中状态为「待发送」的消息，对于未发送成功的消息重新投递。 如果步骤4和5失败，如何处理？ 依靠消费者的确认机制和重试机制，如果消息丢失或者处理失败，则重新进行投递。此外，还可以借助死信队列进行额外处理。 如果步骤6和7失败，即本地消息表更新失败，如何处理？ 执行到这里时，已经可以保证消息发送方和消息接收方的业务数据保持一致了，但是本地消息表的数据还未更新。消息接收方可以设置定时任务，定期扫描本地消息表中的消息，如果消息为「已发送」而不是「已完成」，可以通过重试来再次尝试更新本地消息的状态。此外，消息发送方可以设置定时任务，主动查询消费者的业务状态，如果消息被正常消费，可以直接更新本地消息表。 Saga Saga 是一种分布式事务管理模式，主要用于解决微服务架构中跨多个服务的事务一致性问题。Saga模式通过将一个大事务分解成一系列小的局部事务，并通过补偿机制来保证最终一致性，从而避免了传统的分布式事务（如2PC或3PC）带来的性能瓶颈和复杂性。 协调式 编排式Saga是由一个中央协调者来控制整个Saga事务的执行流程。协调者负责决定每个局部事务的执行顺序，处理每个局部事务的成功或失败，并在必要时触发补偿操作。 编排式 编排式没有中央协调者，而是通过事件驱动的方式来协调各个局部事务的执行。每个服务通过发布事件来通知其他服务，其他服务根据事件决定是否执行后续操作或进行补偿。 对比 编排式Saga适用于事务流程较为清晰、需要集中控制的场景，尤其是在业务流程复杂且需要监控和调度时。它具有较强的控制能力，但可能会带来单点故障问题。 事件驱动式Saga适用于高可扩展性、去中心化的分布式系统，适合松耦合的微服务架构。它更加灵活，但在实现上需要处理更复杂的事件和状态管理。 特性 编排式（Orchestration） 事件驱动式（Choreography） 控制方式 中央协调者控制事务执行 各个服务通过事件进行事务协作 事务执行顺序 顺序执行，协调者决定后续事务的执行 各个服务根据事件决定执行顺序 补偿操作 由协调者决定是否进行补偿操作 由各服务根据事件决定补偿操作 扩展性 较差，协调者可能成为瓶颈 高，服务间解耦，易于扩展 耦合度 高，服务需要与协调者交互 低，服务间解耦，事件驱动 单点故障问题 是，协调者故障会影响整个事务 否，没有单点故障问题 监控与调试 相对简单，所有操作由协调者管理 较为复杂，需要事件追踪和状态管理 最大努力通知 在最大努力通知机制中，发送方会尽最大可能确保通知送达目标，通常通过重试机制提高通知的到达率。在此过程中，若出现网络通信故障或消息队列异常，就可能导致消息传递失败，即消息可能会丢失。因此，最大努力通知机制不能完全保证接收方一定能收到每条消息，但会尽最大努力确保消息通知的完成。 最大努力通知常见实现：生产者重试机制、消费者重试机制、死信队列、定时任务重试 最大努力通知适合在以下场景中使用： 日志记录：日志记录的发送可以采用最大努力通知的方式，保证不影响主业务流程。 非核心业务流程：例如积分更新、通知消息推送等，在用户体验上容错性较高的场景。 事务对比 TCC和2PC/3PC 适用场景： TCC：提供最终一致性，允许短时间内的数据不一致，通过补偿机制在失败后恢复状态。适合对响应速度要求高、允许一定延迟一致性的场景，如电商订单处理、支付等需要预留资源的业务。 2PC：提供强一致性，确保所有参与者最终的状态保持一致，要么全部成功提交，要么全部回滚。适合对一致性要求高的场景，如金融系统、银行转账等场景，要求数据高度一致，不能有任何误差。 执行流程： TCC：三阶段模型，主要包括Try（预留资源）、Confirm（提交）、Cancel（补偿/撤销），在Try或Confirm阶段执行失败可以进行Cancel补偿。 2PC：两阶段模型，主要包括Prepare（准备）和Commit（提交），无独立的回滚机制。 容错和恢复机制： TCC：每个阶段都有独立的失败处理，通过Cancel阶段的补偿操作可以回滚已预留的资源。 2PC：容错能力较弱，尤其在协调者宕机的情况下易发生资源锁死，恢复依赖协调者的重启和日志记录。 实现方式： TCC：是业务层面的分布式事务，不会一直持有资源的锁，性能较好。TCC 方案允许各个参与者自定义Try、Confirm和Cancel事务逻辑，对业务侵入性较高，但具有较好的灵活性和可恢复性。 2PC：是资源层面的分布式事务，需要数据库支持XA协议，在两阶段提交的整个过程中，会一直持有资源的锁，一般高并发性能会比较差。2PC 方案由协调者统一管理事务状态，能确保强一致性，但在分布式系统中较为耗时，且在网络中断或协调者失败时可能会出现阻塞问题。 特点 TCC 2PC 事务管理 由业务逻辑分阶段管理 由协调者统一管理 阶段划分 Try、Confirm、Cancel 准备阶段、提交阶段 执行方式 预留资源+确认/取消业务执行 准备阶段锁定资源+正式提交/回滚 一致性 最终一致性 强一致性 资源锁定时间 较短 较长，可能导致阻塞 容错机制 允许自定义重试和补偿 协调者故障会阻塞事务 性能 较高，灵活性好 较低，且有单点故障风险 MQ事务消息&amp;本地消息表&amp;最大努力通知 适用场景： MQ事务消息：适用于金融交易、订单支付等场景，尤其适合对系统一致性和事务可靠性要求较高的场景。事务消息能确保在消息发送和本地事务之间的一致性。 本地消息表：适用于异步处理或延迟处理，尤其是在高并发、大流量场景中，可以通过本地消息表和定时任务确保消息可靠性。 最大努力通知：适用于容忍一定消息丢失的场景，例如日志记录、通知推送等业务，这类业务的容错性较高，不需要完全保证每条消息的送达，只要尽最大努力确保通知的到达即可。 执行流程： MQ事务消息：消息发送方先发送半消息到 Broker，此时消息不可消费，但 Broker 会记录该消息状态。接着执行本地事务，根据执行结果，发送方通知 Broker 提交或回滚消息，决定消息是否可消费。如果 Broker 在规定时间内未收到确认，会向发送方发起回查请求，以确保消息状态和事务最终一致。 本地消息表：发送方在本地事务中记录消息到本地消息表，确保消息与业务操作同步执行且一致。记录的消息状态为“待发送”，定时任务定期扫描表中待发送的消息，将其发送至消息队列。消息成功消费后，消费者通过确认机制通知发送方，发送方将消息状态更新为“已完成”。 最大努力通知：发送方在消息发送过程中尽量保证消息成功传递给接收方，即使遇到网络问题也会进行多次重试。发送失败的消息会进入重试队列或死信队列，以便后续重发或进一步处理。尽管此机制不能保证百分百的消息送达，但通过多层次的重试和降级措施，努力减少消息丢失。 容错和恢复机制： MQ事务消息：消息发送方先发送半消息到 Broker 并记录状态，接着执行本地事务。根据事务结果决定提交或回滚该消息；如果 Broker 未收到确认，将回查事务状态，确保消息最终一致性。 本地消息表：本地事务中将消息记录到本地消息表并设为“待发送”，由定时任务扫描发送至消息队列。消息消费成功后更新状态为“已完成”，保证业务和消息的最终一致。 最大努力通知：发送方通过多次重试确保消息尽量传递成功，遇到异常则进入重试或死信队列处理。尽管不能确保百分百送达，通过重试和降级机制尽量减少消息丢失。 性能与复杂度： MQ事务消息：性能较低，因为每次发送消息都需要和本地事务进行绑定，并且涉及到消息提交或回滚的操作。系统复杂度较高，需要处理回查等操作。 本地消息表：性能相对较好，通过定时任务和本地消息表机制，不需要等待事务的回查，但需要额外的定时任务和表管理，且数据库的存储压力较大。 最大努力通知：性能最好，消息的发送与本地事务并不绑定，只需要通过重试机制和死信队列保证尽最大努力将消息送达。适用于容忍一定丢失的业务场景。 特点 MQ事务消息 本地消息表 最大努力通知 适用场景 需要保证分布式事务一致性的场景 不要求强一致性，确保消息最终送达 容忍一定消息丢失的场景 执行流程 发送半消息 -&gt; 执行本地事务 -&gt; 提交/回滚消息 -&gt; 回查事务状态 执行本地事务 -&gt; 记录消息 -&gt; 定时任务发送消息 -&gt; 消费者确认 尽最大努力发送消息 -&gt; 重试机制/死信队列 容错机制 通过回查机制保证最终一致性 通过定时任务和重试机制保证消息可靠性 重试机制和死信队列，无法完全保证送达 一致性 强一致性，消息与事务绑定 最终一致性，依赖重试机制和确认机制 无法保证一致性，只能最大努力保证送达 性能 较低，涉及事务回查等操作 较高，通过定时任务和重试机制管理 较高，适合对性能要求高但不要求严格一致性的场景 "},{"title":"【分布式】分布式锁设计与Redisson源码解析","date":"2024-11-05T03:12:31.000Z","url":"/2024/11/05/%E3%80%90%E5%88%86%E5%B8%83%E5%BC%8F%E3%80%91%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E8%AE%BE%E8%AE%A1/","tags":[["Redisson","/tags/Redisson/"],["分布式锁","/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"]],"categories":[["分布式","/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"]],"content":"分布式锁 分布式锁是一种在分布式计算环境中用于控制多个节点（或多个进程）对共享资源的访问的机制。在分布式系统中，多个节点可能需要协调对共享资源的访问，以防止数据的不一致性或冲突。分布式锁允许多个节点在竞争访问共享资源时进行同步，以确保只有一个节点能够获得锁，从而避免冲突和数据损坏。 设计一个分布式锁需要保证以下四大特性： 互斥性：在任意时刻，只能有一个进程持有锁。 进程一致：加锁和解锁的操作必须由同一个进程执行。 防死锁：即使有一个进程在持有锁期间崩溃而未能主动释放锁，必须有其他方式去释放锁，以保证其他进程能够获取到锁。 锁续期：持锁线程执行的操作超出预期时间，只要持锁线程仍然在执行，锁就不应该被释放。 MySQL实现 结构设计 设计表结构：设计一个锁的唯一标识 lock_name 作为表的主键，thread_id 字段存储持有锁的线程ID、设置 counter 字段用于记录重入次数、expires_at 设置锁的过期时间，以防止死锁。 设计索引：还可以在 CREATE 语句中建立联合索引，减少回表次数，优化查询速度，但在高并发场景下执行增删改操作效率会下降。 加锁过程 首次获取锁：通过 SELECT 语句，以 lock_name 和 expires_at 为查询条件，查询存在且未过期的锁。如果锁不存在，则使用 INSERT 语句插入锁标识、线程ID、计数器初始值一和过期时间。如果锁存在，执行下一步骤。（设置过期时间实现「防死锁」；由于 INSERT 语句默认使用行级锁，同一时刻只能有一个线程插入成功，因此保证了「互斥性」） 重复获取锁：判断查询结果中的 thread_id 字段是否与当前线程ID相同。如果相同，说明当前线程需要重复获取锁，执行 UPDATE 语句将 counter 字段加一，并重置过期时间。如果不相同，执行下一步骤。（设置计数器实现可重入锁） 获取锁失败：直接从查询结果返回锁的过期时间，帮助申请锁的线程得知等待锁释放的时间。 解锁过程 检查锁持有者：通过 SELECT 语句，以 lock_name 和 thread_id 为查询条件，查询锁是否由当前线程持有。如果结果为空，则返回 NULL 表示解锁失败。如果结果不为空，执行下一步骤。（通过条件判断保证「进程一致」，即加解锁为同一线程） 减少锁计数器：执行 UPDATE 语句给持有锁的线程的计数器减一，并判断计数器是否大于零。如果大于零，说明锁还没有完全释放，执行 UPDATE 语句重置锁的过期时间，返回 0 表示锁未完全释放；如果等于零，说明当前线程已完全释放锁，则执行 DELETE 语句删除整个锁，返回 1 表示锁完全释放。 Redis实现 结构设计 选用数据结构：采用 String 结构。设置锁的唯一标识作为 KEY，并指定一个唯一的线程标识作为值 VALUE。 加锁过程 设置锁：使用 SET 命令 NX（只在键不存在时设置）和 PX（设置过期时间）选项来实现一个原子操作，确保了即使持锁进程崩溃，其他进程仍然能够获取到锁，从而满足「互斥性」和「防死锁」 。 解锁过程 释放锁：通过 DEL 命令清除锁的键来释放锁。在执行 DEL 操作之前，先使用 GET 命令检查锁的值是否与持锁者的唯一标识匹配，从而满足「进程一致」 。 无论是MySQL还是Redis实现的分布式，虽然都考虑到了互斥性、防死锁和进程一致问题，但是却无法解决锁续期问题。所以，Redis 官方推荐采用 Redisson 实现 Redis 的分布式锁，借助 Redisson 的 WatchDog 机制能够很好的解决锁续期的问题。 Redisson实现 结构设计 选用数据结构：采用 Hash 结构，设置锁的唯一标识为键，值采用 field-value 格式，以线程ID为 field，计数器为 value 实现可重入锁。 加锁过程 执行Lua脚本：整个 Lua 脚本是以事务方式在 Redis 中运行的，由于 Redis 是单线程模型，因此脚本内的所有命令是按顺序一次性执行的，不会在中途被打断或交叉执行，从而保证「互斥性」。 首次获取锁：通过 exists 命令判断锁是否不存在。如果不存在，则执行 hincrby 命令设置 Hash 结构的 field 为线程ID，value 为计数器的初始值一，同时执行 pexpire 命令设置锁的过期时间；如果存在，执行下一步操作。 重复获取锁：通过 hexists 命令判断锁中的 field 是否与当前线程相同。如果相同，则执行 hincrby 命令给 field 对应的计数器加一，同时执行 pexpire 命令重置锁的过期时间，防止锁在持有者持有期间过期；如果不相同，说明当前锁被其他线程持有。 返回结果：如果返回 nil 表明获取锁成功；如果返回的数据不为 null 而是 Long，表明申请锁的线程需要等待的时间。 完整代码如下： 解锁过程 检查锁持有者：通过 hexists 命令查询锁中的 field 是否与当前线程相同。如果不相同，表明锁的持有者不是当前线程，返回 nil，如果相同，执行下一步操作。（通过条件判断保证「进程一致」，即加解锁为同一线程） 减少锁计数器：执行 hincrby 命令给持有锁的线程的计数器减一，并判断计数器是否大于零。如果大于零，说明锁还没有完全释放，执行 pexpire 命令重置锁的过期时间，返回 0 表示锁未完全释放；如果等于零，说明当前线程已完全释放锁，则执行 del 删除整个锁，同时执行 publish 命令通知所有等待锁的其他线程，返回 1 表示锁完全释放。（这里执行消息发布是服务于锁等待机制，防止无意义的申请锁而浪费资源） 完整代码如下： 看门狗机制 当线程尝试执行 tryLock() 方法获取锁时，在内部调用了 tryAcquireAsync() 方法获取锁的等待时间，返回值为 Long 型 。如果返回结果为 null，表明加锁成功；返回结果不为 null，返回值就是需要等待锁的释放时间。 在 tryAcquireAsync() 方法中，首先判断锁是否设置了释放时间。 如果设置了锁的释放时间，直接进行上述 lua脚本 的加锁操作，并返回结果； 如果没有设置锁的释放时间，将锁的过期时间设置为默认值30s并进行 lua脚本 的加锁操作，同时启用看门狗机制，不断的进行自动续约，实现「锁续期」； 可以看到，两种操作都最终使锁被设置了过期时间，防止持有锁的客户端异常退出后锁无法释放的问题（即「防死锁」）。 自动续约的操作由 scheduleExpirationRenewal 方法实现。该方法内部首先会从成员变量的 ConcurrentHashMap 集合中根据当前锁的名称获取值，如果获取不到，说明当前线程任务执行完毕，无需再进行锁的自动续期；如果可以获取到值，则启动一个定时任务，通过递归调用实现每 10s 触发一次任务，在任务内部执行了如下的 lua脚本，从而重置锁的过期时间。 取消自动续约：当持有锁的线程的任务执行完毕后，会执行 remove() 方法删除 ConcurrentHashMap 集合中的键值，而看门狗在获取 ConcurrentHashMap 集合中的键值失败后，就会返回结果，结束自动续约。 锁等待机制 尝试获取锁： 首先调用 tryAcquire() 方法获取锁剩余的存活时间 ttl，如果结果为 null，返回 true 表明加锁成功。 接着计算当前时间与获取锁之前的时间的差值，如果申请锁的耗时大于等待时间，表明申请锁失败，返回 false。 订阅锁释放通知：通过 subscribe 方法，基于当前线程的 threadId 发起一个异步订阅请求，等待锁释放的通知。这一步骤的主要作用是通过订阅锁的释放事件来实现对锁的高效管理，防止无效的锁申请对系统资源造成浪费。 等待锁释放超时：通过 await() 方法（内部使用 CountDownLatch 实现阻塞）在指定时间内等待失败，说明当前线程的等待时间超时，无需再获取锁，需要执行取消订阅和失败处理的逻辑。 取消订阅：通过 cancel() 方法取消订阅。如果取消失败，说明订阅任务正在执行，此时无法直接取消任务。需要执行回调函数等待任务执行完毕；如果取消成功，则执行 acquireFailed() 方法并返回 false。 回调函数取消订阅：通过 onComplete 回调，可以在任务完成后自动触发 unsubscribe 操作，以确保订阅状态被正确清理。 轮询获取锁： 再次获取锁：返回锁的剩余存活时间 ttl；如果 ttl 为空说明获取锁成功，直接返回 true，否则继续下一步。 阻塞获取锁：取锁剩余的存活时间和线程剩余的等待时间的最小值，利用信号量 Semaphore 阻塞获取锁。 取消订阅：无论最终是否成功获取锁，在 finally 中都会调用 unsubscribe() 方法取消订阅，以确保资源释放和避免不必要的等待事件。 "},{"title":"【SpringBoot】源码解析——启动流程","date":"2024-10-26T07:19:09.000Z","url":"/2024/10/26/%E3%80%90SpringBoot%E3%80%91%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E2%80%94%E2%80%94%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","tags":[["源码解析","/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"],["SpringBoot启动流程","/tags/SpringBoot%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"]],"categories":[["Spring","/categories/Spring/"]],"content":"Spring Boot启动流程 Spring Boot 的入口类： Spring Boot 的启动过程可以分为两方面，一个是 new SpringApplication(primarySources) 的初始化过程，一个是 SpringApplication.run() 的应用运行过程。 执行构造函数 构造函数是 Spring Boot 启动过程的第一步，负责初始化各种属性、验证输入并设置必要的上下文，为后续的应用启动和上下文配置奠定基础。Spring Boot 通过一个重载构造函数来接收 null 值的资源加载器和主类数组： 1.初始化字段 重构的构造函数接收ResourceLoader和primarySources参数，并为各种字段进行初始化，比如ResourceLoader, bannerMode, headless等。 2.设置主源 将primarySources转换为LinkedHashSet，以确保启动过程中按顺序处理，避免重复。 3.推断应用类型 通过检查类路径classpath中的组件（如 \"org.springframework.web.reactive.DispatcherHandler）来确定当前应用的类型： REACTIVE：响应式 Web 应用（基于 Reactor 和 WebFlux）。 SERVLET：传统的 Servlet 类型 Web 应用（如使用 Spring MVC）。 NONE：非 Web 应用，通常是命令行应用。 4.加载工厂实例 从 META-INF/spring.factories 中加载配置的初始化器 ApplicationContextInitializer 、BootstrapRegistryInitializer和监听器 ApplicationListener。 BootstrapRegistryInitializer 允许在 Spring Boot 启动时自定义初始化 Bootstrap 注册表。Bootstrap 注册表是一个支持应用上下文的初始化过程的机制，通常用于配置与应用启动相关的共享资源。 ApplicationContextInitializer 允许用户在 ApplicationContext 刷新前进行进一步的初始化配置操作。这包括但不限于添加属性源、修改环境变量、设置 Bean 定义等。 ApplicationListener，用于监听 Spring Boot 的生命周期事件。这些监听器会在应用启动过程中响应不同的事件，如应用启动事件、环境准备事件、上下文刷新事件等。 getSpringFactoriesInstances() 方法的核心方法： loadFactoryNames()：利用 SpringFactoriesLoader 获取指定工厂的实现类的 Set 集合 createSpringFactoriesInstances：通过反射机制实例化每个工厂的实现类 5.推断主类 通过 StackTraceElement 找到 main 方法，确定应用程序的入口类。这通常是标记了 @SpringBootApplication 注解的类。 执行run方法 1.启动计时器并初始化 在执行run方法的开头，首先启动一个计时器以记录应用启动的总时长。 接着，创建一个DefaultBootstrapContext，它会遍历并执行在Bootstrap注册表中的所有初始化器，以确保启动过程中的必要资源和设置得到正确配置。 然后，ConfigurableApplicationContext的引用被声明为null，并配置无头属性，以便在没有用户界面的环境中正常运行。 2.获取并启动监听器 在获取监听器的方法 getRunListeners() 中，将所有的监听器封装为一个 SpringApplicationRunListeners 对象，由于在构造函数执行阶段已经加载了监听器对象，在调用方法 getSpringFactoriesInstances 时会直接查询缓存获取对象。 通知监听器应用即将启动。这一步骤确保所有监听器能够在应用启动的早期阶段参与并进行必要的初始化。 3.装配环境参数 将环境参数与「引导上下文」绑定，prepareEnvironment() 方法会加载应用的外部配置。这包括 application.properties 或 application.yml 文件中的属性，环境变量，系统属性等。 在prepareEnvironment() 方法中，首先会进行环境配置，还会执行监听器的 environmentPrepared() 方法，表明应用程序的环境已经准备好，最后再将环境绑定到应用程序中。 4.打印横幅 5.创建应用上下文 根据构造阶段推断出的 Web 应用类型，创建Spring容器 6.应用上下文准备阶段 进入 prepareContext()方法，具体实现如下： 7.应用上下文刷新阶段 首先通过加锁确保线程安全，创建并配置 BeanFactory，这一过程包括注册 Bean 后处理器和事件监听器。 在 onRefresh() 方法中，还会启动 Web 服务器。 最后，通过配置好的 BeanFactory 实例化所有的 Beans。在这个过程中，BeanFactory 会根据定义的元数据创建和初始化 Beans，并根据需求进行依赖注入，确保整个应用的组件能够顺利协作。 8.应用上下文收尾阶段 9.回调运行器 该方法从上下文 context 中获取所有已注册的 ApplicationRunner 和 CommandLineRunner，并结合传入的应用参数 args执行这些运行器。 这些运行器允许开发者在应用程序启动后执行特定的逻辑，例如初始化数据、设置应用状态或执行启动任务，提供了灵活性和扩展性。 10.异常处理 在整个启动过程中，如果出现任何异常，都会被捕获并通过handleRunFailure()方法进行处理，在该方法中，会通知监听器应用程序启动时出现异常。 该方法会记录错误信息，并通过监听器通知失败事件。最终，抛出IllegalStateException来中止应用启动，确保调用者能够识别到启动失败的状态。 SpringApplicationRunListeners监听器 SpringApplicationRunListeners 是一个具体的类。它实现了 Spring Boot 中的监听器机制，用于在应用程序的不同启动阶段通知注册的监听器（SpringApplicationRunListener 接口的实现类）。通过这个类，Spring Boot 可以在应用启动过程中管理多个监听器，处理各种生命周期事件。 SpringApplicationRunListener 是 Spring Boot 中的一个接口，用于在应用启动过程的不同阶段提供回调。实现这个接口允许监听并响应应用生命周期中的关键事件。该接口定义了多个方法，每个方法对应启动过程中的特定阶段，包括： starting(): 在运行开始时调用，此时尚未开始任何处理，可以用于初始化在启动过程中需要的资源。 environmentPrepared(): 当 SpringApplication 准备好 Environment 但在创建 ApplicationContext 之前调用，这是修改应用环境属性的好时机。 contextPrepared(): 当 ApplicationContext 准备好但在加载之前调用，可以用于对上下文进行预处理。 contextLoaded(): 当 ApplicationContext 被加载但在刷新之前调用，此时所有的 Bean 定义都已加载，但尚未实例化。 started(): 在 ApplicationContext 刷新之后、任何应用运行器和命令行运行器被调用之前调用，此时应用已经准备好接收请求。 running(): 在运行器被调用之后、应用启动完成之前调用，这是在应用启动并准备好服务请求时执行某些动作的好时机。 failed(): 如果启动过程中出现异常，则调用此方法。 SpringFactoriesLoader原理 SpringFactoriesLoader方法会根据传入的工厂类和类加载器，从 META-INF/spring.factories 文件中加载「指定类型对应的工厂类名称」。 loadFactoryNames() loadFactoryNames 方法是一个高级 API，它通过获取入参中的全限定类名 factoryTypeName，在内部调用 loadSpringFactories() 方法获取返回的 Map 集合，并根据 factoryTypeName 获取了 Map 中的实现类的 List 集合。 loadSpringFactories() loadSpringFactories 方法是更加底层的方法，通过缓存机制和类加载器获取 spring.factories 文件中所有配置的工厂及其实现类，将这些信息封装为 Map 集合后返回给上游的 API。 缓存机制 方法会检查是否已经通过当前类加载器加载过 spring.factories 文件。如果缓存 (cache) 中已经存在相应的工厂信息，直接返回缓存的 Map&lt;String, List&lt;String&gt;&gt;，避免重复加载。 加载 META-INF/spring.factories 方法会通过类加载器查找所有路径下名为 META-INF/spring.factories 的文件。由于每个 JAR 包都可能包含一个 META-INF/spring.factories 文件，方法会返回一个 Enumeration&lt;URL&gt; 对象，表示找到的所有相关资源文件。 解析spring.factories文件 通过迭代逐个读取每个找到的 spring.factories 文件。对于每个文件，使用 PropertiesLoaderUtils.loadProperties() 将文件内容解析为 Properties 对象。 每个 Properties 对象对应一个 spring.factories 文件的内容，其中 key 是工厂类型（例如 org.springframework.context.ApplicationContextInitializer），value 是逗号分隔的工厂实现类列表。 将工厂类型和实现类存入Map 遍历 Properties 的 entrySet()。对于每个 entry，key 是工厂类型的全限定类名，value 是对应的工厂实现类名（逗号分隔）。 工厂类型名称通过 entry.getKey() 获取，并使用 String.trim() 去除可能的空白字符。工厂实现类则将逗号分隔的字符串转换为实现类的数组。 "},{"title":"【SpringBoot】源码解析——自动装配与starter机制","date":"2024-10-26T07:09:10.000Z","url":"/2024/10/26/%E3%80%90SpringBoot%E3%80%91%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E2%80%94%E2%80%94%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E4%B8%8Estarter%E6%9C%BA%E5%88%B6/","tags":[["源码解析","/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"],["自动装配","/tags/%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/"],["starter机制","/tags/starter%E6%9C%BA%E5%88%B6/"]],"categories":[["Spring","/categories/Spring/"]],"content":"自动装配机制 在传统的 Spring 框架中，开发者需要通过 XML 文件或 Java 配置类显式地声明 Bean 和各种配置项（例如数据源、事务管理、视图解析器等）。Spring Boot 的自动装配旨在减少这些繁琐的配置，通过默认的配置和条件装配，自动完成很多配置工作，从而减少开发者的配置量。 @SpringBootApplication注解 可以把 @SpringBootApplication 注解看作是 @SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan 注解的集合。 @EnableAutoConfiguration：自动配置机制的核心注解。 @SpringBootConfiguration：作为 @Configuration 注解的扩展，它标识该类为 Spring 的配置类。在该类中可以定义 @Bean 方法，这些方法返回的对象将被注册到 Spring 容器中，由容器管理其生命周期。 @ComponentScan：从声明 @SpringBootApplication 的类所在的包开始，自动扫描并注册 @Component、@Service、@Repository 等注解的类到 Spring 容器中。它确保应用程序的组件、服务、控制器等能够被自动发现和注入。 @EnableAutoConfiguration核心注解 @EnableAutoConfiguration 通过 @Import 注解导入了 AutoConfigurationImportSelector 类。 @Import 注解的作用是将指定的配置类或 ImportSelector 导入到当前的配置类中。 AutoConfigurationImportSelector核心类 AutoConfigurationImportSelector 是自动装配的核心类，其实现了ImportSelector接口的 selectImports方法。 selectImports()方法 该方法主要有两个作用： 判断自动装配开关是否打开，检查 yml 文件是否修改了 spring.boot.enableautoconfiguration=false 调用 getAutoConfigurationEntry()方法，获取需要自动装配的 bean getAutoConfigurationEntry() getAutoConfigurationEntry() 方法的核心作用是在 Spring 启动时，对当前应用中的自动配置类进行去重和筛选。 getCandidateConfigurations()核心方法 跟进到 getCandidateConfigurations() 方法，主要通过 SpringFactoriesLoader 和 ImportCandidates 来加载自动配置的候选类。 使用 SpringFactoriesLoader 加载 META-INF/spring.factories 文件中定义的与 EnableAutoConfiguration.class 相关的实现类。 通过 ImportCandidates 加载与 AutoConfiguration.class 相关的其他候选实现类，并将这些类添加到 configurations 列表中。 最后，使用 Assert.notEmpty() 方法确保 configurations 列表中至少有一个候选类后，返回候选类。 条件化装配 常见注解 Bean 条件注解： @ConditionalOnBean：当容器里有指定 Bean 的条件下 @ConditionalOnMissingBean：当容器里没有指定 Bean 的情况下 @ConditionalOnSingleCandidate：当指定 Bean 在容器中只有一个，或者虽然有多个但是指定首选 Bean Class 条件注解： @ConditionalOnClass：当类路径下有指定类的条件下 @ConditionalOnMissingClass：当类路径下没有指定类的条件下 属性条件注解：@ConditionalOnProperty：指定的属性是否有指定的值 Web 应用条件注解： @ConditionalOnNotWebApplication：当前项目不是 Web 项目的条件下 @ConditionalOnWebApplication：当前项目是 Web 项 目的条件下 其他条件注解： @ConditionalOnResource：类路径是否有指定的值 @ConditionalOnExpression：基于 SpEL 表达式作为判断条件 @ConditionalOnJava：基于 Java 版本作为判断条件 @ConditionalOnJndi：在 JNDI 存在的条件下差在指定的位置 SpringFactoriesLoader原理 SpringFactoriesLoader方法会根据传入的工厂类和类加载器，从 META-INF/spring.factories 文件中加载「指定类型对应的工厂类名称」。 loadFactoryNames() loadFactoryNames 方法是一个高级 API，它通过获取入参中的全限定类名 factoryTypeName，在内部调用 loadSpringFactories() 方法获取返回的 Map 集合，并根据 factoryTypeName 获取了 Map 中的实现类的 List 集合。 loadSpringFactories() loadSpringFactories 方法是更加底层的方法，通过缓存机制和类加载器获取 spring.factories 文件中所有配置的工厂及其实现类，将这些信息封装为 Map 集合后返回给上游的 API。 缓存机制 方法会检查是否已经通过当前类加载器加载过 spring.factories 文件。如果缓存 (cache) 中已经存在相应的工厂信息，直接返回缓存的 Map&lt;String, List&lt;String&gt;&gt;，避免重复加载。 加载 META-INF/spring.factories 方法会通过类加载器查找所有路径下名为 META-INF/spring.factories 的文件。由于每个 JAR 包都可能包含一个 META-INF/spring.factories 文件，方法会返回一个 Enumeration&lt;URL&gt; 对象，表示找到的所有相关资源文件。 解析spring.factories文件 通过迭代逐个读取每个找到的 spring.factories 文件。对于每个文件，使用 PropertiesLoaderUtils.loadProperties() 将文件内容解析为 Properties 对象。 每个 Properties 对象对应一个 spring.factories 文件的内容，其中 key 是工厂类型（例如 org.springframework.context.ApplicationContextInitializer），value 是逗号分隔的工厂实现类列表。 将工厂类型和实现类存入Map 遍历 Properties 的 entrySet()。对于每个 entry，key 是工厂类型的全限定类名，value 是对应的工厂实现类名（逗号分隔）。 工厂类型名称通过 entry.getKey() 获取，并使用 String.trim() 去除可能的空白字符。工厂实现类则将逗号分隔的字符串转换为实现类的数组。 Spring Boot Starter机制 Spring Boot Starter 是一组方便的依赖描述符，旨在简化 Maven 和 Gradle 项目的依赖管理。每个 Starter 通常包含与某种功能或库相关的依赖，开发者只需引入相应的 Starter，就可以自动获得这些依赖及其配置。 实现原理 依赖管理 Spring Boot Starter本质上是一个Maven或Gradle依赖，包含了一组已配置好的常用库。例如，spring-boot-starter-web包含了Spring MVC、Jackson（用于JSON处理）、Tomcat（作为默认的嵌入式Web服务器）等依赖。 通过在项目中添加Starter依赖，开发者无需逐个添加这些库。Starter通常命名为 spring-boot-starter-xxx，其中xxx表示特定的功能模块。例如： spring-boot-starter-data-jpa：包含Spring Data JPA和Hibernate依赖。 spring-boot-starter-security：包含Spring Security依赖。 spring-boot-starter-web：用于开发Web应用，包含Spring MVC及相关依赖。 自动装配机制 Spring Boot在启动时会自动扫描类路径下的资源文件和META-INF/spring.factories，从中找到定义的自动配置类。每个自动配置类根据一组条件（如类路径中是否存在特定类或是否有某些配置属性）来决定是否启用。 自动装配机制基于@EnableAutoConfiguration注解，它通过读取所有路径下 spring.factories 文件中的配置，加载与项目相关的自动配置类。例如： 加载配置文件 Spring Boot默认会从classpath下加载application.properties或application.yml配置文件，开发者可以通过这些文件提供自定义的配置属性，来覆盖自动配置中的默认值。此外，Spring Boot还支持通过命令行参数、环境变量等方式注入配置，以便在不同环境中灵活调整应用配置。 自定义Starter 创建工程 创建一个名为 thread-spring-boot-starter 的工程，使用 Maven 添加所需的基本依赖。 编写配置类 创建自动配置类，使用 @Configuration 注解，并结合条件注解（如 @ConditionalOnClass）来定义自动装配逻辑。 注册配置类 在src/main/resources/META-INF目录下创建一个spring.factories文件，并添加以下配置： 添加Starter依赖 将其打包并安装到本地Maven仓库。 一旦安装完成，其他项目就可以通过在其pom.xml文件中添加以下依赖来使用这个Starter了： 使用Starter 在其他项目中，可以通过@Autowired注解获取ThreadPoolExecutor实例。 "},{"title":"【Redis】深入五大数据类型","date":"2024-10-04T09:21:43.000Z","url":"/2024/10/04/%E3%80%90Redis%E3%80%91%E6%B7%B1%E5%85%A5%E4%BA%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","tags":[["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Redis","/categories/Redis/"]],"content":"数据结构 简单动态字符串（SDS） Redis 是用 C 语言实现的，但是它没有直接使用 C 语言的 char* 字符数组来实现字符串，而是自己封装了一个名为简单动态字符串（simple dynamic string，SDS） 的数据结构来表示字符串。 C语言字符串 C 语言的字符串不足之处以及可以改进的地方： 获取字符串长度的时间复杂度为 O（N）； 字符串的结尾是以 “\\0” 字符标识，字符串里面不能包含 “\\0” 字符，因此不能保存二进制数据； 字符串操作函数不高效且不安全，比如有缓冲区溢出的风险，有可能会造成程序运行终止。比如函数char *strcat(char *dest, const char* src);，可能会超出dest字符串长度。 结构设计 前三个属性len、alloc和flags都是结构体的头信息，只有buf字段才真正存储数据。 结构中的每个成员变量分别介绍下： len，记录了字符串长度。这样获取字符串长度的时候，只需要返回这个成员变量值就行，时间复杂度只需要 O（1）。 alloc，分配给字符数组的空间长度。这样在修改字符串的时候，可以通过 alloc - len 计算出剩余的空间大小，可以用来判断空间是否满足修改需求，如果不满足的话，就会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出的问题。 flags，用来表示不同类型的 SDS。一共设计了 5 种类型，分别是 sdshdr5、sdshdr8、sdshdr16、sdshdr32 和 sdshdr64。这 5 种类型的主要区别就在于，它们数据结构中的 len 和 alloc 成员变量的数据类型不同。 除了设计不同类型的结构体，Redis 在编程上还使用了专门的编译优化来节省内存空间，即在 struct 声明了 __attribute__ ((packed)) ，它的作用是：告诉编译器取消结构体在编译过程中的优化对齐，按照实际占用字节数进行对齐。 buf[]，字符数组，用来保存实际数据。不仅可以保存字符串，也可以保存二进制数据。 flags字段的前3位用于表示SDS的类型： SDS_TYPE_5 (000): 使用5位存储长度信息，适用于长度较小的字符串。 SDS_TYPE_8 (001): 使用1个字节（8位）存储长度信息。 SDS_TYPE_16 (010): 使用2个字节（16位）存储长度信息。 SDS_TYPE_32 (011): 使用4个字节（32位）存储长度信息。 SDS_TYPE_64 (100): 使用8个字节（64位）存储长度信息。 动态扩容 当追加的字符串超过分配的空间时，会触发扩容机制： 如果新字符串小于1M，则新空间为扩展后字符串长度的两倍 + 1（+1是为了保存一个结束标识'\\0'） 如果新字符串大于1M，则新空间为扩展后字符串长度+ 1M + 1。称为内存预分配。 示例：初始有一个内容为\"hi\"的SDS，加上字符串\",Amy\"，新字符串长度小于1M，扩容为原本的2倍+1 整数集合（IntSet） 整数集合是 Set 对象的实现方式之一。具备长度可变、有序等特征，当一个 Set 对象只包含整数值元素，并且元素数量不大时，就会使用Intset这个数据结构作为底层实现。 结构设计 encoding的三种模式： 编码升级 当向intset中插入一个新整数且该整数超过了当前编码类型的范围时，intset会自动进行编码升级。 现在，假设有一个intset，元素为{5，10，20}，采用的编码是INTSET_ENC_INT16，则每个整数占2字节。我们向该其中添加一个数字：50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。 以当前案例来说流程如下: 升级编码为INTSET_ENC_INT32，每个整数占4字节，并按照新的编码方式及元素个数扩容数组 倒序依次将数组中的元素拷贝到扩容后的正确位置 将待添加的元素放入数组末尾 字典（Dict） Redis本身就是一个键值型的数据库，其核心就是通过高效的方式来存储和检索键值对。在Redis的内部，键值对的存储和管理依赖于一种叫做Dict（字典）的数据结构。 结构设计 Dict由三部分组成，分别是：哈希表(DictHashTable)、哈希节点(DictEntry)、字典(Dict) 哈希表： 哈希节点： 字典： 添加元素 当利用Dict添加元素时，Redis首先会根据key计算出hash值，然后利用哈希值对长度取余hash % size获取哈希表的下标。 当size的长度为2的倍数时，满足公式：hash % size == hash &amp; (sizemask)。由于位运算可直接在内存进行操作，不需要转为十进制，所以性能更好。 公式解释：当哈希表的长度为2的倍数时，哈希表大小的掩码sizemask的二进制表示必然是多个1组成的1111，而1按位与任何数，都为数本身，所以可以达到对hash值取余的效果。 当有元素添加进来时，采用头插法插入元素，头插法的时间复杂度为 O(1)，能够在哈希冲突发生时迅速将新元素插入到链表中。 rehash 动态扩容 Dict中的HashTable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。 Dict在每次增加键值对时都会检查负载因子（哈希表中元素的数量与哈希表桶数量的比值）LoadFactor = used/size，满足以下三 种情况时会触发哈希表扩容。 负载因子LoadFactor &gt;= 1，并且服务器没有执行BGSAVE或者BGREWRITEAOF等后台进程； 负载因子LoadFactor &gt; 5； 当Dict执行扩容时，需要扩容到第一个大于等于used + 1（哈希表中元素个数 + 1）的2 ^ n。 动态缩容 满足以下情况时，触发哈希表缩容。 负载因子LoadFactor &lt; 0.1，会触发哈希表缩容 当Dict执行缩容时，需要缩容到第一个大于等于used（哈希表中元素个数）的2 ^ n，但不能小于初始大小4。 渐进式rehash 不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的: 计算新hash表的size，值取决于当前要做的是扩容还是收缩: 如果是扩容，则新size为第一个大于等于dict.ht[0].used+1的2^n 如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n(不得小于4) 按照新的size申请内存空间，创建dictht，并赋值给dict.ht[1] 设置dict.rehashidx=0，标示开始rehash 每次执行新增、查询、修改、删除操作时，都检查一下dict.rehashidx是否大于-1，如果是则将dict.ht[0].table[rehashidx]桶上的所有元素rehash到dict.ht[1]，并且将rehashidx++。直至dict.ht[0]的所有数据都rehash到dict.ht[1]中。 将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存 将rehashidx赋值为-1，代表rehash结束 在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht{0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空 Dict与HashMap Dict 负载因子为5时扩容：较高的负载因子可以使得哈希表桶的数据更加紧凑，提高内存利用率；较高的负载因子可以减少扩容频率，减少资源消耗；在负载因子为 5 时，哈希表桶的链表相对较短，仍能保证相对较快的查找效率。 在负载因子较高时扩容，字典的空间利用率更高，这对于减少内存开销很有帮助。而在负载因子为5时，哈希冲突虽然较多，但Redis通过链表等机制处理冲突，依然能够保持较高的查询速度。 渐进式rehash：Redis在扩容时采用渐进式rehash（渐进式重新哈希），逐步将旧表中的键值对迁移到新表中，以避免扩容时阻塞Redis的正常操作。因此，Redis可以在负载因子较高时继续工作，并在扩容过程中平滑地处理大量数据。 单线程模型：Redis 是单线程处理模型，Dict 不需要处理线程安全问题。这使得 Dict 的操作可以更加简单和高效，因为不需要复杂的锁机制来同步多线程访问。 链地址法： HashMap 负载因子为0.75时扩容：HashMap在设计时追求在插入和查找操作之间的平衡。负载因子为0.75时，哈希冲突率较低，查找和插入性能较高，同时又避免了内存的浪费。 即时扩容：与Redis不同，Java的HashMap在达到扩容阈值时会立即进行扩容操作，重新计算哈希值并将元素放置到新表中。这样做是为了保持插入和查找的性能，避免在高负载因子下，哈希冲突过多导致查询效率大幅下降。 多线程环境：HashMap 本身不是线程安全的，在多线程环境中使用时可能会出现并发修改异常或数据不一致问题。为此，Java 提供了 ConcurrentHashMap，它使用分段锁（Java 7 之前）或 CAS 操作（Java 8 之后）来实现线程安全的哈希表。 红黑树： 压缩列表（ZipList） ZipList是一种特殊的“双端链表”，由一系列特殊编码的连续内存块组成。可以在任意一端进行压入/弹出操作,并且该操作的时间复杂度为 0(1)。 结构设计 ZipList： 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节，通过这个偏移量，可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量。最大值为 UINT16_MAX (65534)，如果超过这个值，此处会记录为 65535，但节点的真实数量需要遍历整个压缩列表才能计算得出。 entry 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF (十进制 255)，用于标记压缩列表的末端。 ZipListEntry： ZipList中的Entry并不像普通链表那样记录前后节点的指针，因为记录两个指针要占用16个字节，浪费内存。而是采用下面的结构： previous_entry_length encoding content previous_entry_length：前一节点的长度，占1个或5个字节。 如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于254字节，则采用5个字节来保这个长度值，第一个字节为0xfe标记采用5字节，后四个字节才是真实长度数据 encoding：编码属性，记录content的数据类型(字符串还是整数)以及长度，占用1个、2个或5个字节 contents：负责保存节点的数据，可以是字符串或整数 encoding ZipListEntry中的encoding编码分为字符串和整数两种，编码格式采用十六进制。 字符串 如果encoding是以\"00\"、\"01\"或者\"10\"开头，则证明content是字符串 编码 编码长度 字符串大小 |00pppppp| 1bytes 最大长度为2^6-1 |01pppppp|qqqqqqqq| 2 bytes 最大长度为2^14-1 |10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| 5 bytes 最大长度为2^32-1 例如，如果需要保存字符串\"ab\"和\"bc\"，abc的ASCII码从97依次递增 整数 如果encoding是以\"11\"开始，则证明content是整数，且encoding固定只占用1个字节 编码 编码长度 整数类型 11000000 1 int16_t (2 bytes) 11010000 1 int32_t (4 bytes) 11100000 1 int64_t (8 bytes) 11110000 1 24位有符整数 (3 bytes) 11111110 1 8位有符整数 (1 byte) 1111xxxx 1 省去content字段，直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值 例如，如果需要保存整数\"2\"和\"6\"，省去content字段直接存储。 连锁更新问题 previous_entry_length：前一节点的长度，占1个或5个字节。 如果前一节点的长度小于等于254字节，则采用1个字节来保存这个长度值 如果前一节点的长度大于254字节，则采用5个字节来保这个长度值，第一个字节为0xfe标记采用5字节，后四个字节才是真实长度数据 假设压缩列表有多个连续的、长度为250~253字节之间的entry，因此节点的previous_entry_length属性用1个字节即可表示，如图所示： 此时如果添加一个新的节点，长度超过了254字节，就会导致e1节点的previous_entry_length属性更新，采用5字节，从而导致后续的节点产生连锁更新的现象。 快速列表（QuickList） 压缩列表 压缩列表有以下缺点： ZipList虽然节省内存，但申请内存必须是连续空间，如果内存占用较多，申请内存效率很低（找不到内存碎片）。 ZipList存在连锁更新问题，元素越多越可能发生连锁更新问题。 结构设计 quickList： 限制大小 为了避免QuickList中的每个ZipList中entry过多，Redis提供了一个配置项：list-max-ziplist-size来限制，默认值为-2。 如果值为正，则代表ZipList的允许的entry个数的最大值 如果值为负，则代表ZipList的最大内存大小，分5种情况 首位压缩 除了控制ZipList的大小，QuickList还可以对节点的ZipList做压缩。通过配置项list-compress-depth来控制。因为链表一般都是从首尾访问较多，所以首尾是不压缩的。这个参数是控制首尾不压缩的节点个数： 0:特殊值，代表不压缩 1:标示QuickList的首尾各有1个节点不压缩，中间节点压缩 2:标示QuickList的首尾各有2个节点不压缩，中间节点压缩 以此类推 跳表（SkipList） 链表在查找元素的时候，因为需要逐一查找，所以查询效率非常低，时间复杂度是O(N)，于是就出现了跳表。跳表是在链表基础上改进过来的，实现了一种「多层」的有序链表，这样的好处是能快读定位数据。 结构设计 zskiplist： zskiplistNode： ZSet 对象要同时保存「元素」和「元素的权重」，对应到跳表节点结构里就是 sds 类型的 ele 变量和 double 类型的 score 变量。每个跳表节点都有一个后向指针（struct zskiplistNode *backward），指向前一个节点，目的是为了方便从跳表的尾节点开始访问节点，这样倒序查找时很方便。 跳表是一个带有层级关系的链表，而且每一层级可以包含多个节点，每一个节点通过指针连接起来，实现这一特性就是靠跳表节点结构体中的zskiplistLevel 结构体类型的 level 数组。 level 数组中的每一个元素代表跳表的一层，也就是由 zskiplistLevel 结构体表示，比如 leve[0] 就表示第一层，leve[1] 就表示第二层。zskiplistLevel 结构体里定义了「指向下一个跳表节点的指针」和「跨度」，跨度用来记录两个节点之间的距离。 跨度实际上是为了计算这个节点在跳表中的排位。具体怎么做的呢？因为跳表中的节点都是按序排列的，那么计算某个节点排位的时候，从头节点点到该结点的查询路径上，将沿途访问过的所有层的跨度累加起来，得到的结果就是目标节点在跳表中的排位。 举个例子，查找图中节点 3 在跳表中的排位，从头节点开始查找节点 3，查找的过程只经过了一个层（L2），并且层的跨度是 3，所以节点 3 在跳表中的排位是 3。 查询过程 查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，共有两个判断条件： 如果下一节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。 如果下一节点的权重「等于」要查找的权重时，并且下一节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。 如果上面两个条件都不满足，或者下一个节点为空时，跳到当前节点的下一层。 如果需要查找「元素：abcd，权重：4」的节点，查找过程如下： 先从头结点最高层开始，L2指向了「元素：abc，权重：3」节点，先查询该节点 由于该层下一节点为NULL，所以跳到下一层，也就是「元素：abc，权重：3」节点的level[1]层 level[1]层的下一节点是「元素：abcde，权重：4」，当前的权重等于查找的权重，但是当前的元素大于查询的元素（abcde &gt; abcd），所以跳转到下一层，也就是「元素：abc，权重：3」节点的level[0]层 查询到「元素：abcd，权重：4」，正是要查询的节点，直接返回。 插入过程 跳表的相邻两层的节点数量最理想的比例是 2:1，查找复杂度可以降低到 O(logN)。 具体的做法是，跳表在添加节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。 插入一个新节点到跳表中，通常会涉及以下步骤： 查找插入位置： 如果下一节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。 如果下一节点的权重「等于」要查找的权重时，并且下一节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。 如果上面两个条件都不满足，或者下一个节点为空时，跳到当前节点的下一层。 随机生成层数：在跳表中，新节点的层数是随机生成的。层数越高，新节点的概率越低。 插入新节点： 更新与新节点相关的前驱节点和后继节点的指针，以保持跳表的正确性。 如果插入的层数比当前跳表的最大层数要高，那么需要更新跳表的头节点层数。 删除过程 查找删除位置： 如果下一节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。 如果下一节点的权重「等于」要查找的权重时，并且下一节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。 如果上面两个条件都不满足，或者下一个节点为空时，跳到当前节点的下一层。 删除新节点： 更新与原有节点相关的前驱节点和后继节点的指针，以保持跳表的正确性。 如果删除后的层数比当前跳表的最大层数要低，那么需要更新跳表的头节点层数。 跳表与平衡树 从内存占用上来比较，跳表比平衡树更灵活一些。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。 在做范围查找的时候，跳表比平衡树操作要简单。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。 从算法实现难度上来比较，跳表比平衡树要简单得多。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速。 listpack listpack的目的是替代压缩列表，它的最大特点是 listpack 中每个节点不再包含前一个节点的长度，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。 主要包含三个方面内容: encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码 data，实际存放的数据: len，encoding+data的总长度: 可以看到，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题。 数据类型 常见数据类型 Redis 中比较常见的数据类型有下面这些： 5 种基础数据类型：String（字符串）、List（列表）、Hash（散列）、Set（集合）、Zset（有序集合）。 3 种特殊数据类型：HyperLogLog（基数统计）、Bitmap （位图）、Geospatial (地理位置)。 RedisObject Redis中的任意数据类型的键和值都会被封装为一个RedisObject，也叫做Redis对象。 Redis中会根据存储的数据类型不同，选择不同的编码方式。encoding的编码方式如下： 数据类型 对应编码方式 OBJ_STRING int、embstr、raw OBJ_LIST LinkedList + ZipList（3.2以前）、QuickList（3.2以后） OBJ_SET IntSet、HT OBJ_ZSET ZipList、HT、SkipList OBJ_HASH ZipList、HT String String 是最基本的key-value结构，key 是唯一标识，value 是具体的值。value不一定是字符串，也可以是数字(整数或浮点数)，value 最多可以容纳的数据长度是 512M。 其基本编码方式是RAW，基于简单动态字符串(SDS)实现，存储上限为512mb。 如果存储的SDS长度小于44字节，则会采用EMBSTR编码，此时object head与SDS是一段连续空间。申请内存时只需要调用一次内存分配函数，效率更高。 如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码，直接将数据保存在RedisObject的ptr指针位置(刚好8字节)，不再需要SDS了。 List List 结构类似一个双端链表，可以从首、尾操作列表中的元素。 在3.2版本之前，Redis采用LinkedList和ZipList编码来实现List，当元素数量小于512并且元素大小小于64字节时采用ZipList编码，超过则采用LinkedList编码 在3.2版本之后，Redis统一采用QuickList编码来实现List Set Set 类型是一个无序并唯一的键值集合，它的存储顺序不会按照插入的先后顺序进行存储。 当存储的所有数据都是整数，并且元素数量不超过set-max-intset-entries时，Set会采用IntSet编码，以节省内存。 为了查询效率和唯一性，Set采用HT编码(Dict)。Dict中的key用来存储元素，value统一为null ZSet ZSet也就是SortedSet，其中每一个元素都需要指定一个score值和member值。ZSet底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求，所以需要同时采用SkipList和HT这两种编码。 SkipList：可以排序，并且可以同时存储score和ele值(member) HT：可以键值存储，并且可以根据key找value 当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此ZSet还会采用ZipList编码来节省内存，不过需要同时满足两个条件: 元素数量小于zset_max_ziplist_entries(默认值128) 每个元素都小于zset_max_ziplist_value(默认值64字节) ZipList本身没有排序功能，而且没有键值对的概念，通过编码实现: ZipList是连续内存，因此score和element是紧挨在一起的两个entry，element在前，score在后 score越小越接近队首，score越大越接近队尾，按照score值升序排列 Hash Hash结构默认采用ZipList编码，用以节省内存。ZipList中相邻的两个entry分别保存field和value 当数据量较大时，Hash结构会转为HT编码，也就是Dict，触发条件有两个: ZipList中的元素数量超过了hash-max-ziplist-entries(默认512) ZipList中的任意entry大小超过了hash-max-ziplist-value(默认64字节) "},{"title":"【常用集合】深入浅出Map集合","date":"2024-09-29T09:03:21.000Z","url":"/2024/09/29/%E3%80%90%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88%E3%80%91%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAMap%E9%9B%86%E5%90%88/","tags":[["HashMap","/tags/HashMap/"],["ConcurrentHashMap","/tags/ConcurrentHashMap/"]],"categories":[["常用集合","/categories/%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88/"]],"content":"HashMap HashMap 主要用来存放键值对，它基于哈希表的 Map 接口实现，是常用的 Java 集合之一，是非线程安全的。 HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个。 底层实现 通过 key 的 hashCode 经扰动函数处理得到 hash 值，并通过 hash &amp; (length - 1) 确定存储位置（length为数组的长度）。若位置已有元素，比较 hash 值和 key，相同则覆盖，不同则用链表解决冲突。扰动函数用于优化 hashCode() 实现，减少哈希碰撞。 存储结构 在 JDK 1.8 之前，HashMap 采用 链地址法 来解决哈希冲突，即将数组与链表结合使用。当多个元素的哈希值映射到同一个桶位时，HashMap 会在该桶位置创建一个链表，所有冲突的元素都以链表的形式存储。在插入新元素时，HashMap 使用 头插法，将新元素插入链表的开头位置，以此处理哈希冲突。 在 JDK 1.8 之后，HashMap 解决哈希冲突的方式进行了优化，仍然采用 数组+链表 的结构，但引入了 红黑树 来提高性能。当某个桶位上的链表长度超过阈值（默认 8），并且哈希表桶的长度大于64时，链表会转换为红黑树，从而将链表的查询元素的时间复杂度由 O(n) 降低为 O(log n)。这样，当哈希冲突较多时，HashMap 仍然能够保持较高的查询和插入效率。 数据插入方式 在 JDK 1.7 及之前的版本中，HashMap 在多线程环境下进行扩容时，由于使用头插法重排链表，多个线程同时操作同一桶位的链表可能导致节点指向错误，形成环形链表，从而引发死循环。 为了解决这个问题，JDK1.8 版本的 HashMap 采用了尾插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。 长度取值 HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。并且， HashMap 总是使用 2 的幂作为哈希表的大小。长度采用 2 的幂，是因为二进制位操作&amp;相对于取模运算%能够提高效率。位运算直接对内存数据操作，不需要转换成10进制进行取模运算。 公式（hash % length == hash &amp; (length - 1)成立的前提是Hash表的长度是 2 的 n 次方。 并发安全问题 死锁问题 在 JDK 1.7 及之前的版本中，HashMap 在多线程环境下进行扩容时，由于使用头插法重排链表，多个线程同时操作同一桶位的链表可能导致节点指向错误，形成环形链表，从而引发死循环。 为了解决这个问题，JDK1.8 版本的 HashMap 采用了尾插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。 其他问题 多线程put的时候，size的个数和真正的个数不一样 多线程put的时候，可能会把上一个put的值覆盖掉 当既有get操作，又有扩容操作的时候，有可能数据刚好被扩容换了桶，导致get不到数据 和其他不支持并发的集合一样，HashMap也采用了fail-fast操作，当多个线程同时put和get的时候，会抛出并发异常 扩容机制 触发扩容 HashMap 中的容量阈值（threshold）是由当前数组容量与负载因子相乘得到的capacity * loadFactor。当 HashMap 中的元素数量超过这个阈值时，会触发扩容操作。默认的负载因子为 0.75，数组的初始容量为 16。 扩容过程 在扩容过程中，HashMap 会创建一个新的数组，容量是原数组的两倍。所有原数组的元素都会被重新分配到这个新数组中。 节点迁移 桶元素重新映射：如果桶中只有一个元素，没有形成链表，则将原来的桶引用置为null，同时，将该元素进行rehash即可。 链表重新链接：扩容后的数据迁移实际上是部分的，一些元素保持原位置，而另一些元素则会被迁移到新的位置。这种优化减少了不必要的数据移动，提升了扩容操作的效率。 保持原位置：对于某个元素，如果 hash &amp; oldLength == 0，那么该元素在扩容后的新数组中仍然保留在原位置。 移动到新位置：如果 hash &amp; oldLength != 0，那么该元素在扩容后的新数组中的位置将从 index 变为 index + oldLength。 取消树化：类似于上述链表的重新链接，当重新链接操作完毕后，会判断两个哈希桶上的链表长度是否小于等于6，如果满足条件，会将红黑树取消树化，退化成链表。 示例：旧哈希桶长度为8，有a、b、c、d四个元素，存储在下标为3的哈希桶上。 如果扩容为16，由于hash(a)&amp;8=0;无需移动，其他的不为零的元素移动到index + oldLength = 3 + 8 = 11 为什么HashMap的负载因子设置为0.75？ 性能和空间的平衡： 负载因子过低（例如 0.5），意味着更频繁地扩容，虽然哈希冲突会减少，但空间利用率较低，增加了内存消耗。 负载因子过高（例如 1.0），虽然提高了空间利用率，但增加了哈希冲突的概率，降低了查找和插入的效率。 数学依据： 负载因子选择为0.75（3/4），可以保证临界值 threshold = loacFactor * capacity为整数。 ConcurrentHashMap 底层实现 同HashMap 线程安全 JDK1.8之前： 使用分段锁：在 JDK 1.7 及之前，ConcurrentHashMap 通过分段锁机制来实现线程安全。它将整个哈希表分为多个段（Segment），每个段内部维护一个独立的哈希表和锁。当对 ConcurrentHashMap 进行并发操作时，不同的线程可以同时操作不同的段，从而提高并发性。 Segment 是继承了 ReentrantLock 的子类：每个段都有自己独立的可重入锁，而操作的同步性依赖于 Segment 的锁机制。 JDK1.8之后： 移除了分段锁：在 JDK 1.8 中，ConcurrentHashMap 不再使用分段锁。相反，它直接使用了一个Node&lt;K, V&gt;[]数组来存储键值对，并且通过更细粒度的锁和无锁操作来实现线程安全。 CAS 操作：在插入和删除节点时，ConcurrentHashMap 使用 CAS（Compare-And-Swap）来确保并发安全。例如，当向一个空桶中插入第一个节点时，ConcurrentHashMap 会使用 CAS 操作直接插入，这样可以避免锁的使用。 Synchronized 锁：在一些复杂操作（如链表或红黑树的插入或删除）中，ConcurrentHashMap 使用 synchronized 来锁定特定的桶（Node），而不是整个段或整个哈希表。这种方式虽然引入了锁，但锁的粒度非常小，只会锁定冲突的桶，因此可以大大减少锁竞争。 并发度： JDK1.8之前，最大并发度是Segment的个数，默认是 16。 JDK1.8之后，最大并发度是Node&lt;K, V&gt;[]数组的大小，并发度更大。 为什么key和value都不能为null? null是一个特殊的值，表示没有对象或没有引用。 key为null： 防止异常：如果键为 null，哈希计算过程中会抛出 NullPointerException。 value为null： 简化实现：在并发条件下无需对null值进行特殊处理，减少条件分支，提高可维护性。 避免二义性：如果使用get(key)方法获取的value为null，则可能该 key 存在，但其对应的值是 null；或者该 key 不存在。 保证复合操作的原子性 ConcurrentHashMap 是线程安全的，这意味着它能够保证在多线程环境下对其进行并发读写操作时，不会出现数据不一致的情况。此外，它也避免了 JDK 1.7 及之前版本的 HashMap 中，在多线程操作时可能导致的 死循环问题。然而，线程安全 不等于 原子性，尤其是在涉及多个操作的情况下，ConcurrentHashMap 并不能确保所有的复合操作都是原子性的。 复合操作：是指由多个基本操作（如 put、get、remove、containsKey 等）组合而成的操作。 例如：如果多个线程同时调用 containsKey 进行检查并随后调用 put，在第一个线程判断某个键不存在时，另一个线程可能已经插入了该键，这样第一个线程的操作就会覆盖另一个线程的结果，导致数据不一致。 为了解决复合操作的原子性问题，ConcurrentHashMap 提供了一些内置的原子复合操作方法。这些方法能够确保整个操作过程在并发环境下是原子性的，即要么全部成功，要么全部失败，不会出现中间状态。 putIfAbsent(K key, V value) 如果指定的键 key 尚未存在于 ConcurrentHashMap 中，则将其对应的值设置为 value。该操作是原子性的，能够确保在并发情况下，只有第一个插入的线程成功，后续插入将被忽略。 这种操作特别适合用于避免重复插入。 computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) 只有当指定的键 key 存在时，才会根据提供的 remappingFunction 计算新值并更新。如果键不存在，则不执行任何操作。该方法同样确保了计算和更新的原子性。 Map对比 HashMap和Hashtable的区别 线程是否安全： HashMap 是非线程安全的，Hashtable 是线程安全的,因为 Hashtable 内部的方法基本都经过synchronized 修饰。 效率： 因为线程安全的问题，HashMap 要比 Hashtable 效率高一点。另外，Hashtable 基本被淘汰，不要在代码中使用它； 对 Null key 和 Null value 的支持： HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；Hashtable 不允许有 null 键和 null 值，否则会抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同： ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间（后文中我会结合源码对这一过程进行分析）。Hashtable 没有这样的机制。 HashMap和HashSet的区别 HashSet 底层就是基于 HashMap 实现的。 HashMap HashSet 实现了 Map 接口 实现 Set 接口 存储键值对 仅存储对象 调用 put()向 map 中添加元素 调用 add()方法向 Set 中添加元素 HashMap 使用键（Key）计算 hashcode HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以equals()方法用来判断对象的相等性 HashMap和TreeMap的区别 TreeMap 和HashMap 都继承自AbstractMap ，但是需要注意的是TreeMap它还实现了NavigableMap接口和SortedMap 接口。 实现 NavigableMap 接口让 TreeMap 有了对集合内元素的搜索的能力。 实现SortedMap接口让 TreeMap 有了对集合中的元素根据键排序的能力。默认是按 key 的升序排序，不过我们也可以指定排序的比较器。 ConcurrentHashMap 和 Hashtable 的区别 底层数据结构 JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样采用数组+链表/红黑二叉树。 Hashtable 一直采用的是 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要） ConcurrentHashMap： 移除了分段锁：在 JDK 1.8 中，ConcurrentHashMap 不再使用分段锁。相反，它直接使用了一个Node数组来存储键值对，并且通过更细粒度的锁和无锁操作来实现线程安全。 CAS 操作：在插入和删除节点时，ConcurrentHashMap 使用 CAS（Compare-And-Swap）来确保并发安全。例如，当向一个空桶中插入第一个节点时，ConcurrentHashMap 会使用 CAS 操作直接插入，这样可以避免锁的使用。 Synchronized 锁：在一些复杂操作（如链表或红黑树的插入或删除）中，ConcurrentHashMap 使用 synchronized 来锁定特定的桶（Node），而不是整个段或整个哈希表。这种方式虽然引入了锁，但锁的粒度非常小，只会锁定冲突的桶，因此可以大大减少锁竞争。 Hashtable： 使用全表锁（Synchronized）：Hashtable 通过对每个操作（如 put、get、remove 等）加上 synchronized 来保证线程安全，但它使用的是全表锁，导致每次操作都必须锁住整个对象。这会导致多个线程在访问不同元素时也会被阻塞，增加锁竞争，降低并发性能。 性能瓶颈：全表锁机制使得即使线程操作不同的键，仍会被串行化。在高并发场景中，锁的竞争严重影响吞吐量，导致性能大幅下降。 如何解决Hash冲突？ 1.开放定址法 开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。 常见的开放寻址技术有线性探测、二次探测和双重散列。 这种方法的缺点是可能导致“聚集”问题，降低哈希表的性能。 2.链地址法 每个哈希桶(bucket)指向一个链表。当发生冲突时，新的元素将被添加到这个链表的末尾。 在Java中，HashMap就是通过这种方式来解决哈希冲突的。Java8之前，HashMap使用链表来实现) 从Java 8开始，当链表长度超过一定阈值时，链表会转换为红黑树，以提高搜索效率。 3.再哈希法 当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。 这种方法需要额外的计算，但可以有效降低冲突率。 4.建立公共溢出区 将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。 5.一致性hash 主要用于分布式系统中，如分布式缓存。它通过将数据均匀分布到多个节点上来减少冲突 "},{"title":"【并发编程】全面解析volatile和synchronized关键字","date":"2024-09-10T07:12:31.000Z","url":"/2024/09/10/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90volatile%E5%92%8Csynchronized%E5%85%B3%E9%94%AE%E5%AD%97/","tags":[["volatile","/tags/volatile/"],["synchronized","/tags/synchronized/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"volatile 可见性问题 Java内存模型： 在Java内存模型（JMM）中，每个线程有自己的工作内存（这是一个抽象概念，不同于物理内存中的缓存），用于存储从主内存中读取的变量副本。线程对变量的操作（如读取、写入）通常在其工作内存中进行，而不是直接在主内存中操作。 当一个线程修改了工作内存中的变量副本后，新的值可能不会立即刷新回主内存。这意味着其他线程在其工作内存中读取该变量时，可能仍然看到旧值，而不是最新值。 CPU缓存机制： 在物理层面上，现代CPU使用多级缓存（如L1、L2、L3）来加速内存访问。每个处理器核心可能会将主内存中的数据加载到自己的缓存中，并在缓存中进行操作。 当一个线程运行在一个处理器核心上，并修改了缓存中的数据，其他处理器核心的缓存可能不会立即同步这些修改，导致其他线程看到的是旧数据。这种情况在多核处理器系统中尤为常见。 指令重排序： 为了优化性能，编译器和CPU可能会对指令进行重排序。例如，编译器可能会将一些操作的顺序调整，使得对变量的读取和写入操作不按程序的逻辑顺序执行。 这种重排序可能导致一个线程的操作在另一个线程中以错误的顺序被观察到，特别是在没有适当的同步措施时。例如，线程A可能先执行了对变量的写操作，而线程B却在此之前读取了这个变量的旧值。 变量可见性 volatile 关键字可以强制将变量的更新立即写入主内存，并确保其他线程读取时能看到最新的值。如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 指令重排序 在 Java 中，volatile 关键字除了可以保证变量的可见性，还有一个重要的作用就是防止 JVM 的指令重排序。 如果我们将变量声明为 volatile ，在对这个变量进行读写操作的时候，会通过插入特定的 内存屏障 的方式来禁止指令重排序。 指令重排序的分类： JVM重排序：JVM在将字节码转换为机器码时，可能会进行指令的重新排列。 CPU重排序：处理器在执行机器码指令时，也可能会根据当前的硬件状态（如缓存、流水线等）对指令进行重新排列。 编译器重排序：编译器在编译代码时可能会调整代码的顺序，以生成更高效的字节码。 指令重排序导致的问题 指令重排序在单线程环境中通常不会导致问题，因为编译器和处理器都会确保程序的最终结果与代码的顺序一致。然而，在多线程环境下，指令重排序可能会导致数据可见性和线程安全问题。 可见性问题：由于指令重排序，一个线程可能在另一个线程未完成操作前看到中间状态的变量值，导致读取到不正确的数据。 线程安全问题：当多个线程同时访问和修改共享变量时，如果没有适当的同步机制，指令重排序可能会使得线程间的操作顺序不同于预期，从而引发竞态条件，导致程序行为异常。 synchronized synchronized 是 Java 中的一个关键字，翻译成中文是同步的意思，主要解决的是多个线程之间访问资源的同步性，可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 使用方法 1.修饰实例方法 （锁当前对象实例） 给当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁 。 2.修饰静态方法 （锁当前类） 给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。 这是因为静态成员不属于任何一个实例对象，归整个类所有，不依赖于类的特定实例，被类的所有实例共享。 3.修饰代码块 （锁指定对象/类） 对括号里指定的对象/类加锁： synchronized(object) 表示进入同步代码库前要获得 给定对象的锁。 synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁 构造方法可以使用synchronized吗？ 不能，因为构造方法本来就是线程安全的，不存在同步的构造方法一说。 静态 synchronized 方法和非静态 synchronized 方法之间的调用互斥吗？ 不互斥。因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例的锁。 加锁方式 synchronized关键字对方法加锁是通过方法的flags标志来实现的，而对同步代码块加锁则是通过monitorenter和monitorexit指令来实现的。 同步方法的flags里面多了一个ACC_SYNCHRONIZED标志。当某个线程要访问某个方法是，会检查是否存在ACC_SYNCHRONIZED标识，如果有设置，则需要先获取监视器锁，方法执行后再释放监视器锁。 同步代码块是由monitorenter获取锁，然后monitorexit释放锁。在执行monitorenter之前需要尝试获取锁，如果这个对象没有被锁定，或者当前线程已经拥有了这个对象的锁，那么就把锁的计数器加一。当执行monitorexit指令时，锁的计数器也会减一。当获取锁失败时会被阻塞，一直等待锁被释放。 对象结构分析 要深入理解 synchronized 的实现原理，必须先了解 Java 对象在内存中的布局，尤其是对象头中的 Mark Word。 对象内存结构 在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 对象头包括两部分信息： 标记字段（Mark Word）：用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。synchronized就是依靠Mark Word字段来进行锁升级、获取锁等操作的。 类型指针（Klass Word）：对象指向它的类元数据的指针，JVM可以通过该指针来确定这个对象是哪个类的实例。 实例数据是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 Mark Word Mark Word 不仅存储了对象的基本信息，还承担着锁状态的管理职责，包括锁的获取、升级、释放等操作。正是通过对 Mark Word 的精细控制，Java 虚拟机（JVM）才能高效地实现线程同步机制，从而保障并发环境下的数据一致性和程序的正确性。 在Java中，锁的状态分为四种，分别是无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态。 Mark Word的低两位用于表示锁的状态，分别为\"01\"(无锁状态)、\"01\"(偏向锁状态)、\"00\"(轻量级锁状态)和\"10\"(重量级锁状态)。但是由于无锁状态和偏向锁都是”01\"，所以在低三位引入偏向锁标记位用\"0\"表示无锁，\"1\"表示偏向。 锁升级过程 对于synchronized关键字，一共有四种状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态（级别从低到高）。 偏向锁 偏向锁提升性能的经验依据是：对于绝大部分锁，在整个同步周期内不仅不存在竞争，而且总由同一线程多次获得。偏向锁会偏向第一个获得它的线程，如果接下来的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程不需要再进行同步。这使得线程获取锁的代价更低。 触发条件：如果JVM启动参数没有禁用偏向锁，那么首次进入synchronized块时会自动开启。 偏向锁的获取过程： 初始状态：当一个对象刚创建时，它处于无锁状态。此时，Mark Word 中包含对象的哈希码和其他一些标记信息。 线程尝试加锁：当线程第一次访问这个对象并进入 synchronized 块时，JVM 会将 Mark Word 中的锁标志位设置为偏向锁，并将该线程的 ID 记录在 Mark Word 中。这样，锁就“偏向”于这个线程。 偏向锁加锁成功：如果同一线程再次访问该对象的 synchronized 块，JVM 检查 Mark Word 中的线程 ID，如果与当前线程的 ID 匹配，则认为该线程已经持有了锁，无需执行任何额外操作，直接进入同步块。这避免了加锁和解锁的开销；如果JVM 发现Mark Word 中的线程 ID 与当前线程不符，偏向锁就会被撤销。偏向锁撤销的过程包括： 偏向锁升级为轻量级锁：JVM 会暂停持有偏向锁的线程，撤销偏向锁，将 Mark Word 更新为指向轻量级锁的状态，并将锁记录在新的线程的栈中。 新线程继续尝试获取锁：如果当前没有其他线程竞争锁，新的线程将获取轻量级锁；否则，锁将进一步升级为重量级锁。 偏向锁的释放过程： 解锁时：当持有偏向锁的线程退出 synchronized 块时，锁不会立即释放，Mark Word 中的线程 ID 和偏向标记位保持不变，仍然表示锁偏向于该线程。 锁重入：如果偏向锁的线程再次进入 synchronized 块，可以直接重用这个锁而无需加锁操作。 锁撤销（偏向锁失效）：如果另一线程尝试访问同一个对象的synchronized块，JVM 发现Mark Word 中的线程 ID 与当前线程不符，偏向锁就会升级为轻量级锁。 轻量级锁 轻量级锁是相对基于OS的互斥量实现的重量级锁而言的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用OS的互斥量而带来的性能消耗。 轻量级锁提升性能的经验依据是：对于绝大部分锁，在整个同步周期内都是不存在竞争的。如果没有竞争，轻量级锁就可以使用 CAS 操作避免互斥量的开销，从而提升效率。 触发条件：当有另一个线程尝试获取已被偏向的锁时，偏向锁会升级为轻量级锁。 轻量级锁的加锁过程： 创建锁记录（Lock Record）：当线程进入同步代码块时，JVM 会在当前线程的栈帧中创建一个锁记录（Lock Record）。这个锁记录用于存储对象当前的 Mark Word 的副本，称为 Displaced Mark Word。此时，锁记录中的 _owner 指针会指向对象的 Mark Word。 CAS 尝试加锁：JVM 使用 CAS（Compare-And-Swap）操作尝试将对象头中的 Mark Word 更新为指向线程栈中 Lock Record 的指针。如果 CAS 操作成功，则表明当前线程获取了轻量级锁，锁的标志位（lock state bits）将更新为 00，表示轻量级锁。 加锁成功：若 CAS 操作成功，表示该线程已经拥有了该对象的锁，可以安全地进入同步代码块执行。 加锁失败（锁竞争）：如果 CAS 操作失败，JVM 会检查对象的Mark Word 是否指向当前线程的栈帧中的锁记录： 当前线程已拥有锁： 如果 Mark Word 已经指向当前线程的锁记录，说明该线程已拥有锁，可以直接进入同步代码块继续执行。 锁被其他线程持有： 如果 Mark Word 指向的是其他线程的锁记录，说明锁被其他线程占用。此时，当前线程会尝试通过 自旋 一定次数获取锁。如果在多次自旋后 CAS 仍然失败，轻量级锁会升级为 重量级锁（标志位更新为 10），并将 Mark Word 指向重量级锁的指针。之后，未获得锁的线程将进入阻塞状态，等待锁的释放。 轻量级锁的解锁过程： CAS 释放锁：当线程退出同步代码块时，JVM 会使用 CAS 操作，将线程中锁记录的 Displaced Mark Word 恢复到对象的 Mark Word，即通过 CAS 将对象头的 Mark Word 替换为最初的值。 解锁成功：如果 CAS 操作成功，意味着锁成功释放，整个同步过程完成。 解锁失败（锁竞争）：如果 CAS 操作失败，说明有其他线程正在尝试获取该锁，或者锁已经升级为重量级锁。在这种情况下，JVM 会在释放锁的同时，唤醒等待该锁的阻塞线程。 重量级锁 监视器锁实现 synchronized 关键字用于实现线程同步，而它的底层是通过监视器锁（Monitor）来实现的。无论是对方法加锁还是对同步代码块加锁，JVM 都是依赖 monitor 机制来保证同步。在进入同步代码块之前，线程必须先获取监视器锁。成功获取锁后，锁的计数器增加 1；执行完同步代码后，计数器减少 1。如果线程无法获取锁，它会进入阻塞状态，直到锁被释放。 在Hotspot中，对象的监视器（monitor）锁对象由ObjectMonitor对象实现（C++），其跟同步相关的数据结构如下： 当多个线程同时访问一段同步代码时，首先会进入_EntryList队列中，当某个线程获取到对象的monitor后进入_owner区域并把 monitor 中的_owner变量设置为当前线程，同时 monitor 中的计数器count加一，即获得对象锁。 若持有 monitor 的线程调用wait()方法，将释放当前持有的 monitor，_owner变量恢复为null，_count自减一，同时该线程进入_WaitSet集合中等待被唤醒。若当前线程执行完毕也将释放 monitor(锁)并复位变量的值，以便其他线程进入获取 monitor(锁)。 加解锁 监视器锁monitor本质上是依赖操作系统的 Mutex Lock 互斥量 来实现的，我们一般称之为重量级锁。因为 OS 实现线程间的切换需要从用户态转换到核心态，这个转换过程成本较高，耗时相对较长，因此synchronized效率会比较低。 重量级锁的锁标志位为'10'，指针指向的是monitor对象的起始地址。 只有在使用重量级锁时，才会涉及到 monitor 及线程状态的控制。线程的生命周期分为五个状态，分别是初始状态（new）、运行状态（runnable）、等待状态（waiting）、阻塞状态（blocking）和终止状态（terminated）。 触发条件：当轻量级锁的CAS操作失败，轻量级锁升级为重量级锁。 重量级锁的加锁过程： 线程进入阻塞队列：如果锁已被其他线程持有，后续尝试获取该锁的线程会进入 ObjectMonitor 对象的 _EntryList 队列，处于阻塞状态等待锁。 线程获取锁进入运行状态：成功获取锁的线程会从 _EntryList 中移出，进入运行状态。此时，ObjectMonitor 的 _owner 字段指向该线程，_count 加一。如果线程重入锁，_count 递增。 调用 wait() 方法进入等待状态：线程调用 wait() 时，释放 monitor 锁，进入等待状态，_owner 置为 null，_count 减一。线程进入 _WaitSet 队列，等待 notify() 或 notifyAll() 唤醒。 线程被唤醒后重新竞争锁：被唤醒的线程从 _WaitSet 移至 _EntryList 队列，重新竞争锁。获取锁后，进入运行状态，_owner 指向该线程。 重量级锁的解锁过程： 线程正常退出同步代码块：线程退出同步代码块时，_count 减一。若 _count 为零，表示锁完全释放。 释放锁和唤醒其他线程：如果 _count 为零，_owner 置为 null。如果 _EntryList 队列中有其他线程，JVM 会唤醒其中一个，让其尝试获取锁。 线程退出或等待结束：线程执行完同步代码或被唤醒后，若锁可用，获取锁并继续执行，否则重新进入阻塞队列。锁释放时，_owner 置为 null，其他线程有机会获取锁。 锁优化 自旋锁 获取轻量级锁时：即当一个线程尝试获取一个被其他线程持有的轻量级锁时，会自旋等待锁的持有者释放锁。 在OpenJDK 8中，轻量级锁的自旋默认是开启的，最多自旋15次，每次自旋的时间逐渐延长。如果15次自旋后仍然没有获取到锁，就会升级为重量级锁。 获取重量级锁时：即当一个线程尝试获取一个被其他线程持有的重量级锁时，它会自旋等待锁的持有者释放锁。 在OpenJDK 8中，默认情况下不会开启重量级锁自旋。如果线程在尝试获取重量级锁时，发现该锁已经被其他线程占用，那么线程会直接阻塞，等待锁被释放。如果锁被持有时间很短，可以考虑开启重量级锁自旋，避免线程挂起和恢复带来的性能损失。 自适应自旋：在JDK6之后的版本中，JVM引入了自适应的自旋机制。该机制通过监控轻量级锁自旋等待的情况，动态调整自旋等待的时间。 如果自旋等待的时间很短，说明锁的竞争不激烈，当前线程可以自旋等待一段时间，避免线程挂起和恢复带来的性能损失。如果自旋等待的时间较长，说明锁的竞争比较激烈，当前线程应该及时释放CPU资源，让其他线程有机会执行。 锁消除 锁消除是JVM在编译或即时编译（JIT）过程中通过逃逸分析判断锁对象是否只在单个线程中使用。如果确定该锁对象不会逃逸到其他线程，即它只在当前线程中被使用，JVM会自动移除这些不必要的同步锁，从而减少锁的开销。 锁粗化 锁粗化是指将多个连续的、临近的小范围锁操作合并为一个更大的锁操作，以减少加锁和解锁的频率，从而提高性能。锁粗化的主要目的是避免在短时间内频繁进行锁的获取和释放操作，因为每次加锁和解锁都带有一定的开销。 两者比较 synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！ volatile 关键字是线程同步的轻量级实现，所以 volatile性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 "},{"title":"【并发编程】从AQS机制到同步工具类","date":"2024-08-30T10:14:20.000Z","url":"/2024/08/30/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E4%BB%8EAQS%E6%9C%BA%E5%88%B6%E5%88%B0%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7%E7%B1%BB/","tags":[["AQS机制","/tags/AQS%E6%9C%BA%E5%88%B6/"],["ReentrantLock","/tags/ReentrantLock/"],["CountDownLatch","/tags/CountDownLatch/"],["CyclicBarrier","/tags/CyclicBarrier/"],["Semaphore","/tags/Semaphore/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"AQS机制 Java 中常用的锁主要有两类，一种是 Synchronized 修饰的锁，被称为 Java 内置锁或监视器锁。另一种就是在 JUC 包中的各类同步器，包括 ReentrantLock（可重入锁）、Semaphore（信号量）、CountDownLatch 等。 所有的同步器都是基于AQS机制来构建的，而 AQS 类的核心数据结构是一种名为CLH锁的变体。 CLH锁 CLH锁是一种基于链表的自旋锁，它通过维护一个隐式的等待队列来实现线程的公平性和高效性。CLH锁的核心思想是每个线程在进入临界区时都会在队列尾部排队，并且自旋等待前驱节点的状态变化。CLH 锁的特点是，它将等待线程的状态信息保存在前驱节点中，而不是在本线程中，这样就避免了过多的缓存一致性流量。 隐式双向链表 加锁过程： 初始化：CLH 锁初始化时，Tail 指向一个状态为 false 的空节点。 线程入队： 线程尝试获取锁时，创建一个状态为 true 的新节点，表示正在等待锁。 线程通过 CAS 操作将新节点插入队列尾部，并更新 Tail 指针。 轮询前驱节点状态：线程不断轮询其前驱节点的状态，直到前驱节点的状态变为 false，表示可以获取锁。 解锁过程： 释放锁：线程完成临界区访问后，将当前节点的状态设置为 false，表示释放锁。 后继节点获取锁：后继节点检测到前驱节点状态变化，获取锁并进入临界区。 AQS对CLH锁的改造 CLH锁存在缺点： 自旋操作，当锁持有时间长时会带来较大的 CPU 开销。 基本的 CLH 锁功能单一，不改造不能支持复杂的功能。 Java 的 AbstractQueuedSynchronizer（AQS）借鉴了 CLH 锁的思想，并在此基础上做了诸多改进，使其更适合构建高效、可扩展的同步器。以下是 AQS 对 CLH 锁所做的一些主要改造： 显式双向链表 AQS 使用了显式的双向链表来维护等待队列，而不是隐式的单向链表。这样改进的好处是，它允许 AQS 更方便地处理队列中的节点操作，比如取消、唤醒特定节点等。 AQS 加锁过程： 初始化：AQS 初始化时，等待队列为空，head 和 tail 指针均为 null，state 表示锁的状态。 线程入队： 当线程尝试获取锁时，AQS 会首先检查锁的状态（state）。如果锁已被其他线程占用，当前线程将无法直接获取锁。 线程会创建一个新的节点（Node），表示自己需要等待锁的释放。通过 CAS 操作，线程原子性地将该节点插入到等待队列的尾部，并更新 tail 指针指向该新节点。 如果队列为空，当前节点会成为队列中的第一个节点（即 head 节点）。 线程阻塞：如果当前线程无法立即获取锁（state 不为 0），线程会进入阻塞状态，调用 LockSupport.park() 挂起自己，等待其他线程释放锁。 线程唤醒：当持有锁的线程释放锁时，会通过LockSupport.unpark()唤醒等待队列中的下一个线程。 AQS 解锁过程： 释放锁：持有锁的线程完成临界区的操作后，会调用 release(int arg) 方法将 state 变量设置为 0，表示锁已释放。 唤醒后继节点：AQS 会调用 LockSupport.unpark(Thread) 来唤醒后继节点的线程，使其从 park() 的阻塞状态中恢复。 后继节点获取锁：被唤醒的后继节点线程会重新尝试获取锁，通过 CAS 操作将 state 从 0 更新为 1。如果成功获取锁，线程将进入临界区执行任务。 多种同步模式 AQS 提供了独占锁（exclusive）和共享锁（shared）两种模式。例如，ReentrantLock 使用的是独占模式，而 Semaphore 和 CountDownLatch 使用的是共享模式。 一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现钩子方法中的tryAcquire-tryRelease或tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 同步状态变量 AQS 使用一个 int 型变量（称为同步状态 state）来表示锁的状态，而不是像 CLH 锁那样依赖前驱节点的布尔变量。AQS 通过 CAS 操作来修改这个状态，确保线程安全。 ReentrantLock 的可重入性：ReentrantLock 通过内部的 state 变量表示锁的占用状态。初始 state 为 0，表示未锁定。当线程 A 调用 lock() 时，通过 tryAcquire() 方法尝试获取锁并将 state 加 1。若获取成功，线程 A 可以多次获取同一锁，state 会累加，体现可重入性。释放时，state 减 1，直到回到 0，锁才真正释放，其他线程才有机会获取锁。 CountDownLatch 的倒计时：CountDownLatch 使用 state 变量表示剩余的倒计时数。初始 state 为 N，表示 N 个子线程。每个子线程执行完任务后调用 countDown()，state 减 1。所有子线程执行完毕（state 变为 0）后，主线程被唤醒，继续执行后续操作。 实现同步器 AQS 的设计：AQS 提供了一个基础的框架和队列管理功能，但具体的同步逻辑并没有在 AQS 中实现，而是留给具体的同步器来定义。这就是模板方法模式的典型应用：AQS 提供了模板方法，这些模板方法依赖于子类实现的钩子方法。 重写钩子方法：具体的同步器，如 ReentrantLock、Semaphore，会通过其定义的内部类 Sync （继承自 AQS）来重写这些钩子方法。这些重写的方法决定了锁的行为（如是否公平、是否可重入、许可的数量等）。 什么是钩子方法？ 钩子方法是一种被声明在抽象类中的方法，一般使用 protected 关键字修饰，它可以是空方法（由子类实现），也可以是默认实现的方法。模板设计模式通过钩子方法控制固定步骤的实现。 除了这些钩子方法，AQS类中其他方法都是final关键字修饰的，无法被重写。 常见同步工具类 ReentrantLock ReentrantLock是一种可重入的互斥锁，允许同一个线程在持有锁的情况下多次获取锁。它提供了更灵活的锁机制，可以显式地获取和释放锁，还支持公平锁和非公平锁的选择。通常用来实现线程间的同步，防止多个线程同时访问共享资源。 ReentrantLock 有一个内部类 Sync，Sync 继承 AQS，添加锁和释放锁的大部分操作实际上都是在 Sync 中实现的。Sync 有公平锁 FairSync 和非公平锁 NonfairSync 两个子类。 公平锁/非公平锁 抽象类Sync继承自AbstractQueuedSynchronizer，实现了AQS的部分方法； NonfairSync继承自Sync，实现了Sync中的方法，主要用于非公平锁的获取； FairSync继承自Sync，实现了Sync中的方法，主要用于公平锁的获取。 可以通过构造方法实现公平锁或非公平锁。 公平锁和非公平锁只有两处不同： 公平锁：当调用lock()时，会先检查等待队列中是否有排队的线程。如果有排队线程，当前线程会加入队列排队；如果没有等待线程且锁没有被占用，则通过 CAS 操作直接获取锁。 非公平锁：当调用lock()时，即使等待队列中有线程，非公平锁仍会尝试直接获取锁。如果CAS获取失败，则调用nonfairTryAcquire()方法，这时如果发现锁已经释放（state == 0），非公平锁会再次通过CAS立即尝试获取锁，不考虑等待队列中的其他线程。 可中断锁 可中断锁与不可中断锁的区别在于：线程尝试获取锁操作失败后，在等待过程中，如果该线程被其他线程中断了，它是如何响应中断请求的。lock方法会忽略中断请求，继续获取锁直到成功；而lockInterruptibly则直接抛出中断异常来立即响应中断，由上层调用者处理中断。 lock()适用于锁获取操作不受中断影响的情况，此时可以忽略中断请求正常执行加锁操作，因为该操作仅仅记录了中断状态（通过Thread.currentThread().interrupt()操作，只是恢复了中断状态为true，并没有对中断进行响应)。 如果要求被中断线程不能参与锁的竞争操作，则应该使用lockInterruptibly方法，一旦检测到中断请求，立即返回不再参与锁的竞争并且取消锁获取操作（即finally中的cancelAcquire操作） 可重入锁 ReentrantLock对于可重入锁的实现依赖于AOS类，它的主要作用就是设置和获取持有独占锁的线程： 可重入锁获取和释放锁的流程如下： 线程获取锁的过程： 当一个线程首次获取锁时，AQS 会将state（代表锁的状态）从 0 增加到 1，表示锁已被占用，并通过setExclusiveOwnerThread(当前线程)方法将当前线程设置为锁的持有者。 如果当前线程尝试再次获取锁（即可重入），会检查exclusiveOwnerThread是否等于当前线程，如果是，则允许线程再次获取锁。此时，AQS（AbstractQueuedSynchronizer）会将state的值递增，记录线程重入的次数。 线程释放锁的过程： 当线程释放锁时，AQS 会将state的值递减。每释放一次，state的值减少 1，直到state减为 0 时，才表示锁被完全释放。 当锁被完全释放时，AQS 会将state减至 0，随后通过setExclusiveOwnerThread(null)方法清除锁的持有者，并唤醒等待队列中的下一个线程尝试获取锁。 对比synchronized 两者都是可重入锁： 可重入锁也叫递归锁，指的是线程可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果是不可重入锁的话，就会造成死锁。 获取和释放锁的动作不同： synchronized 是依赖于 JVM 实现的。当线程进入synchronized代码块或方法时，会自动获取锁，退出时自动释放锁。如果在synchronized代码块中抛出了异常，锁会自动释放。 ReentrantLock 是 JDK 层面实现的，也就是 API 层面。需要lock()和unlock()方法配合 try/finally语句块来获取和释放锁。如果在ReentrantLock锁定的代码中抛出了异常，需要在finally块中手动释放锁，以确保锁的释放。 ReentrantLock增加了一些高级功能：相比synchronized，ReentrantLock增加了一些高级功能。主要来说主要有三点： 等待可中断 : ReentrantLock提供了一种能够中断等待锁的线程的机制，通过 lock.lockInterruptibly() 来实现这个机制。也就是说正在等待的线程可以选择放弃等待，改为处理其他事情。 可实现公平锁 : ReentrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。ReentrantLock默认情况是非公平的，可以通过 ReentrantLock类的ReentrantLock(boolean fair)构造方法来指定是否是公平的。 可实现选择性通知（锁可以绑定多个条件）: synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()方法。 CountDownLatch CountDownLatch是一个计数器，它允许一个或多个线程等待其它线程完成操作后再继续执行，通常用来实现一个线程等待其它多个线程完成操作之后再继续执行的操作。 CountDownLatch内部维护了一个计数器，该计数器通过CountDownLatch的构造方法指定。当调用await()方法时，它将一直阻塞，直到计数器变为0。当其它线程执行完指定的任务后，可以调用countDown()方法将计数器减一。当计数器减为0，所有的线程将同时被唤醒，然后继续执行。 常用方法 CountDownLatch(int count)：CountDownLatch的构造方法，可通过count参数指定计数次数，但是要大于等于0，小于0会抛IIegalArgumentException异常。 void await()：如果计数器不等于0，会一直阻塞（在线程没被打断的情况下）。 boolean await(long timeout,TimeUnit unit)：除非线程被中断，否则会一直阻塞，直至计数器减为0或超出指定时间timeout，当计数器为0返回true，当超过指定时间，返回false。 void countDown()：调用一次，计数器就减1，当等于0时，释放所有线程。如果计数器的初始值就是0，那么就当没有用CountDownLatch吧。 long getCount()：返回当前计数器的数量，可以用来测试和调试。 使用实例 定义线程任务，实现Runnable接口 定义测试类，使用for循环执行任务，知道任务结束完毕后打印结果。 执行结果： 可以发现，当所有任务执行完毕后，才执行了测试类后续的打印任务。 但是如果使用构造函数创建了4个计数new CountDownLatch(4)，但实际只有3个线程，则测试类阻塞，无法打印结果。 CyclicBarrier CyclicBarrier是一个同步屏障，它允许多个线程相互等待，直到到达某个公共屏障点，才能继续执行。通常用来实现多个线程在同一个屏障处等待，然后再一起继续执行的操作。 CyclicBarrier也维护了一个类似计数器的变量，通过CyclicBarrier的构造函数指定，需要大于0，否则抛IllegalArgumenException异常。当线程到达屏障位置时，调用await()方法进行阻塞，直到所有线程到达屏障位置时，所有线程才会被释放，而屏障将会被重置为初始值以便下次使用。 常用方法 CyclicBarrier(int parties)：CyclicBarrier的构造方法，可通过parties参数指定需要到达屏障的线程个数，但是要大于0，否则会抛IllegalArgumentException异常。 CyclicBarrier(int parties,Runnable barrierAction)：另一个构造方法，parties作用同上，barrierAction表示最后一个到达屏障点的线程要执行的逻辑。 int await()：表示线程到达屏障点，并等待其它线程到达，返回值表示当前线程在屏障中的位置（第几个到达的）。 int await(long timeout,TimeUnit unit)：与await()类似，但是设置了超时时间，如果超过指定的时间后，仍然还有线程没有到达屏障点，则等待的线程会被唤醒并执行后续操作。 void reset()：重置屏障状态，即将屏障计数器重置为初始值。 int getParties()：获取需要同步的线程数量。 int getNumberWaiting()：获取当前正在等待的线程数量。 使用实例 定义线程执行的任务，当线程执行完打印任务后，阻塞等待其他线程。 定义最终执行的业务逻辑 定义测试类，当所有线程执行到屏障后，触发最终的业务逻辑。 执行结果： 对比CountDownLatch CyclicBarrier维护线程的计数，而CounDownLatch维护任务的计数。 可重用性：两者最明显的差异就是可重用性。CyclicBarrier所有线程都到达屏障后，计数会重置为初始值。而CountDownLatch永远不会重置。 Semaphore Semaphore是一个计数信号量，它允许多个线程同时访问共享资源，并通过计数器来控制访问数量。它通常用来实现一个线程需要等待获取一个许可证才能访问共享资源，或者需要释放一个许可证才能完成的操作。 Semaphore维护了一个内部计数器（许可permits），主要有两个操作，分别对应Semaphore的acquire和release方法。acquire方法用于获取资源，当计数器大于0时，将计数器减1；当计数器等于0时，将线程阻塞。release方法用于释放资源，将计数器加1，并唤醒一个等待中的线程。 常用方法 Semaphore(int permits)：构造方法，permits表示Semaphore中的许可数量，它决定了同时可以访问某个资源的线程数量。 Semaphore(int permits,boolean fair)：构造方法，当fair为ture，设置为公平信号量。 void acquire()：获取一个许可，如果没有许可，则当前线程被阻塞，直到有许可。如果有许可该方法会将许可数量减1。 void acquire(int permits)：获取指定数量的许可，获取成功同样将许可减去指定数量，失败阻塞。 void release()：释放一个许可，将许可数加1。如果有其他线程正在等待许可，则唤醒其中一个线程。 void release(int n)：释放n个许可。 int availablePermits()：当前可用许可数。 使用实例 信号量的构造方法传入参数为5，设置六个进程获取这5个资源。 返回结果： 可以发现同一时刻只有五个线程获取到资源，当有资源释放时（车牌号5 驶出），其他线程才能获取资源（车牌号6 进入）。 "},{"title":"【单例模式】深入三大单例对象创建方式","date":"2024-08-21T11:27:02.000Z","url":"/2024/08/21/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["单例模式","/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 单例模式是一种创建型设计模式，它确保一个类在内存中只有一个实例，并提供全局访问点来共享该实例。在程序中，如果需要频繁使用同一个对象且它们的作用相同，单例模式可以避免频繁创建对象，减少内存开销。 单例模式主要有两种类型： 懒汉式：在需要使用时才去创建该对象的单例 饿汉式：在类加载时就已经创建好该单例对象，等待被程序使用 2.懒汉式 懒汉式创建对象的方法是在程序使用对象前，判断对象是否为空（是否被实例化），如果为空则创建该对象的实例，如果不为空直接返回该对象的实例即可。 非线程安全 使用 static 关键字修饰对象，表示该对象属于类本身，而不是类的某个实例。在应用程序的整个生命周期中，该类只会存在一个这样的实例。当多线程访问该类时，可以通过类获取这个静态对象，从而避免重复创建新对象。 线程安全 上述方法在多线程创建对象的时候是有问题的。如果两个线程同时判断singleton为空，那么它们都会执行singleton = new Singleton();语句创建一个实例，这就变成双例了。 解决方法也很简单，给获取实例的方法getInstance()加锁即可。 但是这种方法也存在问题：每次去获取对象都需要获取互斥锁，在高并发场景下，可能会多线程阻塞的问题。 优化方案是：如果没有实例化对象则加锁创建，如果已经实例化了，则不需要加锁，直接获取实例。 双重校验锁 在单例模式中，双重校验锁可以确保只有在第一次访问单例对象时才会进行实例化，并且在多线程环境下能够保证单例对象的唯一性。 详细解释下这段代码的执行流程： 多个线程执行getInstance()方法后，进入if (singleton == null)语句块，判断类是否已经被实例化了，如果已经被实例化，直接返回对象，无需继续创建对象。 如果 singleton 为 null，多个线程争抢该锁进行实例化，同一时刻只有一个线程可以进入同步代码块去创建对象。 再次判断该类是否被实例化，这一步确保了上一个线程在进入同步块后创建了对象，当前线程也不会重复创建实例。 防止指令重排 尽管我们使用双重校验锁的方案保证了线程安全并提升了性能，但由于JVM指令重排序的存在，在多线程创建单例时仍然可能存在问题。 当JVM执行new Singleton()创建一个对象时，会经过三个步骤： 为singleton分配内存空间。 初始化singleton对象，将对象的成员变量赋值。 将内存空间的引用赋值给 singleton 变量。 当发生指令重排序时，这个顺序可能会发生变化，导致未完成初始化（即对象的构造方法可能尚未执行完毕，某些属性可能未被赋值）的对象被其他线程访问，如果这些线程使用了该对象的成员变量，就会导致NPE。 具体场景： 当线程A进入同步块并执行singleton= new Singleton()时，由于指令重排，创建对象的顺序变成： 为singleton分配内存空间。 将内存空间的引用赋值给 singleton 变量。 初始化singleton对象，将对象的成员变量赋值。 这时候线程B在第一次检查singleton == null时得到的结果是false，因为instance已经不再是null（已经指向了内存地址），于是直接返回了这个未初始化的半成品对象。 由于singleton指向的对象尚完成初始化，当线程B在访问该对象尚未初始化的成员变量时，就会导致空指针异常NullPointerException。 解决方案： 只需加上volatile关键字即可，使用volatile修饰变量，可以保证其指令执行的顺序与程序指明的顺序一致，不会发生顺序变换。 完整代码如下： 3.饿汉式 饿汉式在类加载时就已经创建好单例对象，因此在程序调用时可以直接返回该对象。也就是说，我们在调用时直接获取单例对象即可，而不需要在调用时再去创建实例。 由于饿汉式在类加载过程中就由JVM创建了单例，因此不存在线程安全问题。 4.破坏单例模式 无论是使用了双重校验锁+volatile关键字的懒汉式还是饿汉式，都无法防止反射和序列化破坏单例模式（创建多个对象），具体演示代码如下： 反射破坏单例模式 这个原理很简单，利用反射可以强制访问类的私有构造器，从而创建另一个对象 序列化破坏单例模式 首先使用将单例对象序列化为文件流或其他形式，再使用反序列化的手段从流中读取对象。 当使用readObject()方法时，一定会创建一个新的对象。 5.枚举类 枚举类不仅能确保线程安全性，还可以防止反序列化破坏和反射攻击。 枚举类的优点： 线程安全：枚举类的实例在 JVM 加载时由 JVM 保证，只会实例化一次，因此枚举的单例实现天然是线程安全的。 防止反序列化破坏：默认情况下，反序列化一个枚举类型时，会通过类加载器加载枚举类，从而保证每个枚举类型在 JVM 中仅存在一个实例。因此枚举类型可以防止反序列化导致的单例破坏。 防止反射攻击：反射攻击是指通过反射机制来调用私有构造器从而创建多个实例的情况，但在枚举类型中，这种操作会抛出 IllegalArgumentException，从而有效地防止了反射攻击。 6.对比 懒汉式 实例化时机：在第一次调用getInstance()方法时，才会创建单例实例。这意味着如果程序一直没有调用这个方法，单例实例将永远不会被创建。 优点：延迟加载（Lazy Loading），只有在需要时才创建实例，节省了系统资源。 缺点：首次创建实例时可能会有性能开销。在多线程环境下，如果没有适当的同步机制，可能会导致线程安全问题，如重复创建实例。 饿汉式 实例化时机：在类加载时就创建单例实例。无论是否调用getInstance()方法，类加载时都会创建实例。 优点：实现简单，类加载时就完成了实例化，避免了多线程同步问题，因为JVM在类加载时确保了线程的安全性。 缺点：即使单例实例从未被使用，也会在程序启动时创建，可能会浪费系统资源，尤其是在实例化过程较重或单例实例不常用的情况下。 枚举类 实例化时机：在枚举类被加载时创建单例实例，与饿汉式类似，枚举类型的实例会在类加载时被创建，并且JVM会确保只有一个实例存在。 优点： 线程安全：枚举类型在Java中是天然线程安全的，JVM保证了枚举实例的唯一性，无需额外的同步控制。 防止反序列化破坏单例：枚举类在反序列化时不会创建新的实例，保证了单例的唯一性。 防止反射攻击：枚举类在Java中不允许通过反射创建实例，因此可以防止反射攻击破坏单例。 缺点：如果单例类需要继承其他类或实现某些接口，枚举类不太适合。 "},{"title":"【Redis集群】集群原理最全解析","date":"2024-08-12T03:00:01.000Z","url":"/2024/08/12/%E3%80%90Redis%E9%9B%86%E7%BE%A4%E3%80%91%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%E9%9B%86%E7%BE%A4%E5%8E%9F%E7%90%86/","tags":[["主从集群","/tags/%E4%B8%BB%E4%BB%8E%E9%9B%86%E7%BE%A4/"],["哨兵机制","/tags/%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/"],["分片集群","/tags/%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4/"]],"categories":[["Redis","/categories/Redis/"]],"content":"主从集群 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。 数据同步概念 Replication Id和offset 在从节点发起数据同步的请求中，有两个重要的属性： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid。 offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id和offset，master才可以判断到底需要同步哪些数据。 当从节点发起主从同步请求时，主节点会判断从节点的replid是否一致，如果不一致，说明是第一次请求数据同步。 repl_baklog缓冲区 repl_baklog缓冲区是主从同步的重要机制，主节点在生成RDB文件期间会将命令记录到这个环形缓冲区中。 该缓冲区用于增量同步，确保从节点的偏移量与主节点保持一致。然而，如果从节点宕机且重启时缓冲区的数据已被覆盖，从节点就无法通过缓冲区恢复全部数据，导致数据不一致。 第一次数据同步 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点。由于生成RDB文件的过程是异步的，主节点同时会持续记录在生成RDB文件期间产生的所有命令。 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。加载完成后，从节点获取缓存区的命令，执行命令同步数据。 全量同步 全量同步：是指主节点生成RDB文件并发送给从节点，从节点清空本地数据并加载RDB文件的过程。它通常发生在从节点首次连接到主节点或数据不一致的情况下。 全量同步的发生场景有两种： 从节点首次连接主节点： 当一个新的从节点第一次连接到主节点时，它没有任何数据副本。因此，需要进行全量同步来获取主节点的完整数据集。全量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。 主节点缓冲区超出容量： 主节点的repl_baklog缓冲区大小有上限，写满后会覆盖最早的数据。如果从节点断开时间过久，导致尚未备份的数据被覆盖，则主节点不能基于缓冲区做数据同步，只能再次使用全量同步获取RDB的完整数据集。 全量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。 增量同步 增量同步：是指主节点在RDB文件生成期间记录的所有命令（写操作）被存储在replication backlog缓冲区中，并在全量同步完成后发送给从节点，从节点执行这些命令的过程。增量同步用于保持主从节点之间的数据一致性。 增量同步的发生场景有两种： 全量同步后的持续增量同步： 在从节点完成初次的全量同步之后，主节点和从节点之间需要保持数据一致性。从节点会不断接收主节点的增量数据以更新其自身的数据状态。 从节点宕机重启后的增量同步： 当从节点因为故障、宕机或其他原因暂时失联，然后重新启动并重新连接到主节点时，主节点会尝试通过增量同步来恢复数据同步的状态。 增量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果一致，说明从节点不是第一次申请数据同步了（即从节点之前进行了全量同步），主节点返回continue，允许从节点获取repl_baklog缓冲区的命令 从节点持续获取缓存区的命令，执行命令同步数据。 优化策略 可以从以下几个方面来优化Redis主从集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘I/O Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘I/O 适当提高repl_baklog缓存区的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 哨兵机制 在主从集群中，slave节点即使宕机，也可以从master节点上恢复数据。然而，如果master节点宕机，即使master节点做了持久化处理，在其重启后虽然能够恢复部分数据，但在重启和故障恢复的过程中，仍然可能会丢失大量数据，这对系统来说是不可接受的。 因此，为了解决上述问题，在主从集群的基础上引入了哨兵机制。哨兵机制的核心作用是监控主从集群中的各个节点，并在检测到master节点宕机时，自动从slave节点中选举一个新的master节点。 哨兵（Sentinel）机制的作用： 服务状态监控： Sentinel会不断检查集群中的master和slave节点是否按预期工作 自动故障恢复： 如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知Redis客户端： Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 服务状态监控 Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： 主观下线： 如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 客观下线： 若超过指定数量(quorum)的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 自动故障恢复 选举新master节点 当sentinel检测到master节点客观下线时，需要在集群中选择一个slave节点作为新的master，选择依据是这样的： 节点断开时间长短：首先会判断slave节点与master节点断开时间长短，如果超过指定值down-after-milliseconds * 10则会排除该slave节点 优先级判断：slave从节点有slave-priority参数，越小优先级越高，如果是0则永不参与选举 数据同步状态：如果从节点优先级一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 节点 ID 大小：最后是判断slave节点的运行id大小，越小优先级越高， 进行故障转移 当节点2（master节点）故障后，sentinel选举节点1为新的master节点，故障转移步骤如下: sentinel给备选的节点1发送slaveof no one命令，让节点1成为master sentinel给其它所有的slave节点发送slaveof 192.168.150.101 7002命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当节点2恢复后会自动成为新的master的slave节点 通知Redis客户端 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。 在pom文件中引入redis的starter依赖 然后在配置文件application.yml中指定sentinel相关信息 配置读写分离 这里的ReadFrom是配置Redis的读取策略，是一个枚举，包括下面选择: MASTER： 从主节点读取 MASTER_PREFERRED： 优先从master节点读取，master不可用才读取replica REPLICA： 从slave(replica)节点读取 REPLICA_PREFERRED： 优先从slave(replica)节点读取，所有的slave都不可用才读取master 分片集群 主从复制和哨兵机制虽然解决了Redis的高可用性和高并发读的问题，但仍然面临以下两个挑战： 海量数据存储的问题：单个Redis实例的内存和存储容量有限，无法处理海量数据。 高并发写入的问题：单个主节点在高并发写入的场景下容易成为性能瓶颈。 Redis中的分片集群（Sharded Cluster）是一种将数据分布在多个Redis节点上的方式。通过将数据水平分片，分片集群能够在数据量增加时提升集群的存储容量，同时将写入压力分散到多个master节点上，提升整体性能。 Redis 分片集群的核心作用： 数据水平扩展： 通过将数据分片存储在多个节点上，Redis 集群能够扩展到多个实例，以应对大规模数据存储和高并发请求。 负载均衡： 将请求均匀分布到不同的分片节点上，避免单点压力过大，确保系统性能的稳定性。 高可用性： 通过主从复制和自动故障恢复机制，Redis 集群能够在某个节点发生故障时，继续提供服务，确保系统的高可用性。 重要概念 散列插槽 Redis 集群通过哈希槽（Hash Slot）机制来分配数据到不同的分片节点上。整个哈希空间分为 16384 个槽，每个键根据其哈希值被分配到一个特定的槽中，而槽则由集群中的各个matser节点持有。 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： 如果key中包含&#123;&#125;，且&#123;&#125;中至少包含1个字符，&#123;&#125;中的部分是有效部分 如果key中不包含&#123;&#125;，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{modox}num，则根据modox计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是插槽的slot值。 Redis客户端如何进行数据访问？ 根据键的哈希值确定数据所在的分片节点，然后直接与该节点通信。 如何将同一类数据保存在同一个Redis节点上？ 只需设置一个统一的有效部分，如{shopId} 配置纪元 配置纪元的作用是标识和跟踪集群配置的版本，确保集群中的所有节点在主节点故障转移和配置变更时保持一致。 1.配置纪元是只增不减的整数： 每个主节点都有一个自身维护的配置纪元 (clusterNode.configEpoch)，表示该主节点的版本。这个配置纪元是集群变更时用于标识和协调的关键因素。 每个主节点的配置纪元都不同，以确保集群内的节点可以正确识别和处理最新的配置变更。 2.从节点会复制主节点的配置纪元： 当从节点与其对应的主节点同步时，它会复制该主节点的配置纪元。这样在主节点发生故障时，从节点可以使用这个配置纪元参与选举并成为新的主节点。 3.全局配置纪元： 整个集群维护一个全局的配置纪元 (clusterState.currentEpoch)，记录集群内所有主节点的配置纪元中的最大版本号。这个全局纪元会在集群发生关键事件（如故障转移、添加/删除节点）时增加，以确保集群状态的一致性。 4.选举时选择纪元数最大的从节点： 在故障转移过程中，集群会优先选择配置纪元最大的从节点作为新的主节点。因为这个从节点的数据更可能是最新的，并且它在选举中更有可能获得其他主节点的支持。 服务状态监控 Redis分片集群的各个节点通过ping/pong进行消息通信，转播槽的信息和节点状态信息，故障发现也是通过这个动作实现的，类似于sentinel，有主观下线和客观下线。 主观下线（PFAIL）： 集群中的每个节点都会定期通过 PING-PONG 消息与其他节点通信。如果一个节点在指定时间内没有响应其他节点的 PING 请求，该节点会被标记为主观下线。 客观下线（FAIL）： 如果多个节点都将同一个节点标记为 PFAIL，那么通过投票机制，该节点将被标记为客观下线（FAIL）。这个状态会在集群中广播，所有节点都认同该节点已不可用。 故障恢复 选举新的master节点 Redis 分片集群和 Sentinel 机制在选举新的 master 节点时规则基本相同，唯一的区别在于节点断开时间的处理方式不同。 节点断开时间长短：每个从节点检查与故障主节点的断线时间，断开时间超过cluster-node-timeout * cluster-slave-validity-factor则取消资格。cluster-slave-validity-factor : 默认是10 优先级判断：slave从节点有slave-priority参数，越小优先级越高，如果是0则永不参与选举 数据同步状态：如果从节点优先级一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 配置纪元： 在故障转移过程中，集群会优先选择配置纪元最大的从节点作为新的主节点。因为配置纪元越大的从节点，数据更可能是新的。 进行故障转移 当新的 master 节点选举完成后，Redis 集群会自动进行故障转移，具体包括以下步骤： 提升新的 master 节点：Redis 集群通过内部命令 SLAVEOF NO ONE 将选中的从节点提升为新的 master 节点。 更新哈希槽映射：Redis 集群会自动更新哈希槽与节点的映射关系，新的 master 节点将执行 CLUSTER DELSLOTS 操作撤销故障主节点负责的槽，并执行 CLUSTER ADDSLOTS 把这些槽委派给自己。 重新配置和广播：Redis 集群将剩余的从节点重新配置为新 master 节点的从节点，并广播新的 master 信息给所有节点，确保集群内所有节点都更新哈希槽映射，并将新 master 的信息同步到其他节点。 节点重连：如果故障的 master 节点恢复上线，它通常会被重新配置为新的 master 的从节点，并同步数据以确保与新 master 保持数据一致性。 通知Redis客户端 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致。 引入redis的starter依赖 配置分片集群地址 配置读写分离 与哨兵模式相比，只有yaml配置文件的配置方式存在差异，如下： 集群伸缩 Redis 集群提供了灵活的节点扩容和收缩方案。在不影响集群对外服务的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容，对节点进行灵活上下线控制，原理可抽象为槽和对应数据在不同节点之间灵活移动。 集群扩容 1.添加节点： Redis分片集群提供了为现有集群添加新节点的，命令如下： 如果需要直接指定新节点为某一master的从节点，可使用如下命令： 2.迁移插槽： 可通过reshard命令将当前节点的散列插槽分配给其他节点。 接着Redis会提示需要移动多少插槽，自行输入即可。 然后需要输入接收插槽的节点ID，确认后即可实现插槽的迁移 3.添加从节点： 由于新的master节点相比其他主节点目前还没有从节点，因此该节点不具备故障转移的能力。 可以在从节点下使用cluster replicate命令为主节点添加对应从节点（在分片集群下slaveof命令添加从节点的操作不再支持）。 从节点内部除了对主节点发起全量复制之外，还需要更新本地节点的集群相关状态。 集群缩容 1.迁移插槽： 缩容操作需要非常谨慎，因为如果下线的节点持有插槽，直接删除可能会引起数据一致性问题，因此需要将槽迁移给其他节点后才能安全下线，流程同上。 接着Redis会提示需要移动多少插槽，自行输入即可。 然后需要输入接收插槽的节点ID，确认后即可实现插槽的迁移 2.忘记节点： 在一个可用的节点上执行删除节点的命令： 手动故障转移 在 Redis 集群中，手动故障转移允许管理员主动介入，以便在发现主节点故障时，迅速将其替换为副本节点，确保系统的持续可用性和稳定性。 此外，手动数据迁移还支持三种不同模式： 缺省： 默认的流程，如图1~6步 force： 省略了对offset的一致性校验 takeover： 直接执行第5步，忽略数据一致性、忽略master状态和其它master的意见 "},{"title":"【JVM】深入JIT优化机制","date":"2024-08-08T12:31:45.000Z","url":"/2024/08/08/%E3%80%90JVM%E3%80%91%E6%B7%B1%E5%85%A5JIT%E4%BC%98%E5%8C%96%E6%9C%BA%E5%88%B6/","tags":[["JIT","/tags/JIT/"]],"categories":[["JVM","/categories/JVM/"]],"content":"1.JIT优化技术 在将高级语言转化为计算机可识别的机器语言时，常用的两种方式是编译和解释。Java在编译过程中，首先将代码编译成字节码。但是，字节码并不能直接在机器上执行。因此，JVM中内置了解释器（Interpreter），它在运行时将字节码逐行翻译成机器码并执行。 然而，解释器的执行方式是一边翻译，一边执行，导致执行效率较低。为了提高效率，HotSpot JVM引入了JIT（Just-In-Time）编译技术。 有了JIT技术后，JVM仍然通过解释器进行初始执行。但当JVM发现某个方法或代码块被频繁执行时，它将其标记为“热点代码”（Hot Spot Code）。JIT随后将这些热点代码编译为机器码，并进行优化。优化后的机器码被缓存起来，以便下次直接使用，从而显著提升执行效率。 2.热点检测 上面我们说过，要想触发JIT，首先需要识别出热点代码。目前主要的热点代码识别方式是热点探测，有以下两种 基于采样的方式探测： 周期性检测各个线程的栈顶，发现某个方法经常出现在栈顶，就认为是热点方法。好处就是简单，缺点就是无法精确确认一个方法的热度。容易受线程阻塞或别的原因千扰热点探测。 基于计数器的热点探测： 采用这种方法的虚拟机会为每个方法，甚至是代码块建立计数器，统计方法的执行次数，某个方法超过阀值就认为是热点方法，触发JIT编译。 在HotSpot虚拟机中使用的是第二种一一基于计数器的热点探测方法，因此它为每个方法准备了两个计数器: 方法调用计数器和回边计数器。 方法计数器。顾名思义，就是记录一个方法被调用次数的计数器 回边计数器。是记录方法中的for或者while的运行次数的计数器 3.编译优化 逃逸分析 全局逃逸：对象超出了方法或线程的范围，比如被存储在静态字段或作为方法的返回值。 如我们新建的staticObject就是全局逃逸的。以及下面的方法中的sb对象，也是全局逃逸的。 参数逃逸： 对象被作为参数传递或被参数引用，但在方法调用期间不会全局逃逸。 如传递到methodB中的param对象，就是发生了参数逃逸的。因为他从methodA中逃逸到了methodB中 无逃逸： 对象可以被标量替换，意味着它的内存分配可以从生成的代码中移除。 如上面的sb，就没有发生逃逸，因为这个对象本身没有作为参数传递，也没有被当做方法返回值，并没有赋值给静态变量。 在Java中，不同的逃逸状态影响JIT (即时编译器)的优化策略： 全局逃逸： 由于对象可能被多个线程访问，全局逃逸的对象一般不适合进行栈上分配或其他内存优化。但JIT可能会进行其他类型的优化，如方法内联或循环优化。 参数逃逸： 这种情况下，对象虽然作为参数传递，但不会被方法外部的代码使用。JIT可以对这些对象进行一些优化，例如锁消除。 无逃逸： 这是最适合优化的情况。JIT可以采取多种优化措施，如在栈上分配内存，消除锁甚至完全消除对象分配 (标量替换)。这些优化可以显著提高性能，减少垃圾收集的压力。 方法内联 方法内联是Java中的一个优化技术，即时编译器JIT用它来提高程序的运行效率。在Java中，方法内联意味着将一个方法的代码直接插入到调用它的地方，从而避免了方法调用的开销。这种优化对于小型且频繁调用的方法特别有用。 锁消除 锁消除是 JIT 编译器在编译期间通过分析代码的同步块，判断是否存在锁竞争的可能性。如果某个锁在多线程环境下不存在竞争，那么它就可以在生成的机器码中消除这些锁操作，以减少不必要的开销。 栈上分配 栈上分配的好处： 减少GC压力：对象分配在栈上，当方法执行完毕后，栈上的内存会自动释放，不需要垃圾回收（GC）来管理，从而减少了GC的压力。 提高性能：栈上的内存分配和释放非常高效，因为它只是对栈指针进行简单的移动操作，而堆上的内存管理相对复杂，需要垃圾回收器的参与。 Java中的对象一定在堆上分配内存吗? 不一定，在HotSpot虚拟机中，存在JIT优化的机制，JIT优化中可能会进行逃逸分析，当经过逃逸分析发现某个对象不会逃逸出当前方法（即它只在方法内部使用），那么这个对象就不会被分配到堆上，而是进行栈上分配。 标量替换 标量是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量，Java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。"},{"title":"【JVM】Java内存区域图文详解","date":"2024-08-07T15:41:59.000Z","url":"/2024/08/07/%E3%80%90JVM%E3%80%91Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3/","tags":[["Java内存区域","/tags/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/"]],"categories":[["JVM","/categories/JVM/"]],"content":"1.JVM运行时区域总览 Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。 JVM运行时区域也成为Java内存区域。 在讨论Java内存模型时，通常将其分为线程共享区域和线程私有区域： 2.线程私有区域 2.1.程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 ⚠️ 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2.Java虚拟机栈 JVM栈可以说是 JVM 运行时数据区域的一个核心，除了一些 Native 方法调用是通过本地方法栈实现的，其他所有的 Java 方法调用都是通过JVM栈来实现的（也需要和其他运行时数据区域比如程序计数器配合）。 从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 局部变量表 局部变量表 主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置） 操作数栈 操作数栈 主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。另外，计算过程中产生的临时变量也会放在操作数栈中。 动态链接 动态链接 主要服务一个方法需要调用其他方法的场景。Class 文件的常量池里保存有大量的符号引用，比如方法引用的符号引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用，这个过程也被称为 动态连接 。 方法返回地址 方法返回地址是当前方法执行完成后，线程应该跳转到的下一条指令的地址。这个返回地址实际上是保存在栈帧中的一个特殊位置，用于在方法执行完毕后恢复程序的执行流程。 2.3.本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 3.线程共享区域 3.1.堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作 GC 堆（Garbage Collected Heap）。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代；再细致一点有：Eden、Survivor、Old 等空间。进一步划分的目的是更好地回收内存，或者更快地分配内存。 JDK 8 版本之前，堆内存被通常分为下面三部分： 新生代(Young Generation) 老生代(Old Generation) 永久代(Permanent Generation) JDK 8 版本之后 PermGen(永久代) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存。 下图所示的 Eden 区、两个 Survivor 区都属于新生代，中间一层属于老年代，最下面一层属于元空间。 字符串常量池 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 JDK1.7 之前，字符串常量池存放在永久代，JDK1.7之后字符串常量池、静态变量和常量从永久代移动了 Java 堆中。 JDK 1.7 为什么要将字符串常量池移动到堆中？ 主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。 3.2.方法区 方法区是JVM规范定义的一块用于存储类的元数据、常量、静态变量、即时编译器(JIT编译器)编译后的代码等数据的内存区域。 当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。 方法区会存储已被虚拟机加载的类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 方法区和永久代以及元空间是什么关系呢？ 方法区和永久代以及元空间的关系很像 Java 中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口可以看作是方法区，也就是说永久代以及元空间是 HotSpot 虚拟机对虚拟机规范中方法区的两种实现方式。并且，永久代是 JDK 1.8 之前的方法区实现，JDK 1.8 及以后方法区的实现变成了元空间。 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 1、永久代有 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当元空间溢出时会得到如下错误：java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义初始元空间大小，如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 2、元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在JDK 1.8中，运行时常量池和字符串常量池逻辑上属于方法区，但是实际存放在堆内存中。 运行时常量池 JVM的运行时常量池是方法区的一部分，它主要用于存放编译期生成的各种字面量和符号引用。这些字面量和符号引用是在类加载的过程中，从Class文件的常量池中加载到方法区的运行时常量池中的。 字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义。字面量包括整数、浮点数和字符串字面量。常见的符号引用包括类符号引用、字段符号引用、方法符号引用、接口方法符号。 常量池会在类加载后存放到方法区的运行时常量池中。 运行时常量池的功能类似于传统编程语言的符号表，尽管它包含了比典型符号表更广泛的数据。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 Class常量池： 可以理解为是Class文件中的资源仓库。 Class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有就是常量池(constant pool table)，用于存放编译器生成的各种字面量和符号引用。 Class是用来保存常量的一个媒介场所，并且是一个中间场所。Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 "},{"title":"【Redis】持久化机制最全解析","date":"2024-08-07T03:57:33.000Z","url":"/2024/08/07/%E3%80%90Redis%E3%80%91%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E6%9C%80%E5%85%A8%E8%A7%A3%E6%9E%90/","tags":[["RDB","/tags/RDB/"],["AOF","/tags/AOF/"]],"categories":[["Redis","/categories/Redis/"]],"content":"RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件）。通过将Redis数据集的快照保存到磁盘上的二进制文件中来实现。生成 RDB 文件的过程可以通过手动命令或自动触发。 实现原理 开始 BGSAVE： Redis 主进程接收到 BGSAVE 命令后，调用 fork 创建一个子进程，子进程会先复制主进程的页表（记录虚拟地址与物理地址的映射关系），而不是立即复制实际的内存数据。 在 fork 完成后，父子进程共享相同的内存页，子进程负责生成 RDB 文件，主进程可以继续处理客户端请求。 生成 RDB 文件： 子进程根据页表，扫描 Redis 数据集，将每个键值对序列化为二进制格式，并写入到临时 RDB 文件中。 序列化后的数据被写入到一个临时 RDB 文件中，以避免影响现有 RDB 文件的使用。 写时复制（Copy-On-Write, COW）： 在生成 RDB 文件的过程中，如果主进程执行了写操作（如插入、更新、删除数据），操作系统会触发写时复制机制。COW 机制会为被修改的内存页创建一个副本，并将写操作应用到这个副本上，而不是直接修改共享的内存页。 主进程在读取被修改的数据时，会从生成的副本中读取，而子进程则继续读取原始的内存页。这种机制确保了子进程在生成 RDB 文件时看到的是一致且稳定的数据快照，而主进程可以不受影响地继续处理新的写操作。 完成 RDB 文件： 子进程生成 RDB 文件后，将临时 RDB 文件重命名为正式的 RDB 文件，确保文件替换过程是原子的。 手动启用 Redis 提供了以下两个命令来手动生成 RDB 快照文件： 这里说 Redis 主线程而不是主进程的主要是因为 Redis 启动之后主要是通过单线程的方式完成主要的工作。如果你想将其描述为 Redis 主进程，也没毛病。 定时启用 可以通过配置文件中的 save 选项进行定时触发RDB持久化。 AOF持久化 AOF全称为Append Only File（追加文件）。通过将每次写操作记录到AOF文件中来实现。这种方式的特点是将 Redis 接收到的每个写命令都追加到文件末尾。 实现原理 AOF 持久化功能的实现可以简单分为 5 步： 命令追加（append）：所有的写命令会追加到 AOF 缓冲区中。 文件写入（write）：将 AOF 缓冲区的数据写入到 AOF 文件中。这一步需要调用write函数（系统调用），write将数据写入到了系统内核缓冲区之后直接返回了（延迟写）。 文件同步（fsync）：AOF 缓冲区根据对应的持久化方式（ fsync 策略）向硬盘做同步操作。这一步需要调用 fsync 函数（系统调用）， fsync 针对单个文件操作，对其进行强制硬盘同步，fsync 将阻塞直到写入磁盘完成后返回，保证了数据持久化。 文件重写（rewrite）：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。 重启加载（load）：当 Redis 重启时，可以加载 AOF 文件进行数据恢复。 启用AOF 与快照持久化相比，AOF 持久化的实时性更好。默认情况下 Redis 没有开启 AOF（Redis 6.0 之后默认开启），可以通过 appendonly 参数开启： 刷盘策略 开启 AOF 持久化后，每执行一条会更改 Redis 中数据的命令，Redis 就会将该命令写入到 AOF 缓冲区 server.aof_buf 中，然后再写入到系统内核缓冲区中，最后根据刷盘策略的配置来决定何时将系统内核缓冲区中的数据同步到硬盘中。 AOF的命令记录的频率也可以通过redis.conf文件来配置。 配置项 刷盘时机 优点 缺点 always 同步刷盘 可靠性高，几乎不丢数据 性能影响大 everysec 每秒刷盘 性能适中 最多丢失1秒数据 no 操作系统控制 性能最好 可靠性较差，可能丢失大量数据 文件重写 随着时间的推移，AOF 文件会因为不断追加写命令而变得越来越大，可能会导致磁盘空间不足和恢复速度变慢。可以通过重新生成一个新的 AOF 文件，将当前的数据集以最少的命令集记录下来，从而缩小文件大小。 AOF 文件重写期间，Redis 还会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。 开启 AOF 重写功能，可以调用 bgrewriteaof 命令手动执行，也可以设置阈值进行自动执行。 auto-aof-rewrite-min-size：如果 AOF 文件大小小于该值，则不会触发 AOF 重写。默认值为 64 MB auto-aof-rewrite-percentage：执行 AOF 重写时，当前 AOF 大小和上一次重写时 AOF 大小的比值。如果当前 AOF 文件大小增加了这个百分比值，将触发 AOF 重写。将此值设置为 0 将禁用自动 AOF 重写。默认值为 100。 AOF校验机制了解吗 AOF 校验机制是 Redis 在启动时对 AOF 文件进行检查，以判断文件是否完整，是否有损坏或者丢失的数据。 这个机制的原理其实非常简单，就是通过使用一种叫做 校验和（checksum） 的数字来验证 AOF 文件。这个校验和是通过对整个 AOF 文件内容进行 CRC64 算法计算得出的数字。如果文件内容发生了变化，那么校验和也会随之改变。因此，Redis 在启动时会比较计算出的校验和与文件末尾保存的校验和（计算的时候会把最后一行保存校验和的内容给忽略掉），从而判断 AOF 文件是否完整。如果发现文件有问题，Redis 就会拒绝启动并提供相应的错误信息。 类似地，RDB 文件也有类似的校验机制来保证 RDB 文件的正确性，这里就不重复进行介绍了。 如何选择RDB和AOF RDB（Redis Database File） 优点： 持久化速度快：RDB 文件在持久化时会生成一个数据快照，可以快速地恢复大规模的数据集。 文件较小：RDB 文件是二进制格式的，通常比 AOF 文件更小，占用的磁盘空间较少。 恢复速度快：RDB 文件在恢复数据时，加载速度通常比 AOF 快，因为 RDB 文件是一个完整的数据快照，不需要逐条回放写操作。 对性能影响小：RDB 持久化是通过子进程完成的，主进程可以继续处理客户端请求，因此对 Redis 性能影响较小。 缺点： 数据丢失风险高：因为 RDB 持久化是定期执行的（如每隔几分钟或根据配置的触发条件），在上一次持久化到下一次持久化之间的数据可能会丢失。 生成过程消耗资源：生成 RDB 文件需要 fork 子进程并占用一定的 CPU 和内存资源，特别是数据量大时会影响性能。 AOF（Append-Only File） 优点： 数据丢失风险低：AOF 通过追加日志记录每次写操作，可以更频繁地持久化数据（甚至可以做到每秒持久化一次），因此数据丢失的风险较低。 可调的同步策略：AOF 提供了多种同步策略（如 always、everysec 和 no），可以根据需要在性能和数据安全性之间做权衡。 更人性化：AOF 文件是可读的日志文件，方便人类阅读和分析，有助于调试和故障排查。 缺点： 文件较大：AOF 文件记录了每个写操作，文件大小通常比 RDB 大很多，占用更多的磁盘空间。 恢复速度慢：AOF 恢复数据时需要回放所有的写操作日志，恢复速度通常比 RDB 慢。 性能开销较大：因为 AOF 需要在每次写操作后追加日志，频繁的磁盘 I/O 操作会带来一定的性能开销，特别是在同步策略设置为 always 时。 综合比较 数据安全性：AOF 提供更高的数据安全性，适用于对数据丢失敏感的场景；RDB 在数据持久化频率较低时有较高的数据丢失风险。 性能：RDB 对 Redis 性能影响较小，适用于性能要求高的场景；AOF 因为频繁的磁盘 I/O 操作，对性能有一定的影响。 恢复速度：RDB 恢复速度更快，适用于需要快速恢复大规模数据集的场景；AOF 需要回放日志，恢复速度较慢。 磁盘空间：RDB 文件较小，节省磁盘空间；AOF 文件较大，占用更多磁盘空间。 混合持久化 Redis 4.0 引入了混合持久化机制，结合了 RDB 和 AOF 的优点，以提高持久化的效率和可靠性。混合持久化在重启恢复数据时使用 RDB 文件的快照来快速加载数据，并且将 AOF 日志应用于此快照以实现更高的数据恢复精度。 工作原理 RDB 快照： Redis 生成 RDB 文件快照，保存整个数据库的二进制数据。 RDB 文件的生成是通过子进程完成的，主进程可以继续处理客户端请求。 AOF 日志： 除了生成 RDB 文件外，Redis 还会将写操作记录到 AOF 日志中。 在混合持久化模式下，AOF 文件的初始部分是一个 RDB 快照，后面紧接着的是增量的 AOF 日志。 持久化过程： 当 Redis 进行持久化操作时，它会首先生成一个 RDB 文件，并将这个文件内容写入 AOF 文件。 在 AOF 文件中，RDB 文件的内容作为初始部分，然后紧跟着追加的 AOF 日志。 数据恢复： 当 Redis 重启时，它会首先加载 AOF 文件中的 RDB 部分（即快照）来快速恢复数据。 然后，它会回放 AOF 文件中 RDB 部分后的增量日志，以确保数据的一致性和完整性。 优缺点 优点： 快速恢复：使用 RDB 部分可以快速加载大部分数据，而不需要逐条回放所有写操作日志，极大地提高了数据恢复的速度。 较高的数据安全性：结合了 RDB 和 AOF 的优点，RDB 部分确保了数据的快速恢复，而 AOF 部分提供了更高的数据持久化频率，降低了数据丢失的风险。 性能优化： 在持久化过程中，RDB 快照的生成是通过子进程完成的，对主进程处理客户端请求的性能影响较小。 AOF 的增量日志记录了自上次快照以来的所有写操作，减少了持久化过程中的 I/O 操作次数，提高了系统性能。 缺点： 配置复杂性：混合持久化的配置比单独使用 RDB 或 AOF 更加复杂，需要合理设置参数以实现最佳性能和数据安全性。 磁盘空间占用：混合持久化模式下，AOF 文件既包含 RDB 快照部分又包含增量日志，可能会占用更多的磁盘空间。 启用混合持久化 要启用混合持久化，可以在 Redis 配置文件 redis.conf 中设置以下参数： "},{"title":"【MySQL】慢sql查询优化","date":"2024-08-04T03:31:24.000Z","url":"/2024/08/04/%E3%80%90MySQL%E3%80%91%E6%85%A2sql%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"],["日志","/tags/%E6%97%A5%E5%BF%97/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"定位慢sql 工具排查慢sql 调试工具：Arthas 运维工具：Skywalking 通过以上工具可以看到哪个接口比较慢，并且可以分析SQL具体的执行时间，定位到哪个sql出了问题。 启用慢查询日志 慢查询日志记录了所有执行时间超过指定参数(long_query_time，单位:秒，默认10秒)的所有SQL语句的日志。 MySQL的慢查询日志默认没有开启，需要在MySQL的配置文件(/etc/my.cnf)中配置如下信息: 配置完成后，重启MySQL服务保证配置生效。 慢查询日志一般的返回结果如下： 需要关注以下内容： Query_time（查询时间）：查询执行的总时间，单位为秒。是关键的指标，用于判断查询的性能。 Lock_time（锁定时间）：表被锁定的时间，单位为秒。可以帮助判断是否存在锁等待问题。 Rows_sent（发送的行数）：查询返回的行数。 Rows_examined（检查的行数）：查询过程中检查的行数，用于判断查询的效率。 分析慢sql profile详情 SHOW PROFILE 是 MySQL 提供的一种用于查看查询语句执行的详细步骤和资源消耗的工具。使用 SHOW PROFILE 命令可以帮助找出查询语句的瓶颈，优化查询性能。 启用 Profiling 在使用 SHOW PROFILE 之前，需要先启用 Profiling： 执行查询 执行你想分析的查询语句： 查看 Profile 列表 使用以下命令查看刚才执行的查询的 Profile： 这将显示一个查询 ID 列表及其对应的查询语句和总执行时间。 查看详细的 Profile 信息 使用 SHOW PROFILE 查看某个查询 ID 的详细信息： 查看CPU信息 典型返回 Status表示查询的不同执行阶段。Duration即每个阶段所用的时间。 explain执行计划 explain 是 MySQL 提供的一种用于分析和调试 SQL 查询的工具。 通过使用 explain，可以了解 MySQL 在执行查询时采用的具体执行计划，包括访问数据表的方式、使用的索引、连接表的顺序等信息。这些信息对于优化查询性能至关重要。 基本概念 EXPLAIN 执行计划支持 SELECT、DELETE、INSERT、REPLACE 以及 UPDATE 语句。我们一般多用于分析 SELECT 查询语句，要获取一条sql语句的执行计划，只需要在语句前加上explain关键字即可。 执行计划的返回结果一般是这样的： 返回结果中各字段的含义解释如下： 列名 含义 id SELECT 查询的序列标识符 select_type SELECT 关键字对应的查询类型 table 用到的表名 partitions 匹配的分区，对于未分区的表，值为 NULL type 表的访问方法 possible_keys 可能用到的索引 key 实际用到的索引 key_len 所选索引的长度 ref 当使用索引等值查询时，与索引作比较的列或常量 rows 预计要读取的行数 filtered 按表条件过滤后，留存的记录数的百分比 Extra 附加信息 字段释意 id 查询的序列标识符，用于表示查询的执行顺序。值越大，优先级越低，执行顺序越靠后。 select_type 查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询，常见的值有： SIMPLE: 简单查询，不包含子查询或 UNION。 PRIMARY: 最外层的 SELECT 查询。 SUBQUERY: 子查询中的第一个 SELECT。 DERIVED: 派生表（子查询中的 FROM 子句）。 UNION: UNION 操作中的第二个或后续的 SELECT 查询。 UNION RESULT: UNION 的结果集。 table 查询用到的表名。 type（重要） 查询执行的类型，描述了查询是如何执行的。常见的类型如下，这些类型的性能从最优到最差排序为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL system：如果表使用的引擎对于表行数统计是精确的（如MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。 const：常数索引扫描，表中只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。 eq_ref：唯一索引扫描。在连接查询时，被连接表针对连接表中的每一行只会返回最多一行结果。它是除了 system 和 const 之外最优的连接方式，通常用于使用主键或唯一索引的所有字段作为连接条件。 ref：普通索引扫描，会扫描索引树的一部分来查找匹配的行。 range：范围扫描，只会遍历索引树的一个范围来查找匹配的行。 index：全索引扫描，会遍历整个索引树来查找匹配的行。 ALL：全表扫描，将遍历整张表来查找匹配的行。 possible_keys possible_keys 列表示 MySQL 执行查询时可能用到的索引。如果这一列为 NULL ，则表示没有索引可以使用。 key（重要） key 列表示 MySQL 实际使用到的索引。如果为 NULL，则表示未用到索引。 key_len（重要） key 列表示 MySQL 实际使用到的索引的长度。 Extra（重要） 这列包含了 MySQL 解析查询的额外信息，通过这些信息，可以更准确的理解 MySQL 到底是如何执行查询的。常见的值如下： Using index：表明查询使用了覆盖索引，不用回表，查询效率非常高。 Using index condition：表示查询优化器选择使用了索引下推这个特性。 Using where：表明查询使用了 WHERE 子句进行条件过滤，通常是因为使用非索引列查询或者未使用覆盖索引。 Using where; Using index：查询的列使用了覆盖索引，但where筛选条件不是联合索引的前导列或者只是前导列的一个范围。 Using filesort：在排序时使用了文件排序而不是索引排序，通常是因为无法使用索引进行排序。 Using temporary：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。 Using join buffer (Block Nested Loop)：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。 优化慢sql sql优化方案 根据explain执行计划的返回结果，我们可以根据以下字段进行sql优化： 通过key和key_len检査是否命中了索引（索引本身存在是否有失效的情况） 通过type字段查看sql是否有进一步的优化空间，是否存在全索引扫描或全表扫描 通过extra字段判断，是否出现了回表的情况，如果出现了，可以尝试添加索引或修改返回字段来修复 索引不合理 回表查询，查询的字段没有使用覆盖索引，需要回表查询数据。 索引失效，如查询条件中使用了函数或者运算、OR连接条件中没有使用索引、不符合最左匹配原则、使用了前导通配符、查询条件使用了隐式的类型转换等。 多表join 在内存中做关联，可以先从数据库中把数据查询出来，然后在代码中进行二次查询，进行数据的关联。 数据冗余，可以把一些重要的数据在表中做冗余，避免使用多表查询。 查询字段过多 查询无关字段，比如使用了select *查询了无关字段。 如果确实需要查询大量字段，可以考虑做分库分表。 数据库连接数不足 业务量太大，可以用分库减少单库的业务压力。 存在一些慢SQL和长事务导致的，需要具体分析。 深分页优化查询 传统分页 传统分页通常使用 OFFSET 和 LIMIT 来实现 这种方法对于小数据集或页数较小时效果较好，但在数据量非常大的情况下，OFFSET 的值越大，数据库需要扫描的行数就越多，性能会急剧下降。 深分页 深分页通过避免使用 OFFSET 来提高性能 1.覆盖索引+子查询： 这种方法通过子查询使用覆盖索引快速定位到分页的起始位置，外部查询从该位置获取实际数据，避免大量数据扫描和回表操作。 如本例中通过子查询定位到了第100001页的起始位置，向后获取100行数据。 这种方法避免了大量数据扫描，适用于有索引列的情况。 2.存储分页结果： 另一种方法是将分页结果存储在缓存（如 Redis）或临时表中，从而避免频繁查询数据库。例如： 这种方法适用于需要多次访问相同分页结果的场景。"},{"title":"【MySQL】全面剖析索引失效、回表查询与索引下推","date":"2024-08-03T08:24:24.000Z","url":"/2024/08/03/%E3%80%90MySQL%E3%80%91%E5%85%A8%E9%9D%A2%E5%89%96%E6%9E%90%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E3%80%81%E5%9B%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2%E4%B8%8E%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.索引失效的情况 以tb_user表举例，id为主键索引、name和phone字段上建立了一个普通索引，name和phone均为varchar类型。 索引列运算 当在 WHERE 子句或 JOIN 子句中对列使用函数或表达式时，索引会失效。 执行以下语句，可以发现执行计划中索引已经生效。 如果我们使用substring函数只取前三个字符，则索引失效。 可以发现type为ALL，key为null，说明本次查询没有执行索引，走的是全表扫描。 隐式类型转换 当列的类型和查询中的值类型不同时，MySQL 可能会进行隐式类型转换，导致索引失效。 执行以下语句，phone为varchar类型，如果等号右侧不加引号，则发生隐式转换，索引失效。 前导通配符查询 使用通配符查询时，如果通配符在字符串的前面，索引会失效。 执行以下语句，查询name字段后缀为ack的数据，索引失效。 or连接条件 当 or 条件中某个列没有索引时，索引会失效 执行以下语句，因为name和phone都是索引字段，索引正常生效。 执行以下语句，因为age字段没有设置索引，所以索引失效查询。 最左匹配原则 对于联合索引（多个列组成的索引），如果查询条件不包含索引的最左前缀部分，索引会失效。 TIPS：这里指的最左是联合索引中的顺序，而不是SQL语句查询条件的顺序。 在本例中，我们新建一个表table，给字段col1、col2、age建立联合索引（col1, col2, age） 遵循最左匹配发展 按照最左前缀法则查询数据。 可以发现，联合索引的总长度为107 不遵循最左匹配法则（查询条件中不包括联合索引的最左前缀部分） 如果不按照最左匹配法则，直接查询col2的数据 本次查询走的是index全索引扫描，性能上要低于ref。 不遵循最左匹配法则（查询条件中包含&gt; &lt;范围查询） 如果查询条件中使用了&gt; &lt;，则不遵循最左匹配法则（可以使用其他范围查询符号），范围查询右侧的索引失效。 执行以下语句，由于age在联合索引（col1, col2, age）中是最后一个，所以不存在其右侧索引失效的情况。 但是如果我们将col2和age调换顺序，改为（col1, age, col2），则col2索引失效。 数据分布情况 MySQL会根据表中数据的分布情况，决定是否使用索引 举一个简单的例子，如果表中的age字段最小值为10，查询条件为age &gt;= 10。则在查询时可能不会走索引，因为走索引和不走索引都需要查询表中的全部数据，不过判断一个语句是否走索引还是要根据explain关键字返回的结果进行判断。 2.回表查询 回表查询是指在使用辅助索引（二级索引）进行查询时，由于辅助索引中不包含查询所需的所有列数据，数据库必须通过索引找到对应的数据行位置，再去实际的数据表（即“回表”）中读取完整的数据行。这种操作会增加额外的 I/O 开销，因此回表查询通常比直接从索引中获取数据的查询更慢。 回表查询示例 假设有以下表数据，id为主键索引，name为普通索引。 主键索引（id）的索引结构如下图，在叶子节点中存储的是每一行的数据。如果我们直接根据id查询，就可以在遍历索引时直接拿到每一行的数据。 辅助索引（name）的索引结构如下，叶子节点存储的是该行的主键（id），如果需要查询该行的数据，则需要遍历索引后获得主键id，再根据这个主键id前往主键索引中查询，这个过程就是回表查询。 避免回表查询 避免回表查询很简单，只需要保证查询的列能够被索引结构覆盖即可。通过创建一个包含所有查询所需列的索引，数据库可以直接从索引中获取所有需要的数据，无需回表。 覆盖索引（Covering Index）是指查询所需的所有列都包含在同一个索引中，从而避免回表操作。这样可以显著提高查询性能。 比如我们直接使用以下语句，就可以避免回表查询，因为name索引中包含了name和id的数据，而无需回到数据库进行查询。 3.索引下推 索引下推（Index Condition Pushdown，ICP）是 MySQL 5.6 及以上版本中引入的一种优化技术，用于提高使用索引查询的效率。ICP 可以减少回表操作（即从索引表跳回数据表读取完整行数据）的次数，从而提高查询性能。 除了可以减少回表次数之外，索引下推还可以减少存储引擎层和 Server 层的数据传输量。 工作原理 在没有索引下推的情况下，MySQL 的查询执行流程通常是： 索引扫描：存储引擎使用索引查找满足索引条件的记录。 返回记录：将这些记录返回给 MySQL 服务器。 行过滤：MySQL 服务器根据剩余的查询条件进一步过滤这些记录。 使用索引下推后，MySQL 优化器会在索引扫描阶段尽可能多地应用查询条件，只有在通过索引扫描无法完全过滤的情况下，才进行回表操作。 适用场景 索引下推在以下场景中尤其有效： 范围查询：对索引列进行范围查询时，例如 BETWEEN、&lt;、&gt; 等。 联合索引查询：在联合索引的前缀列上进行查询，并且查询条件涉及非索引列时。 复杂条件查询：查询条件包含多个过滤条件时，例如 AND、OR 等。 示例 假设有一个包含联合索引 idx_name_age 的表 tb_user： 查询语句： 在没有索引下推的情况下，MySQL 会： 使用索引 idx_name_age 找到 name = 'John' 的所有记录。 回表读取每一条记录的实际数据。 对回表后的数据应用剩余条件 age &gt; 30 和 address LIKE '%Street%' 进行过滤。 在启用索引下推的情况下，MySQL 会： 使用索引 idx_name_age 找到 name = 'John' 且 age &gt; 30 的记录（在索引扫描阶段应用部分条件）。 仅对符合前两个条件的记录进行回表操作。 对回表后的数据应用剩余条件 address LIKE '%Street%' 进行最终过滤。 "},{"title":"【MySQL】一文吃透MVCC执行原理","date":"2024-08-02T08:06:41.000Z","url":"/2024/08/02/%E3%80%90MySQL%E3%80%91%E4%B8%80%E6%96%87%E5%90%83%E9%80%8FMVCC%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/","tags":[["MVCC","/tags/MVCC/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.MVCC是什么？ MVCC全称Multi-Version Concurrency Control，即多版本并发控制。它通过维护数据的多个版本来实现高效的并发控制，用于在多个并发事务同时读写数据库时保持数据的一致性和隔离性。 在搞清楚MVCC的实现原理之前，还需要了解快照读和当前读的概念。 一致性非锁定读（快照读） 简单的select语句（不加锁）就是快照读，读取的是记录数据的可见版本，不加锁，是非阻塞读。 select ... 一致性锁定读（当前读） 读取的是记录的最新版本，读取时需要保证其他并发事务不能修改当前记录，会对读取的记录加锁。如果执行的是下列语句，就是锁定读。 select ... lock in share mode select ... for update insert、update、delete 操作 2.MVCC实现原理 MVCC 的实现依赖于：隐藏字段、Read View、undo log。 隐藏字段 在内部，InnoDB 存储引擎为每行数据添加了三个隐藏字段 隐藏字段 含义 DB_ROW_ID（6字节） 隐藏主键，如果当前表不存在主键，则将该隐藏字段作为主键 DB_TRX_ID（6字节） 最近修改事务ID，记录插入这条数据或最后一次修改该记录的事务ID DB_ROLL_PTR（7字节） 回滚指针，指向这条记录的上一个版本，用于配合undo log 假设有一个学生表，该表没有指定主键。 那么该表实际上的字段如下，如果存在主键则不存在DB_ROW_ID字段。 undo log undo log 分为两种类型：insert undo log 和 update undo log。 Insert undo log 是在事务进行插入操作时生成的日志。其主要作用是用于事务回滚时撤销插入操作。该日志只在回滚时需要，在事务提交后，可被立即删除。 Update undo log 是在事务进行更新或删除操作时生成的日志。其主要作用是用于事务回滚时撤销更新和删除操作。该日志不仅在回滚时需要，在快照读时也需要，不会被立即删除。 一条记录的每一次更新操作产生的 undo log 格式都有一个一个 DB_TRX_ID事务id 和 DB_ROLL_PTR 指针： 通过 DB_TRX_ID 可以知道该记录是被哪个事务修改的； 通过 DB_ROLL_PTR 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链； 举例说明： 事务1已经提前执行了INSERT INTO user (id, age, name) VALUES (10, 10, 'Jack');语句插入了一条记录。则DB_TRX_ID（插入这条数据或最后一次修改该记录的事务ID）为1，由于insert undo log在事务提交后自动删除，所以不存在undo log日志，DB_ROLL_PTR为null。 该示例中有四个并发事务，其他事务在不同的时刻将执行update语句修改记录。 所有事务执行完毕后，当前记录的DB_TRX_ID为4，且形成了一条Update Undo Log版本链，后续MVCC可以利用这条版本链获取旧数据。 Read View ReadView（读视图）是快照读执行时MVCC获取数据的依据，记录并维护系统尚未提交的事务（也称为活跃事务）id。 ReadView有以下四个重要字段： 字段 含义 m_ids 当前活跃事务的ID集合 min_trx_id 最小活跃事务ID max_trx_id 预分配事务ID，当前最大事务ID+1（事务ID自增） creator_trx_id ReadView创建者的事务ID Tips：m_ids的长度可不是max_trx_id - min_trx_id，因为m_ids是当前活跃事务的ID集合，在min_trx_id到max_trx_id即可能有活跃事务，也可能有非活跃事务。 当一个事务需要读取一条记录时，需要遵循以下四条规则进行读取（非常重要）： DB_TRX_ID == creator_trx_id时，说明该数据就是当前事务更改的，可以访问该版本。 DB_TRX_ID &lt; min_trx_id时，比最小活跃事务ID小，说明当前事务已经提交了，可以访问该版本。 DB_TRX_ID &gt; max_trx_id时，比预分配事务ID大，说明当前事务在ReadView生成后才开始，还没有提交不能访问该版本。 min_trx_id &lt;= DB_TRX_ID &lt;= max_trx_id时，如果DB_TRX_ID不在m_ids中，即当前事务已经提交了，可以访问该版本。 看完这些规则我们可以总结以下规律： 当前事务可以读取自己更改的记录，对应第一条规则 只有一个事务提交了，才能去读取该事务ID下的版本记录（保证事务的隔离性，防止脏读），对应第二、三、四条规则 3.MVCC的执行流程 这里承接第二部分举过的案例，来具体分析事务5在不同隔离级别两次查询id为10的记录时，分别会读取哪个版本的数据。学会这个案例之后，就能理解MVCC如何解决不可重复读和幻读的问题。 RC隔离级别 在RC隔离级别下，事务每一次执行快照读时都会生成一次ReadView。 在第一次查询时，还未提交的事务有3、4、5，那么m_ids（活跃事务ID集合）为{3,4,5}，min_trx_id（最小活跃事务ID）为3，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 在第二次查询时，还未提交的事务有4、5，那么m_ids（活跃事务ID集合）为{4,5}，min_trx_id（最小活跃事务ID）为4，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 RR隔离级别 在RR隔离级别下，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。 在第一次查询时，还未提交的事务有3、4、5，那么m_ids（活跃事务ID集合）为{3,4,5}，min_trx_id（最小活跃事务ID）为3，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 在第二次查询时，直接复用ReadView。 读取版本记录 在读取版本记录时，需要根据DB_TRX_ID匹配ReadView的读取规则，判断当前记录对DB_TRX_ID对应的事务是否可见，如果可见，直接读取当前版本，如果不可见，则读取前一个undo log记录继续进行匹配。 我们以第一个ReadView举例，当前undo log版本链和读视图如下： 当DB_TRX_ID为4，存在于活跃事务列表中，因此不可以读取该行数据，需要向前找DB_TRX_ID为3的记录。 当DB_TRX_ID为3时，同样存在于活跃事务列表，因此不可以读取该行数据，需要向前找DB_TRX_ID为2的记录。 当DB_TRX_ID为2时，发现DB_TRX_ID&lt;min_trx_id，符合规则，因此可以读取该行记录。 最后的读取结果为： 10 20 Jack 2 0x00001 4.MVCC小结 MVCC解决不可重复读 RC隔离级别 在RC读取已提交下，事务每一次执行快照读时都会生成一次ReadView，这也就造成了每次读取就有不同 ReadView，那么就会读到已提交的事务修改的内容，不能解决不可重复读的问题。 RR隔离级别 解决 RR 不可重复读主要靠 Readview，在隔离级别为可重复读时，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。由于后续复用了 ReadView，所以数据对当前事务的可见性和第一次是一样的，所以从 undo log 中读到的数据快照和第一次是一样的，即便过程中有其他事务修改也读不到。因此解决了不可重复读的问题。 MVCC解决幻读 InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock（临键锁） 来解决幻读问题： 1、执行快照读 在快照读的情况下，RR 隔离级别使用MVCC，只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”。 2、执行当前读 在当前读的情况下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读。InnoDB 使用Next-key Lock来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。"},{"title":"【MySQL】索引概念解析","date":"2024-08-01T06:05:13.000Z","url":"/2024/08/01/%E3%80%90MySQL%E3%80%91%E7%B4%A2%E5%BC%95%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.什么是索引？ MySQL中的索引是一种数据结构，用于帮助MySQL数据库管理系统快速查询数据。索引的主要目的是提高数据检索的速度，减少数据库系统需要扫描的数据量。 优点： 索引可以极大的提高数据检索效率，降低数据库IO成本 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性 通过索引列对数据进行排序，降低数据排序的成本，减少CPU的消耗 缺点： 创建索引需要消耗物理空间。对于大型数据库，索引可能会占用相当大的磁盘空间。 创建索引和维护索引需要消耗时间，降低表的更新效率。对表中数据进行增删改操作时，那么索引也需要动态的修改，会降低 SQL 执行效率。 适用场景： 具有唯一性约束的字段，比如商品编码，可以适用唯一性索引 频繁使用的列，如主键、外键 经常用于WHERE查询条件的字段，如果查询条件不是⼀个字段，可以建⽴联合索引。 用于GROUP BY或ORDER BY中的字段，由于索引基于B+树实现，会自动维护数据的有序性，降低数据排序的成本。 不适用场景： WHERE、GROUP BY或ORDER BY用不到的字段，索引的作用是快速定位，用不到的话会额外占用空间 存在大量重复元素的字段，如性别，无论怎么搜索可能只会得到一半的数据。 表数据太少的时候，无需创建索引。 频繁修改的列，当对表中数据进行增删改操作时，由于索引需要维护B+树的有序性，会频繁的创建索引，影响数据库的性能。 2.索引结构选型 B+ 树非常适合作为数据库索引结构，特别是在处理大量数据的场景下，能够提供高效的查询、插入和删除操作，并且支持范围查询和顺序扫描。这些特性使得 B+ 树成为 MySQL 等数据库系统中首选的索引数据结构。 下面将针对不同的数据结构进行分析，以说明B+树为何能在众多数据结构中脱颖而出。 Hash表 MySQL 的 InnoDB 存储引擎不直接支持常规哈希索引，但有一种自适应哈希索引（Adaptive Hash Index）。这种索引结合了 B+ 树和哈希索引的特点，适应实际数据访问模式和性能需求。自适应哈希索引的每个哈希桶实际上是一个小型的 B+ 树结构，存储多个键值对，减少了哈希冲突，提高了效率。 MySQL 没有采用哈希索引作为主要索引结构，主要因为哈希索引不支持顺序和范围查询。此外，每次 IO 只能取一个值，限制了查询性能。 二叉查找树(BST) 二叉查找树的性能非常依赖于它的平衡程度。 平衡时：查询时间复杂度为 O(log N)，效率较高。 不平衡时：最坏情况下退化为线性链表，查询效率降至 O(N)。 AVL树 AVL 树是一种高度平衡二叉树，保证任何节点的左右子树高度之差不超过 1，查找、插入和删除的时间复杂度均为 O(log N)。AVL 树通过四种旋转操作（LL、RR、LR、RL）保持平衡，但频繁的旋转操作增加了计算开销，降低了数据库写操作的性能。 每个 AVL 树节点仅存储一个数据，每次磁盘 IO 只能读取一个节点的数据，需要多次 IO 查询多个节点的数据，影响了性能。 红黑树 红黑树是一种自平衡二叉查找树，通过颜色变换和旋转操作保持平衡，具有以下特点： 每个节点非红即黑； 根节点总是黑色； 每个叶子节点是黑色的空节点（NIL）； 红色节点的子节点必须是黑色； 从任意节点到叶子节点的每条路径包含相同数量的黑色节点。 红黑树追求的是大致平衡，查询效率略低于 AVL 树，因为红黑树的平衡性较弱，可能导致树的高度较高，需要多次磁盘 IO 操作。这也是 MySQL 没有选择红黑树的原因之一。但红黑树的插入和删除操作效率高，因为只需进行 O(1) 次数的旋转和变色操作，保持基本平衡状态。 红黑树广泛应用于 TreeMap、TreeSet 和 JDK1.8 的 HashMap 底层，在内存中的表现非常优异。 B树&amp;B+树 B 树也称 B-树,全称为 多路平衡查找树 ，B+ 树是 B 树的一种变体。B 树和B+ 树中的 B 是 Balanced （平衡）的意思。 目前大部分数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。 B 树&amp;B+ 树两者有何异同呢？ 存储方式不同 B 树的所有节点既存放键（key）也存放数据（data），而B+树只有叶子节点存放 key 和 data，非叶子节点只存放 key。 单点查询稳定性不同 B 树的查询波动较大，因为每个节点既存放索引又存放记录，有时访问到非叶子节点就能找到数据，有时需要访问叶子节点才能找到。 B+ 树的非叶子节点仅存放索引，因此可以存放更多的索引，使得 B+ 树比 B 树更「矮胖」，查询底层节点的磁盘 I/O 次数更少。 插入和删除效率不同 在B树中，当内部节点需要删除或插入时，可能会涉及到多个子节点的调整。由于B树的非叶子节点也存储数据，因此分裂或合并操作需要确保数据的完整性和树的平衡。 相比之下，B+树的非叶子节点只存储键信息，不存储实际的数据。因此，在分裂或合并非叶子节点时，只需要处理键信息，这使得操作相对简单且高效。并且，B+树的叶子节点包含所有实际的数据，并且它们之间通过指针相连。这使得在删除节点时，可以更容易地重新组织数据以保持树的平衡。 范围查询效率不同 B+ 树支持范围查询。进行范围查找时，从根节点遍历到叶子节点即可，因为数据都存储在叶子节点上，且叶子节点通过指针连接，便于范围查找。 3.索引的类型 按照底层存储方式角度划分： 聚簇索引（聚集索引）：索引结构和数据一起存放的索引，只有InnoDB 中的主键索引属于聚簇索引。 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，二级索引(辅助索引)就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。 按照应用维度划分： 主键索引 加速查询 + 列值唯一 + 不可以有NULL + 表中只有一个。 普通索引 加速查询 + 列值可以重复 + 可以有NULL。 唯一索引 加速查询 + 列值唯一 + 可以有NULL。 联合索引 多个列组成一个索引，专门用于组合搜索，其效率大于多个单列索引的合并效率。 覆盖索引 一个索引包含所有需要查询的字段的值。 全文索引 对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。 "},{"title":"【工厂模式】深入浅出工厂模式三大实现方案","date":"2024-07-28T08:21:10.000Z","url":"/2024/07/28/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["工厂模式","/tags/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 工厂模式是一种创建型设计模式，通过提供一个接口或抽象类来创建对象，而不是直接实例化对象。工厂模式的主要思想是将对象的创建与使用分离，使得创建对象的过程更加灵活和可扩展。 工厂模式主要包括以下角色： 抽象工厂（Abstract Factory）：定义了一个创建产品对象的接口，可以包含多个方法来创建不同类型的产品。 具体工厂（Concrete Factory）：实现抽象工厂接口，负责实例化具体的产品对象。 抽象产品（Abstract Product）：定义了产品的接口或抽象类，是工厂方法和抽象工厂模式中的基础。 具体产品（Concrete Product）：实现抽象产品接口，具体定义产品的功能和行为。 2.简单工厂模式 简单工厂模式（Simple Factory Pattern）：由一个工厂类根据传入的参数决定创建哪一种产品类的实例。它通常包含一个静态方法，这个方法根据参数创建相应的对象。 定义一个简单的例子：电脑有很多品牌，如惠普电脑、联想电脑，如果需要创建这两个对象时，主动new出来，使用了简单工厂模式后，可以把创建的动作交给工厂类，只需要指定参数即可获取对应的对象。 实现方法 编写产品类 首先创建一个Computer接口，不同的产品实现这一接口 编写工厂类 简单工厂模式不存在抽象工厂，只需编写一个工厂类即可。 测试类使用工厂创建产品 输出结果如下： 小结 简单工厂模式虽然实现比较简单，但是工厂类的职责过重，增加新的产品类型需要修改工厂类，违背了开闭原则。 开闭原则：软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。 对扩展开放（Open for extension）：软件实体应该允许在不改变其现有代码的情况下，通过增加新功能来对其进行扩展。也就是说，当软件的需求发生变化时，我们应该能够通过添加新代码来满足这些需求，而不需要修改已有的代码。 对修改关闭（Closed for modification）：一旦软件实体被开发完成并投入使用，其源代码就不应该再被修改。这可以防止对现有功能的破坏，减少引入新的错误的风险，并使软件更加稳定和可维护。 3.工厂方法模式 工厂方法模式（Factory Method Pattern）：定义一个创建对象的接口，但由子类决定实例化哪个类。工厂方法将对象的创建推迟到子类。 实现方法 编写产品类 编写工厂类 需要定义一个抽象工厂，然后由具体工厂创建对应的产品。 测试类使用不同的具体工厂创建产品 输出结果如下： 小结 优点： 遵循开闭原则，新增产品时不需要修改现有系统代码，只需要添加新的具体工厂和具体产品类。 更符合单一职责原则，每个具体工厂类只负责创建一种产品。 缺点： 增加了系统复杂度，需要增加额外的类和接口。 4.抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。适用于产品族的场景，即多个产品等级结构中相关的产品需要一起创建和使用。 产品等级结构：指产品的继承结构，例如一个电脑抽象类，它有HP电脑、Lenovo电脑等实现类，那么这个电脑抽象类和他的实现类就构成了一个产品等级结构。 产品族：产品族是指由同一个工厂生产的，位于不同产品等级结构中的一组产品。比如，Lenovo除了生产电脑还可以生产打印机等其他产品。 实现方法 编写产品类 编写工厂类 定义一个抽象工厂，该工厂可以创建多个产品。 测试类使用不同的具体工厂创建产品 输出结果如下： 小结 优点： 符合开闭原则，新增产品族时无需修改现有系统代码。 符合单一职责原则，每个具体工厂类只负责创建一类产品族。 保证产品族的一致性，同一个工厂创建的产品是属于同一个产品族的。 缺点： 增加了系统的复杂度。修改产品族时，需要修改所有具体工厂类，扩展性稍差。 5.总结 适用场景： 简单工厂模式：适用于产品种类较少，客户端只需根据参数获得具体产品的简单场景。适合产品种类不经常变化的场合。 工厂方法模式：适用于产品种类较多，每个产品有相应的具体工厂类。适合需要扩展新产品，且不希望修改现有代码的场合。 抽象工厂模式：适用于产品族较多，每个产品族中包含多个相关产品。适合创建一系列相关或相互依赖的产品，且希望统一管理产品族的场合。 "},{"title":"【代理模式】详细剖析静态代理、JDK动态代理和CGLIB动态代理","date":"2024-07-28T03:16:29.000Z","url":"/2024/07/28/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["代理模式","/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 代理模式是常用的Java设计模式，该模式的特点是代理类与委托类共享相同的接口。代理类主要负责预处理消息、过滤消息、将消息转发给委托类，并在事后处理消息等。代理类与委托类之间通常存在关联关系，一个代理类对象与一个委托类对象关联。代理类对象本身不真正实现服务，而是通过调用委托类对象的相关方法来提供特定的服务。 代理模式主要包括以下角色： 抽象主题（Subject）：定义代理类和委托类（RealSubject）的共同接口。这个接口规定了代理类和委托类必须实现的方法，代理类可以通过这个接口来调用委托类的方法。 真实主题（RealSubject）：实现抽象主题，定义委托类的操作。它包含了实际的业务逻辑，是客户端实际需要调用的对象。 代理类（Proxy）：实现抽象主题，持有对委托类的引用，并在其方法被调用时进行控制。代理类在调用委托类的方法前后可以添加一些额外的功能，如日志记录、权限控制、事务处理等。 2.静态代理 静态代理：在编译时期确定代理类和目标类的关系，代理类和目标类都要实现同一个接口。 定义一个简单的例子：假如一个租客需要租房子，他可以直接通过房东（委托类）去租房，也可以经过中介（代理类）去租房。房东（realsubject）和中介（proxy）都需要实现subject接口实现房子出租。 确定接口具体行为 首先创建一个Person接口。这个接口是房东和中介的共同接口，租房行为可以被中介代理。 编写委托类业务逻辑 创建一个委托类，实现subject接口，并编写业务逻辑 代理类增强方法 创建一个代理类，同样实现subject接口，对委托类的方法进行增强 测试类使用代理对象 输出结果如下，可以发现对方法进行了增强 3.动态代理 动态代理：在程序运行时动态生成代理类（subject的实现类）。 相比于静态代理， 动态代理的优势在于其较高的灵活性和代码复用性。同一个动态代理处理器可以代理多个目标对象，而静态代理则需要创建大量的代理类。 在Java中，可以通过JDK和CGLIB实现动态代理。 3.1.JDK动态代理 实现原理 JDK动态代理：在java的java.lang.reflect包下提供了Proxy类和InvocationHandler接口，利用这两个类和接口，可以在运行时动态生成指定接口的实现类。 Proxy类就是用来创建一个代理对象的类，在JDK动态代理中我们需要使用其newProxyInstance方法。 这个方法的作用就是创建一个代理类对象，它接收以下三个参数： loader：一个ClassLoader对象，指定哪个ClassLoader将加载生成的代理类。 interfaces：一个Interface对象数组，定义代理对象实现的一组接口，代理类可以调用这些接口中声明的所有方法。 h：一个InvocationHandler对象，指定代理对象的方法调用将关联到哪个InvocationHandler对象，由它处理实际的方法调用。 InvocationHandler接口提供了一个invoke方法，当代理对象调用方法时，invoke方法会被调用。通过实现这个接口，可以在方法调用前后添加自定义逻辑。 代码实现 确定接口具体行为 这里我们设计两个接口 编写委托类业务逻辑 委托类实现这两个接口，并且定义具体的业务逻辑 编写代理工厂代码 代理工厂负责在运行时动态生成代理类，需要实现InvocationHandler接口重写invoke方法来做方法增强，使用Proxy类创建代理对象。 测试类使用代理对象 在实际使用时，只需要将工厂类生成的代理对象转为需要的代理类，即可实现同时代理多个接口的方法。 返回结果： 3.2.CGLIB动态代理 实现原理 CGLIB动态代理：依赖于ASM下的Enhancer类和MethodInterceptor接口，可以在运行时动态生成目标类的子类。 Enhancer类是用来创建代理对象的类。在CGLIB动态代理中，我们需要使用其create方法。 这个方法的作用是创建一个代理类对象，通常还需要设置以下几个属性： setSuperclass：设置被代理的目标类，CGLIB通过生成目标类的子类来实现代理。 setCallback：设置回调接口，用于处理代理对象的方法调用。 MethodInterceptor接口提供了一个intercept方法，当代理对象调用方法时，intercept方法会被调用。通过实现这个接口，可以在方法调用前后添加自定义逻辑。 与JDK动态代理不同，CGLIB代理不需要目标类实现接口。CGLIB通过生成目标类的子类并重写方法来实现代理，因此它可以代理没有实现接口的类。 代码实现 首先导入依赖 编写委托类业务逻辑（无需实现接口） 测试类使用代理对象 返回结果： 4.总结 静态代理 实现方式： 由程序员显式编写代理类。代理类在编译期确定，编译前就存在代理类的字节码文件。 需要实现与目标对象相同的接口，且在代理类中显式调用目标对象的方法。 优点： 结构简单，容易理解。 缺点： 每增加一个接口，都需要编写对应的代理类，代码量大，维护成本高。静态代理类在编译期生成，灵活性差。 JDK动态代理 实现方式： 使用java.lang.reflect.Proxy类和java.lang.reflect.InvocationHandler接口。 代理类在运行时动态生成，不需要显式编写代理类。 优点： 代理类在运行时生成，增加了代码的灵活性和可维护性。 缺点： 只能代理实现了接口的类，不能代理没有实现接口的类。 CGLIB动态代理 实现方式： 使用CGLIB（Code Generation Library），依赖ASM字节码生成框架。 代理类在运行时动态生成，不需要显式编写代理类。 优点： 不要求目标类实现接口，可以代理普通的类。 性能通常比JDK动态代理更高，尤其在代理大量方法调用时更为显著。 缺点： 不能代理final类和final方法。 适用场景： 静态代理：需要手动编写代理类，适用于简单的场景，但不够灵活，维护成本高。 JDK动态代理：适用于实现了接口的类，代理类在运行时生成，灵活性高，但只能代理接口。 CGLIB动态代理：适用于没有实现接口的类，性能优于JDK动态代理，但不能代理final类和final方法，且使用复杂度稍高。 "},{"title":"【SpringBoot】SpringCache轻松启用Redis缓存","date":"2024-07-15T08:20:41.000Z","url":"/2024/07/15/%E3%80%90SpringBoot%E3%80%91SpringCache%E8%BD%BB%E6%9D%BE%E5%90%AF%E7%94%A8Redis%E7%BC%93%E5%AD%98/","tags":[["Redis","/tags/Redis/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"1.前言 Spring Cache是Spring提供的一种缓存抽象机制，旨在通过简化缓存操作来提高系统性能和响应速度。Spring Cache可以将方法的返回值缓存起来，当下次调用方法时如果从缓存中查询到了数据，可以直接从缓存中获取结果，而无需再次执行方法体中的代码。 2.常用注解 @Cacheable：在方法执行前查看是否有缓存对应的数据，如果有直接返回数据，如果没有调用方法获取数据返回，并缓存起来； @CacheEvict：将一条或多条数据从缓存中删除； @CachePut：将方法的返回值放到缓存中； @EnableCaching：开启缓存注解功能； @Caching：组合多个缓存注解。 3.启用缓存 3.1.配置yaml文件 3.2.添加注解 在启动类上添加注解@EnableCaching： 3.3.创建缓存 使用@CachePut注解。当方法执行完后，如果缓存不存在则创建缓存；如果缓存存在则更新缓存。 注解中的value属性可指定缓存的名称，key属性则可指定缓存的键，可使用SpEL表达式来获取key的值。 这里result表示方法的返回值UserInfo，从UserInfo中获取id属性。 3.4.更新缓存 同样使用@CachePut注解。当方法执行完后，如果缓存不存在则创建缓存；如果缓存存在则更新缓存。 3.5.查询缓存 使用@Cacheable注解。在方法执行前，首先会查询缓存，如果缓存不存在，则根据方法的返回结果创建缓存；如果缓存存在，则直接返回数据，不执行方法。 这里使用request表示方法的参数UserQueryRequest。 3.6.删除缓存 使用@CacheEvict注解。当方法执行完后，会根据key删除对应的缓存。 这里可以使用condition属性，当返回结果为true（删除成功）后，才去删除缓存。 3.7.多重操作 使用@Caching注解，通过使用不同的属性进行相应操作。 创建/更新多个缓存： 删除多个缓存： "},{"title":"【RabbitMQ】一文详解消息可靠性","date":"2024-07-13T15:10:21.000Z","url":"/2024/07/13/%E3%80%90RabbitMQ%E3%80%91%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7/","tags":[["RabbitMQ","/tags/RabbitMQ/"],["消息可靠性","/tags/%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7/"]],"categories":[["消息队列","/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"]],"content":"1.前言 RabbitMQ 是一款高性能、高可靠性的消息中间件，广泛应用于分布式系统中。它允许系统中的各个模块进行异步通信，提供了高度的灵活性和可伸缩性。然而，这种通信模式也带来了一些挑战，其中最重要的之一是确保消息的可靠性。 影响消息可靠性的因素主要有以下几点： 发送消息时连接RabbitMQ失败 发送时丢失： 生产者发送的消息未送达交换机； 消息到达交换机后未到达队列； MQ 宕机，队列中的消息会丢失； 消费者接收到消息后未消费就宕机了。 2.生产者 2.1.生产者重连机制 生产者发送消息时，出现了网络故障，导致与MQ的连接中断。为了解决这个问题，RabbitMQ提供的消息发送时的重连机制。即：当RabbitTemplate与MQ连接超时后，多次重试。 在生产者yml文件添加配置开启重连机制 当网络不稳定的时候，利用重试机制可以有效提高消息发送的成功率。但是RabbitMQ提供的重试机制是阻塞式的重试。 如果对于业务性能有要求，建议禁用重试机制。如果一定要使用，就需要合理配置等待时长和重试次数，或者使用异步线程来执行发送消息的代码 2.2.生产者确认机制 RabbitMQ的生产者确认机制（Publisher Confirm）是一种确保消息从生产者发送到MQ过程中不丢失的机制。当消息发送到 RabbitMQ 后，系统会返回一个结果给消息的发送者，表明消息的处理状态。这个结果有两种可能的值： 返回结果有两种方式： publisher-confirm(发送者确认) 消息成功投递到交换机，返回ACK。 消息未投递到交换机，返回NACK。（可能是由于网络波动未能连接到RabbitMQ，可利用生产者重连机制解决） publisher-return(发送者回执) 消息投递到交换机了，但是没有路由到队列。返回ACK和路由失败原因。（这种问题一般是因为路由键设置错误，可以人为规避） 通过这种机制，生产者在发送消息后获取返回的回执结果，从而采取对应的策略，如消息重发或记录失败信息。 3.数据持久化 3.1.配置持久化 在默认情况下，RabbitMQ会将接收到的信息保存在内存中以降低消息收发的延迟。这样会导致两个问题 RabbitMQ宕机，存在内存中的消息会丢失。 内存空间有限，当消费者故障或处理过慢时，会导致消息积压，引发MQ阻塞。 为了提升性能，默认情况下MQ的数据都是在内存存储的临时数据，重启后就会消失。RabbitMQ可以通过配置数据持久化，从而将消息保存在磁盘，包括： 交换机持久化（确保RabbitMQ重启后交换机仍然存在） 队列持久化（确保RabbitMQ重启后队列仍然存在） 消息持久化（确保RabbitMQ重启后队列中的消息仍然存在） 由于Spring会在创建队列时默认将交换机和队列设置为持久化，发送消息时也默认指定消息为持久化消息，因此不需要额外配置。 3.2.惰性队列 从RabbitMQ的3.6.0版本开始，就增加了Lazy Queue的概念，也就是惰性队列。 在3.12版本后，所有队列都是Lazy Queue模式，无法更改。 惰性队列的特点如下： 接收到消息后直接存入磁盘而非内存(内存中只保留最近的消息，默认2048条) 消费者要消费消息时才会从磁盘中读取并加载到内存 支持数百万条的消息存储 对于低于3.12版本的情况，可以使用注解的arguments来指定 3.3.为什么需要数据持久化？ 数据持久化在 RabbitMQ 中有以下重要作用： 队列和交换机的持久化： 防止重启后丢失：将队列和交换机设置为持久化，可以防止 RabbitMQ 服务器重启后丢失这些队列和交换机，确保它们的存在和绑定关系保持不变。 消息的持久化： 安全性： 防止数据丢失：消息持久化后，可以防止 RabbitMQ 服务器重启或宕机时数据丢失，方便数据恢复，保证消息的可靠性和耐久性。 性能： 内存管理：未持久化的临时消息默认存储在内存中。内存空间有限，大量消息涌入时会导致内存占满，系统需要进行 page out 操作将消息写入磁盘。频繁的 page out 操作会严重影响性能。 预防内存溢出：通过持久化消息，可以缓解内存压力，防止因内存溢出导致的系统性能问题和崩溃。 4.消费者 4.1.消费者确认机制 为了确认消费者是否正确处理了消息，RabbitMQ提供了消费者确认机制。当消费者处理消息后，会返回回执信息给RabbitMQ。回执有三种值： ack：消息处理成功，RabbitMQ从队列中删除消息。 nack：消息处理失败，RabbitMQ需要再次投递消息。 reject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除消息。 在SpringBoot项目中，我们可以通过配置文件选择回执信息的处理方式，一共有三种处理方式： none：不处理。RabbitMQ 假定消费者获取消息后会一定会成功处理，因此消息投递后立即返回ack，将消息从队列中删除。 manual：手动模式。需要在业务代码结束后，调用SpringAMQP提供的API发送ack或reject，存在代码侵入问题，但比较灵活。 auto：自动模式。SpringAMQP利用AOP对我们的消息处理逻辑进行了环绕增强，返回结果如下： 如果消费者正常处理消息，自动返回ack并删除队列的消息。 如果消费者消息处理失败，自动返回nack并重新向消费者投递消息。 如果消息校验异常，自动返回reject并删除队列中的消息。 注意：手动模式返回回执消息时通常需要显式指定requeue参数，当requeue=true时，表明消息需要重新入队；当requeue=false时，RabbitMQ将从队列删除消息。 4.2.消息失败重试机制 当消费者出现异常后，消息会不断requeue（重新入队）到队列，再重新发送给消费者，然后再次异常，再次requeue无限循环，导致mq的消息处理飙升，带来不必要的压力。 可以通过设置yml文件开启失败重试机制，在消息异常时利用本地重试，而不是无限制的进行requeue操作。 4.3.消息失败处理策略 在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有 MessageRecoverer 接口来处理，它包含三种不同的实现： RejectAndDontRequeueRecoverer：重试次数耗尽后，直接reject，丢弃消息，这是默认采取的方式； ImmediateRequeueMessageRecoverer：重试次数耗尽后，返回nack，消息重新入队； RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机。 5.死信队列 尽管通过以上设置可以确保消息在生产者、消息队列和消费者之间的传递过程中不会丢失，但在某些情况下，消费者仍可能无法成功处理消息（如消息重试次数耗尽后仍无法被消费）。这时候，我们需要一个机制来妥善处理这些无法被正常消费的消息。死信队列便是用于解决这一问题的兜底机制。 5.1.死信 当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）： 消息被拒绝： 当消费者明确拒绝一个消息并且设置不再重新入队（requeue=false）时，这个消息会被标记为死信。 消息过期： 每个消息或队列可以设置一个TTL（Time-To-Live），即消息的存活时间。如果消息在队列中停留的时间超过了这个TTL，消息会被认为过期，并被转移到死信队列。 队列达到最大长度： 如果队列设置了最大长度并且达到了这个限制，那么新进入的消息会被转移到死信队列中。 5.2.创建死信队列 5.2.1.创建死信交换机和死信队列 正常使用注解，创建交换机和队列即可 5.2.2.绑定死信交换机 如果队列通过dead-letter-exchange属性指定了一个交换机，那么该队列中的死信就会投递到这个交换机中。这个交换机称为死信交换机（Dead Letter Exchange，简称DLX） 可以通过@Argument注解指定死信交互机和路由键，如下。 "},{"title":"【SpringBoot】随机盐值+双重SHA256加密实战","date":"2024-07-08T15:12:15.000Z","url":"/2024/07/08/%E3%80%90SpringBoot%E3%80%91%E9%9A%8F%E6%9C%BA%E7%9B%90%E5%80%BC+%E5%8F%8C%E9%87%8DSHA256%E5%8A%A0%E5%AF%86%E5%AE%9E%E6%88%98/","tags":[["密码加密","/tags/%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"1.SHA-256和Salt 1.1.什么是SHA-256 SHA-256是一种信息摘要算法，也是一种密码散列函数。对于任意长度的消息，SHA256都会产生一个256bit长的散列值（哈希值），用于确保信息传输完整一致，称作消息摘要。这个摘要相当于是个长度为32个字节的数组，通常用一个长度为64的十六进制字符串来表示。 SHA-256的具备以下几个关键特点： 固定长度输出：无论输入数据的大小，SHA-256都会产生一个256位（32字节）的固定长度散列值。 不可逆性：SHA-256的设计使得从生成的散列值无法还原原始输入数据。这种不可逆性在安全性上是非常重要的。 抗碰撞性：找到两个不同的输入数据具有相同的散列值（碰撞）是极其困难的。虽然理论上碰撞可能发生，但SHA-256被设计得非常抗碰撞。 除了SHA-256之外，还有一个密码散列函数MD5，过去也常被用于密码加密，但MD5在安全性上低于SHA-256，现在已经很少用于密码加密了，本文不做考虑。 SHA-256 和 MD5 的比较： 特性 SHA-256 MD5 输出长度 256 位（64 个十六进制字符） 128 位（32 个十六进制字符） 安全性 高 低 计算速度 较慢 快 抗碰撞能力 强 弱 应用场景 数据完整性校验、数字签名、密码存储、区块链 曾用于文件校验、密码存储 推荐使用 是 否 1.2.什么是随机盐值 盐值（salt）是一种在密码学和安全计算中常用的随机数据，用于增强密码散列的安全性。 随机盐值（random salt）是一种用于增强密码散列安全性的技术。它是一个随机生成的数据块，在将密码输入散列函数之前，将盐值与密码组合。通过引入随机盐值，可以有效地防止彩虹表攻击和相同密码散列值重复的问题。 盐值的作用： 防止彩虹表攻击： 彩虹表是一个预计算的哈希值数据库，用于快速查找常见密码的哈希值。通过在密码哈希之前加入随机盐值，即使密码相同，其最终的哈希值也会不同，从而使彩虹表无效。 避免散列值重复： 如果两个用户使用相同的密码，在没有盐值的情况下，他们的哈希值会相同。加入盐值后，即使密码相同，生成的哈希值也会不同，这有助于防止攻击者通过观察哈希值来推测用户是否使用了相同的密码。 增加攻击难度： 盐值增加了密码哈希的复杂性。即使攻击者获取了存储的哈希值和盐值，他们仍需对每个盐值进行单独的暴力破解，显著增加了破解的时间和计算成本。 1.3.如何进行加密操作 本文采用的加密方式是在前端采用md加密防止明文传输，后端对密码二次加密后再进行随机盐值的混入。 2.前端实现 引入md5.min.js 3.后端实现 3.1.导入Maven依赖 3.2.密码加密 3.2.1.密码加盐 首先使用Apache的RandomStringUtils工具类，生成16位的盐值。然后将盐拼接到明文后面，进行SHA256加密。 这个加密后的SHA256是个固定64长度的字符串。 3.2.2.随机盐值混合 加盐后的SHA256码长度为80位，这里我们采用的盐值混合规则：将SHA-256散列值的每四个字符中间插入一个盐值字符，依次交替排列。 这样就完成了加密的操作：密码加盐 + 盐值混合。 3.3.密码解密 3.3.1.提取盐值和加盐密码 按照加密时采用的规则：将SHA-256散列值的每四个字符中间插入一个盐值字符，依次交替排列。 我们可以将盐值和加盐后的SHA-256码 3.3.2.比较密码 最后，将取出的盐值与原始密码再次加盐，再次得到加盐密码，与sha256Hex比较即可判断密码是否相同。 3.4.完整工具类 "},{"title":"【消息队列】RabbitMQ基本概念","date":"2024-06-30T15:52:12.469Z","url":"/2024/06/30/%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","tags":[["RabbitMQ","/tags/RabbitMQ/"]],"categories":[["消息队列","/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"]],"content":"1.RabbitMQ的架构 Producer(生产者)：生产者是消息的发送方，负责将消息发布到RabbitMQ的交换器 (Exchange)。 Consumer(消费者)：消费者是消息的接收方，负责从队列中获取消息，并进行处理和消费。 Exchange(交换器)：交换器是消息的接收和路由中心，它接收来自生产者的消息，并将消息路由到一个或多个与之绑定的队列(Queue)中。 Queue(队列)：队列是消息的存储和消费地，它保存着未被消费的消息，等待消费者(Consumer) 从队列中获取并处理消息。 Binding(绑定)：绑定是交换器和队列之间的关联关系，它定义了交换器将消息路由到哪些队列中。 VHost(虚拟主机)：它类似于操作系统中的命名空间，用于将RabbitMQ的资源进行隔离和分组。每个VHost拥有自己的交换器、队列、绑定和权限设置，不同VHost之间的资源相互独立，互不干扰。VHost可以用于将不同的应用或服务进行隔离，以防止彼此之间的消息冲突和资源竞争。 2.RabbitMQ的工作模式 简单模式（Simple Mode）：在这种模式下，有一个生产者和一个消费者。生产者发送消息，消费者接收并处理这些消息。这种模式是最简单的RabbitMQ使用方式。 Work模式（Work Queues）：在这种模式下，有一个生产者和多个消费者。生产者发送消息到队列中，消费者从队列中取出消息进行处理。RabbitMQ通过轮询的方式将消息平均发送给消费者，确保一条消息只被一个消费者接收和处理。 发布/订阅模式（Publish/Subscribe）：在这种模式下，生产者发送消息到交换机，交换机将消息广播到所有与之绑定的队列中，然后消费者从队列中取出消息进行消费。这种模式允许消费者有选择性地接收消息。 路由模式（Routing）：路由模式与发布/订阅模式类似，但是生产者发送消息时需要指定一个路由键（routing key），交换机根据路由键将消息发送到匹配的队列中。消费者需要将其队列绑定到交换机，并指定路由键以便接收消息。 Topic模式：这种模式是路由模式的扩展，它使用更灵活的匹配规则。生产者发送消息时指定一个路由键，消费者可以将其队列绑定到交换机上，并指定一个模式（topic），该模式可以包含通配符，用于匹配路由键。这样，消费者可以接收符合特定模式的所有消息。 RPC模式：支持生产者和消费者不在同一个系统中，即允许远程调用的情况。通常，消费者作为服务端，放置在远程的系统中，提供接口，生产者调用接口，并发送消息。 3.RabbitMQ的作用 应用解耦：在生产者和消费者之间建立了一个缓冲区，使得两者之间的处理速度可以异步进行，不需要严格匹配。这大大增加了系统的灵活性和可扩展性。 削峰填谷：在高并发场景下，大量请求可能瞬间涌入系统，导致系统压力过大。RabbitMQ可以作为一个缓冲存储，将一部分请求暂时存入队列中，起到“削峰”的作用，保证系统平稳运行，在高峰期过去后，继续从队列中取出消息进行处理，直到积压的消息被完全消费。 异步通信：RabbitMQ支持异步通信，生产者将消息发送到队列后，不必等待消费者处理完消息再返回结果，而是可以继续执行其他任务。这大大提高了系统的并发处理能力和响应速度。 流量整形：RabbitMQ可以对消息进行优先级排序、延迟处理等，使得消息的处理更加有序和高效。 4.如何处理RabbitMQ的消息积压问题？ 增加消费者数量：当消息队列中的消息数量超出当前消费者的处理能力时，可以动态地增加消费者的数量。这样，更多的消费者可以并行地处理消息，从而加快消息的消费速度。 提高消费者的处理能力：优化消费者的代码逻辑，提升消费者的性能，例如通过多线程或其他并行处理技术，可以提高单个消费者的处理速度。 设置消息的过期时间：为了避免消息无限期地积压在队列中，可以为消息设置一个合理的过期时间。当消息在队列中等待时间过长且未被消费时，RabbitMQ可以自动将其丢弃或进行其他处理。 5.RabbitMQ的死信队列 死信队列介绍 RabbitMQ的死信队列(Dead Letter Queue，简称DLQ)是一种用于处理消息处理失败或无法路由的消息的机制。它允许将无法被正常消费的消息重新路由到另一个队列，以便稍后进行进一步的处理、分析或排查问题。 当消息队列里面的消息出现以下几种情况时，就可能会被称为”死信”: 消息处理失败：当消费者由于代码错误、消息格式不正确、业务规则冲突等原因无法成功处理一条消息时，这条消息可以被标记为死信。 消息过期：在RabbitMQ中，消息可以设置过期时间。如果消息在规定的时间内没有被消费，它可以被认为是死信并被发送到死信队列。 消息被拒绝：当消费者明确拒绝一条消息时，它可以被标记为死信并发送到死信队列。拒绝消息的原因可能是消息无法处理，或者消费者认为消息不符合处理条件。 消息无法路由：当消息不能被路由到任何队列时，例如，没有匹配的绑定关系或路由键时，消息可以被发送到死信队列。 当消息变成”死信”之后，如果配置了死信队列，它将被发送到死信交换机，死信交换机将死信投递到一个队列上，这个队列就是死信队列。但是如果没有配置死信队列，那么这个消息将被丢弃。 配置死信队列 在RabbitMQ中，死信队列通常与交换机(Exchange) 和队列(Queue) 之间的绑定关系一起使用。要设置死信队列，通常需要以下步骤: 创建死信队列：定义一个用于存储死信消息的队列。 创建死信交换机：为死信队列定义一个交换机，通常是一个direct类型的交换机。 将队列与死信交换机绑定：将主要队列和死信交换机绑定，以便无法处理的消息能够被转发到死信队列。 在主要队列上设置死信属性：通过设置队列的x-dead-letter-exchange和x-dead-letter-routing-key属性，来指定死信消息应该被发送到哪个交换机和路由键。 当消息被标记为死信时，它将被发送到死信队列，并可以由应用程序进一步处理、审查或记录。这种机制有助于增加消息处理的可靠性和容错性，确保不丢失重要的消息，并提供了一种处理失败消息的方式。 6.RabbitMQ如何实现延迟队列 死信队列 当RabbitMQ中的一条正常的消息，因为过了存活时间(TTL过期)、队列长度超限、被消费者拒绝等原因无法被 消费时，就会变成Dead Message，即死信。 实现方法 当一个消息变成死信之后，他就能被重新发送到死信队列中(其实是交换机-exchange)。基于这样的机制，就可以实现延迟消息了。那就是我们给一个消息设定TTL，但是并不消费这个消息，等他过期，过期后就会进入到死信队列，然后我们再监听死信队列的消息消费就行了。 而且，RabbitMQ中的这个TTL是可以设置任意时长的，这相比于RocketMQ只支持一些固定的时长而显得更加灵活一些。 死信队列实现延迟队列的缺点 但是，死信队列的实现方式存在一个问题，那就是可能造成队头阻塞，因为队列是先进先出的，而目每次只会判断队头的消息是否过期，那么，如果队头的消息时间很长，一直都不过期，那么就会阻塞整个队列，这时候即使排在他后面的消息过期了，那么也会被一直阻塞。 基于RabbitMQ的死信队列，可以实现延迟消息，非常灵活的实现定时关单，并且借助RabbitMQ的集群扩展性可以实现高可用，以及处理大并发量。他的缺点一是可能存在消息阻塞的问题；二是方案比较复杂，不仅要依赖RabbitMQ，而且还需要声明很多队列出来，增加系统的复杂度。 RabbitMQ插件 实现方法 基于插件的方式，消息并不会立即进入队列，而是先把他们保存在一个基于Erlang开发的Mnesia数据库中，然后通过一个定时器去查询需要被投递的消息，再把他们投递到x-delayed-message交换机中。 基于RabbitMQ插件的方式可以实现延迟消息，并且不存在消息阳塞的问题，但是因为是基于插件的，而这个插件 支持的最大延长时间是(232)-1 毫秒，大约49天，超过这个时就会被立即消费。 RabbitMQ插件实现延迟队列的缺点 不过这个方案也有一定的限制，它将延迟消息存在于 Mnesia 表中，并且在当前节点上具有单个磁盘副本，存在丢失的可能。 目前该插件的当前设计并不真正适合包合大量延迟消息(例如数十万或数百万)的场景，另外该插件的一个可变性来源是依赖于 Erlang 计时器，在系统中使用了一定数量的长时间计时器之后，它们开始争用调度程序资源，并且时间漂移不断累积。 7.保证RabbitMQ的消息可靠性 为了确保消息不丢失，可以采取以下措施： 持久化消息： 将消息持久化到磁盘中，这样即使系统崩溃，消息也不会丢失。常见的MQ如RabbitMQ、Kafka、ActiveMQ都支持消息持久化。 确认机制（Acknowledgment）： 发送方和接收方都需要确认消息的接收。发送方在发送消息后等待接收方的确认，接收方处理消息后需要确认已成功处理。 重复发送： 如果在设定时间内没有收到确认，发送方可以重试发送消息。这需要消息处理具备幂等性，即多次处理同一条消息不会造成副作用。 死信队列（Dead Letter Queue, DLQ）： 当消息无法被成功处理或无法被确认时，将其转移到死信队列中进行后续处理。 高可用集群： 通过MQ集群来实现高可用性，防止单点故障导致消息丢失。 "},{"title":"【并发编程】线程池参数及创建方法","date":"2024-05-27T15:43:21.000Z","url":"/2024/05/27/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E5%8F%8A%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95/","tags":[["线程池","/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"1.什么是线程池 线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是等待下一个任务。 为什么要使用线程池？ 使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 2.线程池七大参数 2.1.核心线程数 corePoolSize：线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了allowCoreThreadTimeOut。当提交一个新任务时，如果线程池中的线程数小于corePoolSize，那么就会创建一个新线程来执行任务。 2.2.最大线程数 maximumPoolSize：线程池中允许的最大线程数。在当前线程数达到corePoolSize后，如果继续有任务被提交到线程池，会将任务存放到工作队列中。如果队列也已满，则会去创建一个新线程处理任务。 2.3.空闲线程存活时间 keepAliveTime：非核心线程的空闲存活时间。当线程数大于corePoolSize时，空闲时间超过keepAliveTime的线程将被终止。 这个参数在设置了allowCoreThreadTimeOut=true时对核心线程同样有效。 2.4.空闲线程存活时间单位 unit：keepAliveTime的计量单位。例如，TimeUnit.SECONDS、TimeUnit.MILLISECONDS等。 2.5.工作队列 workQueue：新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列： ①ArrayBlockingQueue 基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。 ②LinkedBlockingQuene 基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而基本不会去创建新线程直到maxPoolSize（很难达到Interger.MAX这个数），因此使用该工作队列时，参数maxPoolSize其实是不起作用的。 ③SynchronousQuene 一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。 ④PriorityBlockingQueue 具有优先级的无界阻塞队列，优先级通过参数Comparator实现。 2.6.线程工厂 threadFactory：创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等 2.7.拒绝策略 handler：当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，线程池会执行拒绝策略。JDK中提供了4中拒绝策略： ①CallerRunsPolicy 该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。 ②AbortPolicy 该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。 ③DiscardPolicy 该策略下，直接丢弃任务，什么都不做。 ④DiscardOldestPolicy 该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列 3.如何设定线程池大小 一般来说，有两种类型的线程：CPU 密集型和 IO 密集型。 CPU 密集型的线程主要进行计算和逻辑处理，需要占用大量的 CPU 资源。 IO 密集型的线程主要进行输入输出操作，如读写文件、网络通信等，需要等待 IO 设备的响应，而不占用太多的 CPU 资源。 常见且简单的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 4.如何创建线程池 方式一：通过ThreadPoolExecutor构造函数来创建（推荐）。 通过这种方式可以根据服务器硬件配置，灵活设定线程池参数。 方式二：通过 Executor 框架的工具类 Executors 来创建（不推荐）。 FixedThreadPool：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。初始大小为 0。当有新任务提交时，如果当前线程池中没有线程可用，它会创建一个新的线程来处理该任务。如果在一段时间内（默认为 60 秒）没有新任务提交，核心线程会超时并被销毁，从而缩小线程池的大小。 ScheduledThreadPool：该方法返回一个用来在给定的延迟后运行任务或者定期执行任务的线程池。 Executors 返回线程池对象的弊端 FixedThreadPool 和 SingleThreadExecutor：使用的是无界的 LinkedBlockingQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool：使用的是同步队列 SynchronousQueue, 允许创建的线程数量为 Integer.MAX_VALUE ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool 和 SingleThreadScheduledExecutor : 使用的无界的延迟阻塞队列DelayedWorkQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 "},{"title":"【SpringBoot】MapStruct实现优雅的数据复制","date":"2024-05-05T14:10:21.000Z","url":"/2024/05/05/%E3%80%90SpringBoot%E3%80%91MapStruct%E5%AE%9E%E7%8E%B0%E4%BC%98%E9%9B%85%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/","tags":[["数据复制","/tags/%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/"],["MapStruct","/tags/MapStruct/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":" 做项目时你是否遇到过以下情况： DTO（数据传输对象）与Entity之间的转换：在Java的Web应用中，通常不会直接将数据库中的Entity实体对象返回给前端。而是会创建一个DTO对象，这个DTO对象只包含需要返回给前端的字段。此时，就需要将Entity转换为DTO。 复杂对象的映射：当需要映射的对象包含大量的字段，或者字段之间存在复杂的依赖关系时，手动编写映射代码不仅繁琐，而且容易出错。 1.为什么选择MapStruct 1.1.常见的属性映射方法 一般来说，不使用MapStruct框架进行属性映射，常有的方法以下两种： Getter/Setter方法手动映射 这种方法最朴素，手动编写代码将源对象的属性存入目标对象，需要注意实体类中嵌套属性的判空操作以防止空指针异常。 BeanUtils.copyProperties()方法进行映射 BeanUtils底层使用的是反射机制实现属性的映射。反射是一种在运行时动态获取类信息、调用方法或访问字段的机制，无法利用JVM的优化机制，因此通常比直接方法调用慢得多。 此外，BeanUtils 只能同属性映射，或者在属性相同的情况下，允许被映射的对象属性少；但当遇到被映射的属性数据类型被修改或者被映射的字段名被修改，则会导致映射失败。 1.2.MapStruct的优势 MapStruct是一个基于注解的Java代码生成器，它通过分析带有@Mapper注解的接口，在编译时自动生成实现该接口的映射器类。这个映射器类包含了用于执行对象之间映射的具体代码。 与常规方法相比，MapStruct具备的优势有： 简化代码。对于对象内属性较多的情况，使用MapStruct框架无须手动对每个属性进行get/set和属性判空操作。MapStruct可以通过注解和映射接口来定义映射规则，自动生成映射代码，从而大大简化了这种复杂对象的映射过程。 性能优越。相较于反射这种映射方法，MapStruct在编译期生成映射的静态代码，可以充分利用JVM的优化机制，对于企业级的项目应用来说，这种方式能大大提高数据复制的性能。 类型安全。由于MapStruct在编译期生成映射代码，这意味着如果源对象和目标对象的映射存在错误，那么可以在编译时就发现错误。相比之下，BeanUtils在运行时使用反射来执行属性复制，这可能会导致类型不匹配的问题在运行时才发现。 灵活映射。MapStruct可以轻松处理嵌套对象和集合的映射。对于嵌套对象，MapStruct可以递归地应用映射规则；对于集合，MapStruct可以自动迭代集合中的每个元素并应用相应的映射规则。 有开发者对比过两者的性能差距，如下表。这充分体现了MapStruct性能的强大。 对象转换次数 属性个数 BeanUtils耗时 MapStruct耗时 5千万次 6 14秒 1秒 5千万次 15 36秒 1秒 5千万次 25 55秒 1秒 2.MapStruct快速入门 在快速入门中，我们的任务是将dto的数据复制到实体类中。 2.1.导入Maven依赖 2.2.创建相关对象 注意，实体类要具有get/set方法，这里我使用了lombok的@Data注解来实现。 dto类我使用了@Builder注解，可以快速为对象赋初始值。 2.3.创建转换器Converter 使用抽象类来定义转换器，只需中@Mapping注解中填写target和source的字段名，即可实现属性复制。 2.4.测试 在SpringBoot的测试类中测试，这里我使用DTO类的@Builder注解提供的方法为dto赋初值模拟实际开发，通过调用converter的方法实现属性映射。 结果如图： 最后，我们可以发现在target包的converter的相同目录下，生成了TestConverter的实现类 里面为我们编写好了映射的代码。 3.MapStruct进阶操作 如果仅是这种简单层级的对象映射，还不足以体现MapStruct的灵活性。下面将介绍MapStruct的进阶技巧。 3.1.嵌套映射 假设我们的Hotel实体类中嵌套了另外一个实体类Master dto对象为： 我们需要把personName和personAge映射到Hotel实体类的Master中，怎么做？ 很简单，只需要在target属性中加上Hotel实体类嵌套实体类的字段名，加字符.，再跟上嵌套类的字段名即可 结果如图： 3.2.集合映射 如果源对象和目标对象的集合的元素类型都是基本数据类型，直接在target和source中填写字段名即可。 若源对象和目标对象的集合元素类型不同，怎么做？ 这个案例我们需要把DTO的personList映射到masterList中。 编写converter，这次需要进行两层映射。 第一层将person集合映射到master集合上。 第二层将person对象的属性映射到master对象中。 结果如图： 查看target包下的代码，可以发现MapStruct除了两层映射外，还帮你自动生成了迭代集合添加元素的代码，从而实现集合元素的复制。 4.字段的逻辑处理 4.1.复杂逻辑处理（qualifiedByName和@Named） 这次我们需要把dto中的personName和personAge的list集合映射到实体类的masters集合中。常规的集合映射无法处理这种情况，这时需要使用到qualifiedByName和@Named进行特殊处理。 这就需要拿到两个list的数据，进行手动处理了。在@Mapping注解的qualifiedByName属性指定方法名定位处理逻辑的方法，@Named(\"dtoToMasters\")。 利用stream流进行处理。 返回结果： 4.2.额外逻辑处理（ignore和@AfterMapping） @Mappings的ignore属性，也可以对一个字段（不能是集合）进行额外逻辑处理。通常搭配@AfterMapping注解使用。 这个案例中，我们需要根据DTO的mount属性判断是否大于15，如果大于，则判断hotel实体类的isSuccess为true 编写converter，注意@AfterMapping注解下的方法的参数列表，需要使用@MappingTarget注解指明目标对象， 测试方法 返回结果 4.3.简单逻辑处理（expression） expression可以在注解中编写简单的处理逻辑 在这个案例中我需要在实体类的nowTime字段获取当前时间。 直接在expression属性中使用方法获取当前时间。 结果如下 "},{"title":"【项目开发】Java调用Python方法一文详解","date":"2024-04-26T08:23:20.000Z","url":"/2024/04/26/%E3%80%90%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E3%80%91Java%E8%B0%83%E7%94%A8Python%E6%96%B9%E6%B3%95/","tags":[["调用python","/tags/%E8%B0%83%E7%94%A8python/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"前言 在这个人工智能技术迅速发展的时代，对于我们学生而言，参加软件竞赛已不再是单纯的技术比拼。传统的纯Java编写项目，虽然有其稳定与高效的优势，但在面对日益复杂的算法需求时，其竞争力已逐渐减弱。因此，将Java与Python这两种编程语言的优势相结合，实现算法与软件的完美融合，已成为提升项目竞争力的关键。 本文将详细讲解使用Java调用Python的三大方法，并分析各个方法的优势。 1.jython库（不推荐） 首先在pom.xml中导入jython对应依赖 1.1.手动编写Python语句 这里我们编写一个简单的a + b函数的实现样例。 这样就可以得到返回结果 1.2.读取Python文件进行调用 编写一个jythonTest.py文件 使用PythonInterpreter.execfile方法调用py文件 总结： 上述两个方法的优点是其集成性，Jython允许你在Java中直接执行Python代码，使得Java和Python代码可以直接进行交互。 缺点也是明显的，首先，Jython说到底是Java的库，可能无法完全支持Python的所有库和功能，其次，Python工程师的工作高度耦合在Java代码中，如果你和你的Python同事不能够忍受这种开发方式，那么就不要用这种方法。 2.Java调用命令行（推荐） 这种方法的原理是通过Java代码调用操作系统的命令行接口，然后在命令行中执行Python脚本。 Java程序可以通过Runtime.getRuntime().exec()方法或者更高级的ProcessBuilder类来实现这一功能。执行Python脚本后，Java程序可以通过InputStream流来捕获并处理Python脚本的输出结果。 2.1.Runtime.getRuntime().exec()方法调用 首先需要编写执行Python脚本的命令行语句 使用Runtime.getRuntime().exec()方法执行命令 使用process.getInputStream()捕获InputStream流，从而获取执行结果 使用process.getErrorStream()捕获标准错误流（可选），如果Python语句报错会打印报错信息 等待进程结束 ，使用process.waitFor()方法获取返回码 完整代码为： 2.2.ProcessBuilder类调用（最推荐） 编写命令 使用ProcessBuilder启动进程 完整代码： 总结： 两种方法都是通过创建一个Process进程调用命令行获取Python脚本的执行结果。主要区别为第二种方法需要额外创建一个ProcessBuilder类，ProcessBuilder类接收的是String数组，相较于Runtime只接收一个String字符串，ProcessBuilder类编写命令更加灵活。 3.调用云端模型（最推荐） 这种方法需要Python工程师在百度云、腾讯云等云平台上开启接口，然后通过Hutool的工具类发送HTTP请求调用云端接口。 首先需要导入Hutool库 填写接口地址，使用Map作为表单数据（一般使用表单，比较灵活，文件和文本都可以传输） 使用HttpRequest发送请求 判断返回结果合法性，进行相关处理 获取返回结果 完整代码： 4.总结 jython库 优点： 直接在Java程序中执行，可以直接利用Java虚拟机（JVM）的性能优势，减少进程间通信的开销。 缺点： Jython可能无法完全支持Python的所有库和功能。 Java调用命令行 优点： Java和Python进程是独立的，这使得它们可以更容易地并行运行，而不会影响彼此的性能。 对于对硬件要求比较高的Python模型， 本地部署可能存在一定的困难。 对于开发人员比较友好，两者的开发工作分离。 缺点： 进程间通讯可能会引入额外的开销和复杂性。 调用云端模型 优点： 对于开发人员比较友好，两者的开发工作分离。 利于构建大型算法模型，如百度云等云端平台对于Python模型支持度很高，比起本地更容易部署。 缺点： 需要Java和Python工程师对HTTP请求有一定了解。 云端接口需要支付一定的费用。 "},{"title":"【SpringSecurity】认证授权全流程实战","date":"2024-01-18T08:20:13.000Z","url":"/2024/01/18/%E3%80%90SpringSecurity%E3%80%91%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%85%A8%E6%B5%81%E7%A8%8B%E5%AE%9E%E6%88%98/","tags":[["身份校验","/tags/%E8%BA%AB%E4%BB%BD%E6%A0%A1%E9%AA%8C/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"介绍 Spring Security是一个强大且灵活的身份验证和访问控制框架，用于Java应用程序。它是基于Spring框架的一个子项目，旨在为应用程序提供安全性。 Spring Security致力于为Java应用程序提供认证和授权功能。开发者可以轻松地为应用程序添加强大的安全性，以满足各种复杂的安全需求。 SpringSecurity完整流程 JwtAuthenticationTokenFilter：这里是我们自己定义的过滤器，主要负责放行不携带token的请求（如注册或登录请求），并对携带token的请求设置授权信息 UsernamePasswordAuthenticationFilter：负责处理我们在登陆页面填写了用户名密码后的登陆请求。入门案例的认证工作主要由它负责 ExceptionTranslationFilter：处理过滤器链中抛出的任何AccessDeniedException和AuthenticationException FilterSecurityInterceptor：负责权限校验的过滤器。 一般认证工作流程 Authentication接口：它的实现类表示当前访问系统的用户，封装了用户相关信息。 AuthenticationManager接口：定义了认证Authentication的方法 UserDetailsService接口：加载用户特定数据的核心接口。里面定义了一个根据用户名查询用户信息的方法。 UserDetails接口：提供核心用户信息。通过UserDetailsService根据用户名获取处理的用户信息要封装成UserDetails对象返回。然后将这些信息封装到Authentication对象中。 数据库 数据库的采用RBAC权限模型（基于角色的权限控制）进行设计。 RBAC至少需要三张表：用户表–角色表–权限表（多对多的关系比较合理） 用户表（user）：存储用户名、密码等基础信息，进行登录校验 角色表（role）：对用户的角色进行分配 权限表（menu）：存储使用不同功能所需的权限 注册流程 配置匿名访问 在配置类中允许注册请求可以匿名访问 编写实现类 registerDTO中存在字符串roleId和实体类user，先取出user判断是否存在相同手机号。若该手机号没有注册过用户，对密码进行加密后即可将用户存入数据库。 创建register方法映射，保存用户的同时也要将roleId一并存入关系表中，使用户获得对应角色。如下图。 登录流程 配置匿名访问 在配置类中允许登录请求可以匿名访问 调用UserDetailsServiceImpl 登录流程一般对应认证工作流程 先看这段代码：UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(user.getUserPhone(), user.getUserPassword());这里先用用户手机号和密码生成UsernamePasswordAuthenticationToken 再看这段代码：Authentication authenticate = authenticationManager.authenticate(authenticationToken);利用authenticate调用自定义实现类UserDetailsServiceImpl，根据用户名判断用户是否存在（对应认证流程的1、2、3、4） 实现UserDetailsServiceImpl 由于试下的是UserDetailsService接口，所以必须实现其方法loadUserByUsername（根据用户名查询数据库是否存在）这里我传入的是手机号。数据库中若存在用户，则返回UserDetails对象（这里的权限信息暂且不看，对应认证流程的5、5.1、5.2、6） UserDetails对象返回后，authenticate方法会默认通过PasswordEncoder比对UserDetails与Authentication的密码是否相同。因为UserDetails是通过自定义实现类从数据库中查询出的user对象，而Authentication相当于是用户输入的用户名和密码，也就可以理解为通过前面自定义实现类利用用户名查询到用户后，再看这个用户的密码是否正确。如果用户名或密码不正确，authenticate将会为空，则抛出异常信息。（对应认证流程的7） 由于这里的登录流程不涉及8，9，10，所以不再叙述。 在剩下的代码中我们利用用userId生成了jwt的令牌token，将其存入Redis中并返回token给前端。 登出流程 编写过滤器 除login、register请求外的所有请求都需要携带token才能访问，因此需要设计token拦截器代码，如下。 对于不携带token的请求（如登录/注册）直接放行；对于携带token的请求先判断该用户是否登录，即redis中是否存在相关信息，若存在，将用户授权信息存入SecurityContextHolder，方便用户授权，最后直接放行。 此外，还需将token拦截器设置在过滤器UsernamePasswordAuthenticationFilter的前面。 编写实现类 获取SecurityContextHolder中的用户id后，删除redis中存储的值，即登出成功。 授权流程 确保实现类正确编写： 在token拦截器中，我们添加了这段代码。 这样非登录/注册请求都会被设置授权信息。 为对应接口添加注解@PreAuthorize，就会检验该请求是否存在相关请求。 完整代码 config类 controller类 dto类 entity类 UserDetails的实现类 filter类 handler类 service实现类 utils类 "},{"title":"中国大学生服务外包创新创业大赛赛题分析","date":"2024-01-08T12:53:12.000Z","url":"/2024/01/08/%E8%B5%9B%E9%A2%98/","tags":[["赛题分析","/tags/%E8%B5%9B%E9%A2%98%E5%88%86%E6%9E%90/"]],"categories":[["比赛","/categories/%E6%AF%94%E8%B5%9B/"]],"content":"【A01】 基于文心大模型的智能阅卷平台设计与开发 技术栈 算法部分：PaddleOCR+NLP+文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 问题及解决方案 评阅效率与成本 问题描述：传统评阅面临评阅速度较慢、人力需求大的问题，特别是在大量试卷需要评阅时，评阅及反馈时效性能否被满足，将是一个巨大的挑战； 关键点：大量试卷、评阅及反馈时效性 解决方案： 批量评阅，选择、填空、判断题等机器评阅，计算题、简答题等主观题需人工阅卷。主观题可尝试利用自然语言处理（NLP）技术，让机器能够更准确地理解和评价学生的文字表达，将题干和答案输入给文心大模型，由大模型给阅卷老师提出建议辅助阅卷。 分配任务，将试卷任务划分为多个子任务，由多个评阅者同时处理，以提高评阅速度；将试卷任务分配给多个评阅者，多个评阅者负责不同数量的试卷。 实时反馈，设计评阅者和学生两个角色，让评阅者或学生能够迅速获取评阅结果并及时进行反馈，如老师评阅完后生成时间等数据、试卷评阅完后发送信息给学生，除分数外还可以包括评语和建议。学生端可以设计往期的数据统计、教师端可以设计往期评阅趋势等。 有阅无评 问题描述：在实际评阅场景中。往往需要更具有针对性和个性化的评价和建议，而不仅仅是分数反馈。在这过程中，评阅者需要将关键概念，结合学生自身特点进行充分关联，提供更贴近学生需求的知识延展和学科建议； 关键点：针对性和个性化的评价和建议、关联学生特点、提供知识延展和学科建议 解决方案： 反馈设计，评阅者可以输入关键概念给文心大模型，基于文心大模型给出专业评价和建议，或者评阅者自行写出评价和建议。学生端收到反馈后，可以使用文心大模型提取反馈中的知识，并给出学习建议。 增加多样性，包括提供更多的学科相关资源链接、在线学习平台等。这可以帮助学生更全面地理解和拓展相关知识。 学情信息跟踪实际 问题描述：评阅者在实际操作中难以全面、数字化地记录学生的评价和进步情况，进而影响评估的针对性和指导性作用； 关键点：全面、数字化记录学生的评价和进步情况 解决方案： 学情数据可视化，评阅者可以看到该学生历届考试的试卷、评阅建议或成绩趋势图等。 实际评阅风格的单一 问题描述：在实际评阅过程中，评语描述表达方式相对统一，难以通过多样化的评语风格来更好地引导学生理解知识点和提高学业水平； 关键点：多样化的评语风格 解决方案： 通用评语库， 构建一个通用的评语库，包含常见的评价和建议，评阅者可以从中选择或修改，以提高评价的一致性和效率。还可以提供评阅者多样化的评语模板，包括鼓励性的、建议性的、肯定的、挑战性的等不同类型。评阅者可以根据学生的表现灵活选择合适的模板，使评语更富有变化。 学科限制与切换 问题描述：由于不同学科具有独特的评价标准和要求，传统评阅工具难以在不同学科间切换，需要更多的学科专业性和差异化的评阅方式。 关键点：更多的学科专业性和差异化的评阅方式 解决方案： 学科试卷分配给对应的学科老师 学科专业化的大模型支持， 集成不同学科领域的专业化大模型，使其能够理解和应用特定学科的术语、概念和评价标准。这有助于提高评阅的准确性和专业性？？？ 任务清单 （1）试卷图像快速采集与存储； （2）字符识别与提取； （3）内容理解与评阅内容生成； （4）评阅内容的二次编辑； （5）评阅结果的可视化、整理与导出； （6）学情数据可视化； （7）跨平台支持； （8）实时采集与分析（可选）； （9）其它拓展功能和创新方向，如软硬一体解决方案。 功能模块 试卷导入模块 评阅分配模块 智能评阅模块 学情分析模块 个人信息模块 【A15】基于知识图谱的大学生就业能力评价和职位推荐系统 技术栈 算法部分：知识图谱、NLP、推荐和预测模型、文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 参考网站 老鱼简历： 牛客： 数据集 链接： 提取码：vmeh 技术要求与指标 推荐有效性达到80% 以上（用户调查：电子或计算机类相关专业毕业生简历与岗位样例库进行匹配）； 系统数据库中个人隐私信息（姓名、手机、邮箱、通讯地址等至少4项）进行加密，只能被授权人员解密； 架构设计上可并发支持1000人以上同时在线使用，推荐响应时间在5s以内； 技术不限，开发工具不限，可采用开源技术。 业务背景 知识图谱构建 职位推荐系统需要构建一个包含相关职位、技能要求、行业信息等多个领域知识的知识图谱。 聚合和分析用户信息 与知识库相结合，对用户的个人简历、求职意向、工作经验等信息进行聚合和分析处理。根据用户提供的信息学习用户的兴趣和特征，并利用这些特征在知识图谱上匹配职位需求和其他相关信息。 建立推荐模型 根据用户信息设计推荐算法和预测模型。通常来说，推荐算法有协同过滤、关键因素模型、深度学习模型等，具体选择应该根据实际情况来确定。 推荐结果展示 将推荐结果呈现给用户，并支持用户进行选择和反馈。需要注意的是推荐结果的呈现方式也应该根据不同用户群体的需求和喜好来定制化。 问题及解决方案 实现一套大学生就业能力评价和智能岗位推荐系统，根据提供的岗位信息样例库，设计一套含智能算法的软件系统方案：基于知识图谱的岗位信息，利用职位和用户信息，结合推荐算法和相关技术，为用户提供符合其需求和兴趣的职位推荐结果和能力评价结果。 能力评价 问题描述：用户上传个人简历，并明确自己期望的职位，系统自动判断用户与期望职位间的契合度，差异性，给出提升建议，让其知晓对于意向岗位自身知识和技能上的缺陷，找到短板，有针对性提升自我能力。 关键点：判断用户与期望职位间的契合度、给出提升建议 解决方案： 信息提取，利用自然语言处理（NLP）技术提取用户简历和期望职位中的关键词和技能。设计算法计算技能匹配度分数，考虑关键技能的重要性和权重。 构建知识图谱，基于构建的知识图谱，将用户的技能与职位要求进行匹配。利用图谱中的关系和节点信息，评估用户对于职位所需知识的覆盖程度。 差异性分析，分析用户的简历和期望职位之间的差异，包括技能、工作经验、项目经历等方面。 提升建议生成，基于差异性分析和匹配度评估，生成个性化的提升建议。针对用户的短板提供培训建议、学习资源链接、实践项目建议等，以提高其在期望职位上的竞争力。 用户反馈机制，提供用户反馈功能，让用户对系统的匹配度评估和提升建议进行确认。 岗位推荐 问题描述：用户上传个人简历，系统自动分析简历内容，生成推荐职位，用户可以给出推荐是否有效的反馈。如果不满意，可修订简历部分内容，重新进行推荐。 关键点：推荐职位 解决方案： 简历内容分析，利用自然语言处理（NLP）技术对用户上传的简历进行内容分析。 用户兴趣，基于用户上传的历史简历数据，建立用户的兴趣和偏好模型。考虑用户之前的工作经验、求职意向、职业发展方向等信息。 职位推荐算法设计，选择合适的推荐算法，如协同过滤、基于内容的推荐、深度学习模型等。结合用户的兴趣模型和简历特征向量，计算与不同职位的匹配度。 修订简历，如果用户不满意推荐结果，系统应提供修订简历的功能。用户可以编辑、添加或删除简历中的信息，系统重新分析并生成更新后的推荐职位。 用户历史记录查看，用户可以查看自己历史上传的简历和推荐结果，了解职业发展轨迹。 招聘推荐 问题描述：企业招聘人员输入或上传职位要求，系统自动分析匹配求职人员简历，筛选出符合期望的人员列表。 关键点：招聘推荐 解决方案： 职位要求解析，利用自然语言处理技术，对企业输入或上传的职位要求进行解析。提取关键技能、经验要求、学历等信息，构建职位要求的特征向量。 求职人员匹配度计算，基于已有的用户简历数据，计算每个求职人员与职位要求的匹配度。利用算法综合考虑关键技能匹配、工作经验匹配等因素。 候选人员筛选，根据匹配度计算结果，筛选出符合职位要求的候选人员列表。设置合适的匹配度阈值，确保选出的人员满足企业的期望。可以设计系统推荐排名，推荐排名靠前的人员供企业参考。考虑推荐结果的多样性，确保涵盖不同技能和经验背景的候选人。 候选人员详细信息展示，提供候选人员的详细信息，包括简历、技能、工作经历等。支持企业预览候选人员的综合素质，以便更好地做出招聘决策。 用户期望 对开发的产品方案期望如下： （1）算法优化合理，求职与招聘推荐结果与用户期望一致性高； （2）胜任度能力评价结果合理，给出的提升建议符合用户短板； （3）保护简历中个人数据的安全，不侵犯用户隐私； （4）扩展功能：可以对注册用户的数据和操作行为进行统计分析，给出热门职位、热门技能、热门专业等的一些趋势图等。 功能模块 简历上传模块 能力评价模块 岗位推荐模块 招聘推荐模块 热门推荐模块 个人信息模块 队伍分配 A01：周锦辉+罗骏岚+李鑫+杨康庆+禹乐 A15：李翔+李慧聪+周亚+朱豪尔+郑国盛 "},{"title":"【IDEA结合Git实现项目管理实战】四、git冲突篇","date":"2024-01-04T10:21:22.000Z","url":"/2024/01/04/%E5%9B%9B%E3%80%81%E8%A7%A3%E5%86%B3%E4%BB%A3%E7%A0%81%E5%86%B2%E7%AA%81/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 在使用Git进行项目管理时，代码合并是一项常见而重要的操作。本文将重点探讨两种常用的代码合并操作：合并（merge）和变基（rebase）。在进行代码合并时，我们难免会遇到Git冲突的情况。本文也将通过举例详细介绍如何通过IDEA使用Git进行合并或变基操作时可能遇到的代码冲突情况，并提供解决方法。 什么是git冲突 在多分支并行处理时，每个分支可能基于不同版本的主干分支创建。如果每个分支都独立开发而没有进行代码合并，自然不会出现代码冲突。但是，当两个分支同时修改同一文件时，在代码合并时就会出现冲突。 下图为两个分支分别使用合并/变基操作解决冲突后的提交树。 解决git冲突 介绍完冲突出现的原因，那么如何解决冲突呢？在解决git冲突时，我们需要确定以哪个分支的文件版本为准，或者取两个分支的文件的部分片段进行整合。 IDEA提供了强大的冲突解决功能，供用户处理git冲突。下面将进行详细介绍。 当前分支dev1的代码： 目标分支dev的代码： 我们现在的目标是让两个分支合并后的代码中同时出现method1、method2、access和acess2这四个方法。 执行合并后，出现界面： 左侧为当前分支dev1的提交记录，中间为合并前的预览结果，右侧为目标分支dev的提交记录。 其中红色区域为代码存在差异的部分。 先来看第一块红色区域的中间部分的代码。大家一定会疑惑预览结果中出现这段代码是什么意思？为什么会出现报错呢？ 这里其实是git对于左右侧存在差异的代码的标记。符号&lt;&lt;&lt;&lt;&lt;&lt;&lt; xxx的下方是左侧存在差异的代码，符号&gt;&gt;&gt;&gt;&gt;&gt; xxx的上方是右侧存在差异的代码，比如&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD箭头所指方向也就是我们当前分支的方向（在左侧），在该箭头下面的部分是当前分支的与目标分支的差异代码，这里因为左侧比右侧少了一段代码，因此下面啥东西没有；=======代表分割符号，该分割符号的下面就是目标分支的代码，即import java.util.concurrent.TimeUnit；&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev也就代表目标分支的方向（在右侧） 那么如何解决冲突呢？对于我们的目标来说，我们的输出语句自然不需要导入这个包，因此把语句import java.util.concurrent.TimeUnit;给删除即可。 点击左侧的箭头符号，可以把中间区域被替换成左侧的红色区域（那根细线，也就是没有代码）。 点击后中间区域消失。 再来看第二个红色区域，根据我们的目标，我们要将这四个方法都添加进入中间区域。 先点击左侧的箭头。可以发现中间区域被替换为左侧代码，右侧向左箭头变成了向左下箭头。 这个向左下的箭头代表将右侧的代码添加到中间代码的下方。 点击后如图： 那么一切就大功告成了，冲突解决成功，点击应用按钮。 git提示还有冲突未处理，这是为什么？ 把界面翻到上面，发现这个红色区域还没有处理，我们点击那个查号，作用是将冲突标记为已解决。 这时IDEA提示所有变更已被处理，那么我们就可以放心大胆的合并了。 合并成功！ 合并/变基详解 合并（git merge） 当前分支和目标分支执行合并操作时，Git会将当前分支的最新提交记录与目标分支的最新提交记录合并，并在当前分支形成一个新的提交记录。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行合并操作，dev中的提交记录添加到了分支dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行合并操作，git给出提示（已是最新 删除dev），当前分支dev1没有变动。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行合并操作，在目标分支dev1中生成一个新的提交记录Merge branch 'dev' into dev1，该提交记录包含了这两个提交记录的变更，如图。 在提交树中，可以看到两个提交记录合并为一个记录。 变基（git rebase） 当前分支和目标分支执行变基操作时，Git会将目标分支的最新提交记录依次应用到当前分支的每个新的提交记录中。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行变基操作，dev中的两条记录添加到了dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行变基操作，没有发生变化。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行变基操作，dev分支的提交记录添加到了dev1分支中。 总结 可以发现，无论是对于合并还是变基操作的示例1和示例2，最终执行操作后的结果都是一样的。对于合并操作，git将两个分支进行合并，最后生成一个新的提交记录，提交树存在交叉。对于变基操作，git将目标分支的提交记录应用到当前分支，提交树仍然是线性的。如图所示。 至于在实际开发中选择合并还是变基，还是看个人喜好了。 代码冲突示例 注意：本文为方便理解，所有示例均简单的修改项目中的md文件，实际开发中可能存在对多个文件的冲突，但万变不离其宗，只要你具备了解决单个文件代码冲突的能力，那么多个文件的冲突也能轻松应对。 合并/变基分支1 分支情况，当前dev1的两个提交记录博文1和博文2都在dev的提交记录博文3之前，其余分支一样 时间顺序：博文1-&gt;博文2-&gt;博文3 合并 此时合并有代码冲突，解决这个冲突。 发现该冲突只针对博文2，也就是最后一个提交记录 变基 该冲突为博文1和博文3的冲突 该冲突为变基后的博文1和博文2的冲突 合并/变基分支2 分支情况： dev中的博文3在dev1中的博文1和博文2之间 时间顺序：博文1-&gt;博文3-&gt;博文2 合并 基于上述情况，合并分支存在代码冲突 在代码冲突中，存在博文2和博文3的冲突， 冲突解决后如图所示。 这里紫色因为博文3是属于别的分支过来的，其父提交是add README.md.。所以从add README.md.出发，与dev1原本的提交记录博文2结合形成一个新的提交记录Merge branch 'dev' into dev1 结论：分支以时间顺序进行排序，合并分支永远是两个分支的最后一个提交历史进行合并。 变基 博文1和博文3存在冲突 冲突解决后，选择提交消息不变 依然存在冲突 可以发现该冲突来自于已经变基的提交博文1和之后的博文2 得到变基后的提交树 合并/变基分支3 分支情况： dev1中的两个提交记录博文1和博文2在dev中的博文3提交之后 时间顺序：博文3-&gt;博文1-&gt;博文2 合并 博文2和博文3存在代码冲突 变基 冲突来自于博文3和博文1 冲突来自变基后的博文1和博文2 总结 通过这三个代码冲突的示例，看到区别了吗？ 在合并操作时，冲突通常发生在两个分支的最新提交记录上。这是因为合并是将两个不同的分支合并为一个，而最新的提交记录是两个分支的端点。如果两个分支都对同一文件进行了修改，Git 无法确定应该选择哪个更改，因此会产生冲突。 在变基操作时，冲突可能发生在当前分支的提交记录和目标分支的提交记录之间的每个提交记录上。这是因为变基是将一系列提交应用到另一个分支上，而不仅仅是最新的提交。如果两个分支都修改了相同的文件，冲突可能会在每个提交记录上发生，而不仅仅是最新的提交。 总的来说，冲突是由于两个分支都对同一文件进行了修改，而 Git 无法自动解决冲突的情况下发生的。在合并操作中，冲突通常发生在最新的提交记录上；在变基操作中，冲突可能发生在多个提交记录上。"},{"title":"【IDEA结合Git实现项目管理实战】三、实战篇","date":"2023-12-15T04:01:20.000Z","url":"/2023/12/15/%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的实战篇，将正式介绍如何使用IDEA结合Git进行项目管理。 注意：本文假设你已经成功在IDEA中配置了git 在配置篇中，当我们已经在本地推送/克隆了一个项目后，我们能看到如下两个功能块。 在本文中，我们称红色箭头所指为git日志，蓝色箭头所指为git工具栏。 设置默认添加文件 还记得基础篇我们提到的四个工作区域吗？如果我们想要将变更的文件推送到远程仓库，我们首先需要先保证变更的文件在暂存区中。 在基础篇中，我们提到每当创建一个文件时，idea都会询问我们是否将文件添加到git，即是否添加到暂存区中。为了避免这种麻烦，我们打开设置，按以下流程选择无提示添加，这样idea就会默认将我们创建的文件自动添加到git了。 配置完该设置后，相当于每次创建文件都会默认执行git命令git add，作用是将文件添加到暂存区中。 推送更改的文件到本地库 当我们在本地完成了我们的工作后，我们可以先将变更的文件提交到本地库中保存。 这里我们点击蓝色箭头所指git工具栏的第二个按钮，即提交按钮。 该按钮对应git命令git commit，作用是将文件提交到本地库中。 这里我简单的修改了下我自己的更改，填写完提交信息后点击提交。 如果点击提交并推送的话，就可以直接将变更推送到远程仓库，省去了一个步骤。 提交并推送按钮对应git命令git commit &amp; git push，作用是将暂存区的变更直接推送到远程仓库 提交完毕后，我们打开git日志，能在当前的本地分支中看到我们刚才提交记录： 将本地库的提交记录推送到远程库中 当我们已经确定本地库的工作已经结束后，我们就可以将本地库的记录推送到远程库中。 我们点击git工具栏的第三个按钮，即推送按钮。 该按钮对应git命令git push，作用是将本地的提交记录推送到远程库中。 这里我准备了两个提交记录，按ctrl + 鼠标左键可以多选提交记录。 点击推送按钮后，我们可以在git日志中看到远程仓库中出现了我们刚才推送的提交记录。 此时我们打开gitee，可以发现提交记录同样生效于gitee中。 拉取远程分支的变更 当你的远程仓库发生改变时（你的队友推送了提交记录到远程仓库中），而你自己的远程分支不会自动拉取别人的提交记录，也就是说你的远程分支不具备别人的提交历史。 此时我们需要点击提取所有远程按钮，这时候就可以把远程仓库的所有本更拉取到自己的远程分支中。 提取所有远程按钮对应git命令git fetch，作用是将远程仓库的变更拉取到远程分支中。 这里我在gitee中手动添加了一个README.md文件。 点击按钮后，可以看到远程分支中增加了这条记录，同时，我们可以注意到本地分支名后出现了向下的蓝色箭头，这代表着当前远程分支已经更新了，本地分支也应该进行更新。 我们选中对应的本地分支，点击更新所选内容按钮，就可以把远程分支的提交记录添加到我们的本地分支中。 这里我们也可以选择git工具栏的第一个按钮，这样就可以指定方式传入变更。 该按钮对应git命令git merge或git rebase，作用是传入变更到当前分支 结果如下 新建分支 正常来说我们开发项目肯定不可能只用一个本地分支，一般会在其他分支中开发完毕后再推送到主分支中，这时候就需要新建分支的操作了。 分支可以简单的理解为我想基于这个提交以及它所有的父提交进行新的工作。当我们新建分支时，Git就会将HEAD指向的提交记录以及该提交记录之前的所有提交记录保存到新分支中。 如果我们要新建分支，可以点击左侧工具栏的加号按钮，这样就得到了一个新的分支。 该按钮对应git命令git branch，作用是基于当前提交历史创建新的分支。 新建后我们得到了一个全新的分支dev，他包含了master的全部提交历史。 当然，由于HEAD当前指向的是最后一个提交记录，所以新分支dev就包含了master的全部提交记录。如果我们要基于测试1这个提交历史创建分支的话，需要右键该提交历史，点击新建分支后再输入分支名并创建即可。 这样我们就得到了一个包含测试1之前所有提交记录的分支。 回到指定的提交历史 假设你当前开发的代码出现了问题，如何找回之前的代码？这时候就可以通过签出这一git提供功能回到之前的提交历史。当你签出到一个提交历史时，你就获取到了这个提交历史的所有代码。 注意：在签出前确保你当前的代码已经提交到了本地或者远程分支中，签出到其他提交记录时IDEA不会帮你自动保存当前的代码。 在签出的操作中，我们必须要明白HEAD这一概念，HEAD 是一个指向当前所在分支的指针，或者是指向当前所在提交记录的指针。在IDEA中，HEAD的位置可以通过黄色便签来看到，比如在下图master的提交历史中，HEAD指向的分支就是master，同时指向提交消息为add README.md.的提交记录。这里我们将下图称为图1。 如果我们要切换到测试1的提交历史上，右键测试1，点击签出修订。 签出修订按钮对应git命令git checkout，作用是将HEAD指针转移到当前分支/提交记录。 可以看到黄色便签转移到了测试1上，证明我们当前在测试1的提交历史上，这时我们就可以基于测试1的代码进行修改了。这里我们将下图称为图2。 但是这里有个问题，为什么图2在master分支上的黄色便签消失了？而在图1中黄色便签既指向了master又指向了最后一个提交记录add README.md. 这里要明白一个原理，那就是分支本身也能看作是一个指针，这个指针恒指向该分支的最后一个提交记录。 那么在图1中，HEAD其实是通过指向分支进而指向了该分支的最后一个提交记录，即HEAD-&gt;master-&gt;add README.md.。而在图2中，HEAD被称作游离的HEAD，是因为它指向的并不是分支而是该分支的一个提交历史，自然就不会指向master分支了，即HEAD-&gt;测试1。 基于指定提交历史修改代码 在上一个回到指定的提交历史的操作中，我们通过签出的操作获取到了指定的提交历史的代码，这时我们就能够基于这个提交历史的代码进行开发。 举个例子，假设我们要回到之前刚添加md文件的提交历史上进行代码开发，给md文件进行修改。我们当前分支的最新提交为博文3，如何回到提交历史add README.md.上？ 第一步我们肯定要签出到add README.md.上，签出后可以发现项目文件回到了刚开始提交md文件的时候。 这时我们对md文件进行修改，这里我删除了一些段落。 然后点击提交按钮，将这个修改历史666提交到本地分支中。 此时Git给出了警告。如果我们无视警告，仍然点击提交按钮，就会发现原本存在于add README.md.提交记录的指针消失了，而且我们修改后的提交记录666也并没有被提交到本地分支中。 还记得之前我们提到的游离的HEAD吗？如果直接在特定提交上修改代码并运行 git commit，这实际上会在游离的 HEAD 状态下创建一个新的提交，而不会创建分支。这样的操作可能会导致出现一个游离的提交，Git 的垃圾回收机制可能会删除这些提交。 解决方法1 一种方法是：我们在指定提交历史上创建一个分支，然后在新分支上开发完毕后进行提交。这两步操作前文已经详细介绍了，不再赘述，如图所示。 解决方法2 第二种方法则是：对指定提交记录执行git revert命令，该命令会基于指定提交记录创建一个新的提交历史 我们可以右键一个提交记录，选择还原提交选项。 如果当前分支的提交记录和还原的提交记录的文件存在差异的话会出现代码冲突。这里因为我当前分支的第一个提交记录博文3和add README.md.在md文件上存在差异，所以出现了冲突。 由于我们当前的目标是回到add README.md.这个提交记录的代码中，所以我们选择忽略来自其他提交的变更，保留add README.md.提交记录的原本代码即可。 写好提交消息后点击提交 可以发现我们这样就创建了一个和add README.md.一模一样的提交记录了。 接着我们可以基于这个提交记录的代码进行开发，这里前文已经详细介绍了，不再赘述。 将远程分支拉取到本地 设想这样一个场景：你的同事创建了一个新的远程分支并做了一个新的功能，而这个分支是你本地没有的，你现在的工作要基于这个新的功能才能进行下去，那么我们就需要把这个远程分支拉取到本地来。 在这个例子中，你的同事创建了远程分支origin/dev，而我们本地并没有与其对应的分支。 我们只需要右键该分支，点击签出，即可将该远程分支拉取到本地。 代码整合 在项目合作中，将其他人的代码整合到自己的代码中是经常用到的操作，这时就需要利用到Git的合并或变基功能。 一般来说代码整合会遇到以下几种情况： 当前分支正好比其他分支少了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，当前dev1中没有dev中的add README.md.的修改记录。首先确保我们当前分支在dev1分支上。 我们右键dev分支，选择变基或合并均可。 可以看到dev的提交记录整合到了dev1中。 当前分支和其他分支都修改了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，dev1中我写的md文件的内容较少。 在dev中我的队友写的md文件的内容较多。 现在一样要确保当前分支在dev1中，并且右键dev分支选择合并/变基来整合代码。这个时候就出现了冲突，大家应该很容易就想明白了，因为我当前分支和我队友的分支都同时存在文件名相同、内容不同的文件，这个问题不解决的话自然无法整合。 解决冲突的话就需要你和队友进行协商。比如这里我的分支dev1就比较少，队友的分支dev写的比我详细，所以我可以进行\"妥协\"，直接用队友的md文件即可，点击接受他们的这个按钮就可以使用队友的md文件了。 当然，我们也可以不进行妥协，IDEA提供了强大的修订功能，通过点击合并按钮，我们可以选择整合该文件的特定部分。具体如何操作，大家自行练习吧~ 删除提交 我们在实际开发中难免会提交一些无用的提交记录，这时需要利用删除提交的操作。 右键一个提交记录，选择删除提交选项，即可删除。 很简单吧？但要注意已经推送到远程分支的提交是不可删除的。 这里的绿色提交记录代表还未推送到远程分支的提交记录，棕色代表已经推送到远程分支的提交记录。 "},{"title":"【IDEA结合Git实现项目管理实战】二、基础篇","date":"2023-12-10T08:20:10.000Z","url":"/2023/12/10/%E4%BA%8C%E3%80%81Git%E7%9A%84%E5%9B%9B%E4%B8%AA%E5%8C%BA%E5%9F%9F/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的基础篇，将简要介绍使用Git所应该知道的最基本的知识，因此不算深究原理，哈哈。 注意：本文假设你已经成功在IDEA中配置了git Git的四个工作区域 基本概念 Git本地有三个工作区域，分别是工作区、暂存区、本地仓库 Git远程有一个工作区域，叫做远程仓库 工作区：开发者当前工作的目录，主要包含项目的实际文件，其中可能包括未进行版本管理的新文件和已修改的文件。 暂存区：作为一个临时存储区域，用于存放工作区中已经修改的文件的快照，以便将它们作为一个逻辑单元提交到本地仓库。 本地库：包含了项目的提交历史，开发者可以对其进行各种操作，如分支合并、变基、签出等。本地库是在开发者本地计算机上的存储库。 远程库： 存放在代码托管平台（如Gitee、GitHub）上的仓库，包含了已推送的代码版本、分支、标签等信息。开发者可以通过推送和拉取操作与远程库进行交互，以保持代码同步。 概念解释 工作区 一下子引入了这么多概念，可能大家会有点接受不了，那么这里对以上概念进行详细解释。 具体来说，工作区可以看做你在IDEA中打开的项目目录，如下图： 这里的项目目录就可以看作是工作区，但是在这个工作区中，既存在正常颜色（白色）的文件名，也存在红色、绿色和蓝色的文件名，那么这些文件分别代表什么含义？这里的文件的颜色，其实也就对应着上文所谓未进行版本管理的新文件和已修改的文件。如下表： 颜色 含义 白色 在当前提交历史未改动或者已经提交到本地库的文件 红色 未进行版本管理的新文件，也被称为未跟踪的文件 绿色 已经进行版本管理的新文件，该文件已被添加到暂存区中 蓝色 当前工作区中被修改的文件，该文件已被添加到暂存区中，与绿色相似 红色文件名、绿色文件名实例 大家可以自行测试下，如果你直接在项目目录中创建一个文件的话，IDEA会做出提醒： 这时如果你选择添加的话，该文件就会变成绿色文件从而被Git进行版本管理，如果选择取消的话，说明你不希望该文件被Git管理，那么该文件就是红色文件而不能被提交到暂存区。 总结： 当你创建一个新文件时，它就是红色的，表示这是一个未跟踪的文件。通过运行 git add 命令，将文件添加到暂存区，此时文件变为绿色，表示它已经被 Git 管理并准备提交。 如果你不想将一个未跟踪的文件或已修改的文件纳入版本管理，你可以选择不使用 git add 命令，或者使用 git reset 命令来取消已经添加到暂存区的文件。取消后，文件会回到红色状态，表示它未被跟踪或未被修改。 但是对于我们日常开发来说，我们创建一个文件肯定是有用意的，一般都希望该文件被提交上去，所以我们一般都选择添加文件，这样才能通过添加到暂存区，再到本地库最终推送到远程仓库中。 白色、蓝色文件名实例 比如这里我删除了md文件中的其中一行，md文件的文件名由白色转为了蓝色 该文件一般在该项目已经提交到本地库中后，我们准备开发新的功能时才会出现，也就是我们修改了当前已经存在于本地库中的文件，该文件就会被转化成蓝色文件。因为该文件已经被Git版本管理过了，所以可以直接提交到本地库。 总结： 蓝色文件可以直接提交到本地库中，因为其已经被Git版本管理了 蓝色文件可以简单认为就是白色文件被修改后的文件 暂存区 经过前面的内容，大家多少应该可以感觉到，暂存区其实是我们看不见、摸不着的存在，在暂存区中存在的文件一般是绿色和蓝色文件，也就是我们已经使用git add命令添加的文件。 当我们开发完毕一个新的功能后，我们就会准备将这次修改记录提交到本地仓库，这时候使用git commit命令就可以将暂存区中的文件提交到本地仓库中。 提交实例 在该实例中，我们删除了md文件的其中一行（可以看到changes一栏中是蓝色文件，如果你把一个新的文件也添加到暂存区的话就是前文提到的绿色文件），这里我们要在下方框中填写相关介绍，以便帮助其他团队成员或日后的自己知道这次的提交做了什么。 本地库 具体来说，本地库包含了开发者本地完整的提交历史和所有的本地分支，本地库中的提交记录是暂存区通过执行git commit后得来的。 这里要注意本地分支和本地库的区别，本地分支只是本地库众多分支的其中一个分支，当然本地库也可以只包含一个本地分支（但是一般来说开发者不会这么做）。 本地分支实例 这里展示了本地库中的master分支，在这个分支中存在着该分支的提交历史和各种信息。 远程库 远程库是存放在代码托管平台（如Gitee、GitHub）上的仓库，本地库执行git push命令可将本地的提交记录推送到远程仓库中。 远程分支和远程库的区别同本地分支与本地库的区别一样 远程分支实例 同上本地分支实例。 需要注意的是，由于我们在团队开发中涉及到多人的共同协作，因此每个人都可以向远程仓库中推送自己的代码，也就意味在当你在开发新的代码时，远程仓库可能已经发生了变动，那么远程分支也就不能实时和远程仓库的提交记录保持同步了，这时我们需要使用git fetch命令，将远程仓库的提交记录拉取到的远程分支中。 总结 通过对于工作区域的认识，大家想必已经了解了一个文件如何从工作区一步步提交到远程仓库中，当然这里存在着很多操作中的细节，如切换分支，合并来自其他分支的结果等，而我们是使用IDEA结合Git实现版本管理，同样存在着如何具体使用的问题。由于本篇只讲理论，就不过多介绍了，本系列将会继续更新下去，大家敬请期待~"},{"title":"【IDEA结合Git实现项目管理实战】一、配置篇","date":"2023-12-07T02:30:29.000Z","url":"/2023/12/07/%E4%B8%80%E3%80%81IDEA%E9%85%8D%E7%BD%AE/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的配置篇，将介绍如何使用IDEA整合git，从而实现项目管理。 本文将使用Gitee作为项目管理工具。 注意：本文假定你已经拥有了一个Gitee账号并已经配置了密钥。 下载插件 打开设置 在插件中搜索Gitee并下载安装，安装完毕后IDEA会提醒重启IDE，重启后插件才会生效！ 添加账号 IDEA重启后，再次打开设置，在版本控制中可以看到Gitee这一栏，点击加号添加账号 点击加号后我们选择Log in via Gitee 授权完毕后点击应用。 项目管理 最后一个步骤就是正式的实现项目的版本控制了，实现这一步骤有两种操作。 第一种是将IDEA本地的项目上传到gitee中 第二种是从远程clone一个仓库到IDEA本地中 下面我们来逐个介绍这两种操作，并简述这两者之间的区别和使用场景。 1.将IDEA本地的项目上传到Gitee中 现在我们打开你想要托管给Gitee的项目，打开工具栏的VCS，点击Share Project on Gitee 这里我们可以设置仓库名（Repository name），Remote是远程分支名，可以不用修改，Description是这个仓库的描述，这里自己填写即可。 填写完毕后点击share，弹出这个窗口。 我们需要在本窗口中添加需要进行版本管理的文件以进行初始化，可以看到，本项目的所有文件都是理论篇提到的红色文件名的文件，这是因为这个项目还没有上传到远程仓库，也就不存在被Git跟踪的文件，所以都是红色文件名。 这里我们填写下提交信息并点击添加按钮，就可以上传成功了。 上传完毕后，我们可以观察到IDEA中出现了这三个功能块，至于这些功能块有何具体作用，我们将在基础篇详细介绍，这里不过多解释了。 这时候我们可以打开gitee的网站，点击顶部工具栏的头像，选择我的仓库，就可以看到刚刚创建的仓库了! 2.从远程clone一个仓库到IDEA本地中 这一步我们将从远程仓库中clone一个项目到本地中。 我们在gitee中选择一个想要clone的远程仓库，这里我使用的是我自己的远程仓库：  点击VCS，选择从版本控制中获取。 在仓库URL中，在URL中粘贴我们刚才复制的HTTPS地址，在目录中选择我们想要放置远程仓库代码的本地地址，填写完毕后点击克隆。 注意：目录必须是一个空目录 等待克隆完成后项目会自动跳转到你选择的目录。 总结 通过以上的介绍，我们了解到实现版本控制有两种操作： 第一种操作适用于以下场景： 本地你已经开发好了项目，需要将项目托管给远程仓库。 第二种操作适用于以下场景： 团队已经有了远程仓库（有人已经将远程仓库创建好了），这时候我们直接clone即可。 在gitee或github中看到了优质项目，我们clone本地进行学习 "},{"title":"Redis持久化问题排查","date":"2022-10-29T12:49:50.000Z","url":"/2022/10/29/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Redis持久化问题排查 前言：在学习黑马的Redis持久化课程时因为老师用的MobaXterm软件与我使用的VMware存在差异，同时，老师上传的视频的redis配置与我大为不同而导致了一系列的问题，在进行了一天的排查（死杠）后终于解决了问题，感慨良多。在遇到问题解决不了的时候还是要多回头看，没准在出发点就能找到问题了。 下面就将展示问题的描述和解决方案 问题描述 老师在使用MobaXterm软件时使用redis-server redis.conf命令指定配置文件在前台启动Redis 然而我在使用redis-server命令却出现了下面的两种情况： 第一种：不指定配置文件启动Redis 发现监听6379端口失败 第二种：指定配置文件启动Redis 这里要先cd到自己的Redis安装路径下，然后使用命令redis-server redis.conf 发现没有产生日志，这里使用ps -ef | grep redis查看后端端口占用情况 显然，后台端口已经被占用了，这里是因为Redis配置了开机自启动和后台启动，导致端口占用而无法使用redis-server命令 问题解决 在执行以下操作前建议使用VMware创建一个快照，防止后续操作不当导致虚拟机设置出现问题 先使用以下命令关闭后台端口 此时cd到Redis的安装目录下，使用命令ps -ef | grep redis查看后台端口占用情况，发现后台端口关闭 然后在当前路径下使用vi redis.conf关闭Redis后台启动 将 daemonize 修改为 no 这样Redis就不会在后台启动了！ 接着可以开心的使用redis-server redis.conf指定配置文件启动redis了！ 然而，如果你使用该命令后又出现了如下情况 发现当前窗口确实是像前台启动一样阻塞的，但是却没有任何日志信息输出，这时我们打开RESP等图形化界面看一下连接是否成功 嗯，确实成功了，说明Redis服务确实启动了，但还是没有日志文件，那么这是怎么回事呢？ 哈哈，不用着急，我们先使用ctrl + c停止当前端口，然后再回到RESP上发现连接终止，这就说明我们之前做的操作没有问题！现在我们回到终端，使用vi redis.conf命令进入到配置文件中 找到logfile，发现logfile后面的引号内为一个日志文件 这个时候一切都真相大白了，因为在一开始配置Redis时，我们将在logfile的引号内写入了一个日志文件名称（一开始的logfile内的引号默认是空的），这就导致了我们使用redis-server命令输出的日志都进入到这个日志文件中，所以启动时当然看不到任何信息了！现在我们将引号内的文件命删除，保存并退出 再次使用redis-server redis.conf命令启动Redis 启动成功，大功告成！"},{"title":"VMWare演示Redis持久化","date":"2022-10-28T12:49:50.000Z","url":"/2022/10/28/VMWare%E6%BC%94%E7%A4%BARedis%E6%8C%81%E4%B9%85%E5%8C%96/","tags":[["学习笔记","/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"]],"categories":[["Redis","/categories/Redis/"]],"content":"VMware演示Redis持久化 Redis持久化的作用 Redis是内存存储，如果出现服务器宕机、服务重启等情况可能会丢失数据，利用Redis持久化可以将数据写入磁盘中，这样Redis就可以利用持久化的文件进行数据恢复，数据安全性得以大大提升。 RDB RDB全称为Redis Database Backup file(Redis数据备份文件)，也被叫做Redis数据快照。简单来说就是把内存重的所有数据都记录道磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。 RDB实现数据持久化有两种命令：save和bgsave save命令适合用于准备将Redis停机的情况。 在执行save命令时会占用Redis的主进程，阻塞所有命令，因此不适用于Redis正在运行的情况。 而bgsave命令会执行fork操作开启一个子进程，避免主进程收到影响。 此外，当Redis停机时会执行一次RDB操作进行数据备份。 下面将对该情况进行验证 RDB演示 使用redis-server redis.conf指令在任意路径下运行Redis 接下来创建一个新的终端创建连接，存入键值对 接着回到上一个终端使用ctrl + c关闭Redis，可以看到DB saved on disk表示文件正常保存 在当前路径下使用ll查看所有文件，看到dump.rdb快照文件已经生成 接着再次在当前终端使用redis-server redis.conf启动Redis，此时数据会自动恢复，回到另一个终端窗口 此时将使用ctrl + c关闭当前命令行客户端，重新使用redis-cli开启一个新的命令行 这时使用get num命令发现数据返回为789，（如果get num返回错误Error：Server closed the connection要将redis.conf的protected-mode设置为No关闭保护模式）表明数据备份成功 Redis停机自动执行RDB证明完毕！ RDB相关配置 这里需要 cd 到redis的安装目录下，我的安装目录在 /usr/local/src/redis-6.2.6下 这里老师用的MobaXterm可以直接使用第三方文件打开配置文件进行修改，因为我用的是VMware所以只能使用Linux命令进行修改配置 使用 vi redis.conf修改配置文件 这里有一些小技巧： 按下INSERT键进入修改状态，此时才可以对文件进行修改 按下ESC键可以使用一些命令：/xxx可以查找跳跃到当前文件中的xxx名称，:wq保存当前文件并退出，:q不保存并退出当前文件，:q!不保存并强制退出当前文件 下面就可以对配置文件进行修改了！ 修改为五秒内执行一次操作就出发RDB备份 接着 再查找rdbcompression后修改为yes即可 使用:wq保存并退出 rdb文件名修改后使用终端再次运行Redis，发现此时没有DB文件录入消息，之前的dump.rdb文件已不能被读取 到另一个终端重新使用redis-cli开启命令行，使用get num命令发现结果为空，再set 一个新的键值，回到Redis窗口发现出现Background saving started字段即表示成功！ AOF AOF全称为Append Only File (追加文件)。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 开启AOF需要在Redis的配置文件中修改配置，AOF记录命令的频率有三种:always 、everysec和no always配置胜在能记录每一次Redis执行的写命令，几乎不会丢失数据，但对性能会有很大的影响 everysec配置每隔一秒将缓冲区里的数据写入到AOF文件，避免直接对AOF文件操作而性能上有了提升，但是在数据间隔的一秒内如果出现服务宕机的情况会丢失最多一秒的数据 no配置执行频率最低，性能最好，但是可靠性很低，不推荐使用 AOF演示 先使用 cd 命令进入到Redis的安装路径下 使用vi redis.conf命令修改redis配置文件 将之前的save 5 1注释掉，写入save \"\"，表示禁用RDB 将appendonly修改为yes，开启AOF AOF执行频率的命令配置默认为appendfsync everysec，不需要修改 修改完毕后回到终端，使用rm -rf *.rdb删除之前的RDB文件 可以看到当前目录下rdb文件已删除 使用redis-server redis.conf命令重启Redis 发现日志中出现没有对RDB的读取，修改成功 在另一终端打开命令行，输入如下指令 返回空集合，说明所有数据已被清除 img set一个键值对 这时我们打开文件夹进入本机的Redis的安装路径下，发现AOF文件已经被创建成功，打开AOF文件，set命令已经被写入成功，说明AOF已经生效了 接下来验证AOF的重启恢复，先重启Redis服务，红框内的日志表示数据已经从AOF文件加载完毕 这时候我们回到命令行使用keys *命令，返回num，依然有数据，说明AOF重启能保证数据的恢复，证明完毕 RDB和AOF的比较 "},{"title":"VMware软件安装及问题解决","date":"2022-10-08T03:26:00.000Z","url":"/2022/10/08/VMware%E5%AE%89%E8%A3%85/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"VMware软件安装 前言：最近学习Redis时需要使用Linux系统，导致不得不去安装一个虚拟机了，在准备安装VMware软件时遇到了一些问题，这里给出记录和解决方案，以做参考学习使用 准备工作 本文将在Windows系统下使用VMware软件配置Linux虚拟机，下文将会详解VMware软件的安装步骤 VMware软件下载地址： Linux CentOS 7版本下载映像文件地址： 按图片所示下载即可 这里我选择的是CentOS-7.0-x86_64-DVD-2009.iso 版本 在下载路径下右键以管理员身份运行 在这里要敲重点！！！如果你没有以管理员身份运行这个选项，那么就看接下来的问题解决部分，如果你有这个选项请点击跳转自行无视。 问题解决 按住快捷键Win+R打开运行窗口，输入“regedit”， 这样就打开了注册表编辑器 在编辑器左侧依次找到HKEY_CURRENT_USER 然后将RestrictToPermittedSnapins的值设置为0 上面的方法如果输入路径后发现MMC不存在，那么下面这个方法就派上用场了！ 按下Win + R键打开运行，输入gpedit.msc打开组策略编辑器，这里可能你又会惊讶的发现，gpedit.msc打不开了！别着急，下面还有解决方案。当然，如果你能打开组策略编辑器的话可以点击这里继续往下看 对于无法打开组策略编辑器的情况，win + r 键打开运行后，输入notepad打开记事本，复制粘贴以下内容 记事本左上角文件另存为到任意路径下 文件名写为gpedit.cmd，保存类型选择所有文件，编码选择ANSI 点击保存后在你保存的路径下右键以管理员身份运行 等待运行结束后发现win + R键打开的gpedit.msc可以正常打开了 接下来进入组策略编辑器，双击计算机配置 --&gt; Windows设置 --&gt; 安全设置 双击本地策略 进入安全选项找到下面两项选择已启用 大功告成！！！ 安装VMware 当解决上述问题后开始准备下载VM软件了，右键以管理员身份运行后等待片刻来到安装界面 一路下一步来到这个界面，安装路径自己选择，我选择安装在D盘下的目录里 勾选取消“启动时检查产品更新” 一路下一步，点击安装 点击许可证 这里要输入许可证密钥（这里大家自行网络上获取），然后点击输入，安装结束，注意不要误点到跳过了。 输入密钥确认安装后就可以使用VMware了！"}]