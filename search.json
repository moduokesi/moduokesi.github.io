[{"title":"【并发编程】线程池参数及创建方法","date":"2024-05-27T15:43:21.000Z","url":"/2024/05/27/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E5%8F%8A%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95/","tags":[["多线程","/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"目录： 1.什么是线程池 2.线程池七大参数 3.如何设定线程池参数 4.如何创建线程池 1.什么是线程池 线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是等待下一个任务。 为什么要使用线程池？ 使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 2.线程池七大参数 2.1.核心线程数 corePoolSize：线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了allowCoreThreadTimeOut。当提交一个新任务时，如果线程池中的线程数小于corePoolSize，那么就会创建一个新线程来执行任务。 2.2.最大线程数 maximumPoolSize：线程池中允许的最大线程数。在当前线程数达到corePoolSize后，如果继续有任务被提交到线程池，会将任务存放到工作队列中。如果队列也已满，则会去创建一个新线程处理任务。 2.3.空闲线程存活时间 keepAliveTime：非核心线程的空闲存活时间。当线程数大于corePoolSize时，空闲时间超过keepAliveTime的线程将被终止。 这个参数在设置了allowCoreThreadTimeOut=true时对核心线程同样有效。 2.4.空闲线程存活时间单位 unit：keepAliveTime的计量单位。例如，TimeUnit.SECONDS、TimeUnit.MILLISECONDS等。 2.5.工作队列 workQueue：新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列： ①ArrayBlockingQueue 基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。 ②LinkedBlockingQuene 基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而基本不会去创建新线程直到maxPoolSize（很难达到Interger.MAX这个数），因此使用该工作队列时，参数maxPoolSize其实是不起作用的。 ③SynchronousQuene 一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。 ④PriorityBlockingQueue 具有优先级的无界阻塞队列，优先级通过参数Comparator实现。 2.6.线程工厂 threadFactory：创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等 2.7.拒绝策略 handler：当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，线程池会执行拒绝策略。JDK中提供了4中拒绝策略： ①CallerRunsPolicy 该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。 ②AbortPolicy 该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。 ③DiscardPolicy 该策略下，直接丢弃任务，什么都不做。 ④DiscardOldestPolicy 该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列 3.如何设定线程池大小 一般来说，有两种类型的线程：CPU 密集型和 IO 密集型。 CPU 密集型的线程主要进行计算和逻辑处理，需要占用大量的 CPU 资源。 IO 密集型的线程主要进行输入输出操作，如读写文件、网络通信等，需要等待 IO 设备的响应，而不占用太多的 CPU 资源。 常见且简单的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 4.如何创建线程池 方式一：通过ThreadPoolExecutor构造函数来创建（推荐）。 通过这种方式可以根据服务器硬件配置，灵活设定线程池参数。 方式二：通过 Executor 框架的工具类 Executors 来创建（不推荐）。 FixedThreadPool：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。初始大小为 0。当有新任务提交时，如果当前线程池中没有线程可用，它会创建一个新的线程来处理该任务。如果在一段时间内（默认为 60 秒）没有新任务提交，核心线程会超时并被销毁，从而缩小线程池的大小。 ScheduledThreadPool：该方法返回一个用来在给定的延迟后运行任务或者定期执行任务的线程池。 Executors 返回线程池对象的弊端 FixedThreadPool 和 SingleThreadExecutor：使用的是无界的 LinkedBlockingQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool：使用的是同步队列 SynchronousQueue, 允许创建的线程数量为 Integer.MAX_VALUE ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool 和 SingleThreadScheduledExecutor : 使用的无界的延迟阻塞队列DelayedWorkQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 "},{"title":"【SpringBoot】MapStruct实现优雅的数据复制","date":"2024-05-05T14:10:21.000Z","url":"/2024/05/05/%E3%80%90SpringBoot%E3%80%91MapStruct%E5%AE%9E%E7%8E%B0%E4%BC%98%E9%9B%85%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/","tags":[["数据复制","/tags/%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/"]],"categories":[["Spring","/categories/Spring/"]],"content":"目录： 1.为什么选择MapStruct 2.MapStruct快速入门 3.MapStruct进阶 4.字段的逻辑处理 做项目时你是否遇到过以下情况： DTO（数据传输对象）与Entity之间的转换：在Java的Web应用中，通常不会直接将数据库中的Entity实体对象返回给前端。而是会创建一个DTO对象，这个DTO对象只包含需要返回给前端的字段。此时，就需要将Entity转换为DTO。 复杂对象的映射：当需要映射的对象包含大量的字段，或者字段之间存在复杂的依赖关系时，手动编写映射代码不仅繁琐，而且容易出错。 1.为什么选择MapStruct 1.1.常见的属性映射方法 一般来说，不使用MapStruct框架进行属性映射，常有的方法以下两种： Getter/Setter方法手动映射 这种方法最朴素，手动编写代码将源对象的属性存入目标对象，需要注意实体类中嵌套属性的判空操作以防止空指针异常。 BeanUtils.copyProperties()方法进行映射 BeanUtils底层使用的是反射机制实现属性的映射。反射是一种在运行时动态获取类信息、调用方法或访问字段的机制，无法利用JVM的优化机制，因此通常比直接方法调用慢得多。 此外，BeanUtils 只能同属性映射，或者在属性相同的情况下，允许被映射的对象属性少；但当遇到被映射的属性数据类型被修改或者被映射的字段名被修改，则会导致映射失败。 1.2.MapStruct的优势 MapStruct是一个基于注解的Java代码生成器，它通过分析带有@Mapper注解的接口，在编译时自动生成实现该接口的映射器类。这个映射器类包含了用于执行对象之间映射的具体代码。 与常规方法相比，MapStruct具备的优势有： 简化代码。对于对象内属性较多的情况，使用MapStruct框架无须手动对每个属性进行get/set和属性判空操作。MapStruct可以通过注解和映射接口来定义映射规则，自动生成映射代码，从而大大简化了这种复杂对象的映射过程。 性能优越。相较于反射这种映射方法，MapStruct在编译期生成映射的静态代码，可以充分利用JVM的优化机制，对于企业级的项目应用来说，这种方式能大大提高数据复制的性能。 类型安全。由于MapStruct在编译期生成映射代码，这意味着如果源对象和目标对象的映射存在错误，那么可以在编译时就发现错误。相比之下，BeanUtils在运行时使用反射来执行属性复制，这可能会导致类型不匹配的问题在运行时才发现。 灵活映射。MapStruct可以轻松处理嵌套对象和集合的映射。对于嵌套对象，MapStruct可以递归地应用映射规则；对于集合，MapStruct可以自动迭代集合中的每个元素并应用相应的映射规则。 有开发者对比过两者的性能差距，如下表。这充分体现了MapStruct性能的强大。 对象转换次数 属性个数 BeanUtils耗时 MapStruct耗时 5千万次 6 14秒 1秒 5千万次 15 36秒 1秒 5千万次 25 55秒 1秒 2.MapStruct快速入门 在快速入门中，我们的任务是将dto的数据复制到实体类中。 2.1.导入Maven依赖 2.2.创建相关对象 注意，实体类要具有get/set方法，这里我使用了lombok的@Data注解来实现。 dto类我使用了@Builder注解，可以快速为对象赋初始值。 2.3.创建转换器Converter 使用抽象类来定义转换器，只需中@Mapping注解中填写target和source的字段名，即可实现属性复制。 2.4.测试 在SpringBoot的测试类中测试，这里我使用DTO类的@Builder注解提供的方法为dto赋初值模拟实际开发，通过调用converter的方法实现属性映射。 结果如图： 最后，我们可以发现在target包的converter的相同目录下，生成了TestConverter的实现类 里面为我们编写好了映射的代码。 3.MapStruct进阶操作 如果仅是这种简单层级的对象映射，还不足以体现MapStruct的灵活性。下面将介绍MapStruct的进阶技巧。 3.1.嵌套映射 假设我们的Hotel实体类中嵌套了另外一个实体类Master dto对象为： 我们需要把personName和personAge映射到Hotel实体类的Master中，怎么做？ 很简单，只需要在target属性中加上Hotel实体类嵌套实体类的字段名，加字符.，再跟上嵌套类的字段名即可 结果如图： 3.2.集合映射 如果源对象和目标对象的集合的元素类型都是基本数据类型，直接在target和source中填写字段名即可。 若源对象和目标对象的集合元素类型不同，怎么做？ 这个案例我们需要把DTO的personList映射到masterList中。 编写converter，这次需要进行两层映射。 第一层将person集合映射到master集合上。 第二层将person对象的属性映射到master对象中。 结果如图： 查看target包下的代码，可以发现MapStruct除了两层映射外，还帮你自动生成了迭代集合添加元素的代码，从而实现集合元素的复制。 4.字段的逻辑处理 4.1.复杂逻辑处理（qualifiedByName和@Named） 这次我们需要把dto中的personName和personAge的list集合映射到实体类的masters集合中。常规的集合映射无法处理这种情况，这时需要使用到qualifiedByName和@Named进行特殊处理。 这就需要拿到两个list的数据，进行手动处理了。在@Mapping注解的qualifiedByName属性指定方法名定位处理逻辑的方法，@Named(\"dtoToMasters\")。 利用stream流进行处理。 返回结果： 4.2.额外逻辑处理（ignore和@AfterMapping） @Mappings的ignore属性，也可以对一个字段（不能是集合）进行额外逻辑处理。通常搭配@AfterMapping注解使用。 这个案例中，我们需要根据DTO的mount属性判断是否大于15，如果大于，则判断hotel实体类的isSuccess为true 编写converter，注意@AfterMapping注解下的方法的参数列表，需要使用@MappingTarget注解指明目标对象， 测试方法 返回结果 4.3.简单逻辑处理（expression） expression可以在注解中编写简单的处理逻辑 在这个案例中我需要在实体类的nowTime字段获取当前时间。 直接在expression属性中使用方法获取当前时间。 结果如下 "},{"title":"【项目开发】Java调用Python方法一文详解","date":"2024-04-26T08:23:20.000Z","url":"/2024/04/26/%E3%80%90%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E3%80%91Java%E8%B0%83%E7%94%A8Python%E6%96%B9%E6%B3%95/","tags":[["项目开发","/tags/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"前言 在这个人工智能技术迅速发展的时代，对于我们学生而言，参加软件竞赛已不再是单纯的技术比拼。传统的纯Java编写项目，虽然有其稳定与高效的优势，但在面对日益复杂的算法需求时，其竞争力已逐渐减弱。因此，将Java与Python这两种编程语言的优势相结合，实现算法与软件的完美融合，已成为提升项目竞争力的关键。 本文将详细讲解使用Java调用Python的三大方法，并分析各个方法的优势。 1.jython库（不推荐） 首先在pom.xml中导入jython对应依赖 1.1.手动编写Python语句 这里我们编写一个简单的a + b函数的实现样例。 这样就可以得到返回结果 1.2.读取Python文件进行调用 编写一个jythonTest.py文件 使用PythonInterpreter.execfile方法调用py文件 总结： 上述两个方法的优点是其集成性，Jython允许你在Java中直接执行Python代码，使得Java和Python代码可以直接进行交互。 缺点也是明显的，首先，Jython说到底是Java的库，可能无法完全支持Python的所有库和功能，其次，Python工程师的工作高度耦合在Java代码中，如果你和你的Python同事不能够忍受这种开发方式，那么就不要用这种方法。 2.Java调用命令行（推荐） 这种方法的原理是通过Java代码调用操作系统的命令行接口，然后在命令行中执行Python脚本。 Java程序可以通过Runtime.getRuntime().exec()方法或者更高级的ProcessBuilder类来实现这一功能。执行Python脚本后，Java程序可以通过InputStream流来捕获并处理Python脚本的输出结果。 2.1.Runtime.getRuntime().exec()方法调用 首先需要编写执行Python脚本的命令行语句 使用Runtime.getRuntime().exec()方法执行命令 使用process.getInputStream()捕获InputStream流，从而获取执行结果 使用process.getErrorStream()捕获标准错误流（可选），如果Python语句报错会打印报错信息 等待进程结束 ，使用process.waitFor()方法获取返回码 完整代码为： 2.2.ProcessBuilder类调用（最推荐） 编写命令 使用ProcessBuilder启动进程 完整代码： 总结： 两种方法都是通过创建一个Process进程调用命令行获取Python脚本的执行结果。主要区别为第二种方法需要额外创建一个ProcessBuilder类，ProcessBuilder类接收的是String数组，相较于Runtime只接收一个String字符串，ProcessBuilder类编写命令更加灵活。 3.调用云端模型（最推荐） 这种方法需要Python工程师在百度云、腾讯云等云平台上开启接口，然后通过Hutool的工具类发送HTTP请求调用云端接口。 首先需要导入Hutool库 填写接口地址，使用Map作为表单数据（一般使用表单，比较灵活，文件和文本都可以传输） 使用HttpRequest发送请求 判断返回结果合法性，进行相关处理 获取返回结果 完整代码： 4.总结 jython库 优点： 直接在Java程序中执行，可以直接利用Java虚拟机（JVM）的性能优势，减少进程间通信的开销。 缺点： Jython可能无法完全支持Python的所有库和功能。 Java调用命令行 优点： Java和Python进程是独立的，这使得它们可以更容易地并行运行，而不会影响彼此的性能。 对于对硬件要求比较高的Python模型， 本地部署可能存在一定的困难。 对于开发人员比较友好，两者的开发工作分离。 缺点： 进程间通讯可能会引入额外的开销和复杂性。 调用云端模型 优点： 对于开发人员比较友好，两者的开发工作分离。 利于构建大型算法模型，如百度云等云端平台对于Python模型支持度很高，比起本地更容易部署。 缺点： 需要Java和Python工程师对HTTP请求有一定了解。 云端接口需要支付一定的费用。 "},{"title":"常见的设计模式","date":"2024-02-08T03:22:40.000Z","url":"/2024/02/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","tags":[["学习笔记","/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"],["设计模式","/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"单例模式 懒汉式 通常使用双重校验+锁的方式实现 volatile关键字可防止指令重排，比如创建一个对象，JVM会分为三步： 为singleton分配内存空间 初始化singleton对象 将singleton指向分配好的内存空间 指令重排：JVM在保证最终结果正确的情况下，可以不按照程序编码的顺序执行语句，尽可能提高程序的性能 而volatile关键字会防止指令重排，从而保证线程安全。 饿汉式 饿汉式在类加载时就创建好了对象，程序调用时可直接返回对象。 工厂模式 对于接口存在多个实现类时，如果使用if-else语句进行实例化接口的操作，不利于代码扩展，如果换用其他实现类很麻烦。 使用静态工厂模式可以灵活切换实现类，提高通用性。 这样只需调用工厂类的静态方法，传入指定参数即可实例化对象。 代理模式 实现原理： 实现被代理的接口 通过构造函数接收一个被代理的接口实现类 调用被代理的接口实现类，在调用前后增加对应的操作 比如，我们需要在调用代码沙箱前，输出请求参数日志；在代码沙箱调用后，输出响应结果日志，便于管理员去分析数据。 可以使用代理模式，提供一个Proxy，来增强代码沙箱的能力（代理模式的作用就是增强能力） 原本：需要用户自己去调用多次 使用代理后：不仅不用改变原本的代码沙箱实现类，而且对调用者来说，调用方式几乎没有改变，也不需要在每个调用沙箱的地方去写统计代码。 代理类： 使用方法： 策略模式 我们的判题策略可能会有很多种，比如: 我们的代码沙箱本身执行程序需要消耗时间，这个时间可能不同的编程语 言是不同的，比如沙箱执行 Java 要额外花 10 秒。 我们可以采用策略模式，针对不同的情况，定义独立的策略，便于分别修改策略和维护。而不是把所有的判题逻 辑、if ... else ...代码全部混在一起写. 定义一个策略接口，让代码更通用化 定义判题上下文对象，用于定义在策略中传递的参数 (可以理解为一种 DTO) 实现接口，实现各种判题策略 但是，如果选择某种判题策略的过程比较复杂，如果都写在调用判题服务的代码中，代码会越来越复杂，会有大量 if ... else ...，所以建议单独编写一个判断策略的类。 定义JudgeManager，目的是尽量简化对判题功能的调用，让调用方写最少的代码、调用最简单。对于判题 策略的选取，也是在JudgeManager 里处理的。 模板方法模式 定义一个模板方法抽象类。 先复制具体的实现类，把代码从完整的方法抽离成一个个子方法。 接着子类可以继承模板方法 java原生代码沙箱实现，可以直接复用模板方法定义好的方法实现。 docker代码沙箱实现，根据需求重写模板方法。 "},{"title":"SpringSecurity+Redis+Jwt实战运用","date":"2024-01-18T08:20:13.000Z","url":"/2024/01/18/SpringSecurity%E5%AE%9E%E6%88%98%E8%BF%90%E7%94%A8/","tags":[["身份校验","/tags/%E8%BA%AB%E4%BB%BD%E6%A0%A1%E9%AA%8C/"]],"categories":[["Spring","/categories/Spring/"]],"content":"介绍 Spring Security是一个强大且灵活的身份验证和访问控制框架，用于Java应用程序。它是基于Spring框架的一个子项目，旨在为应用程序提供安全性。 Spring Security致力于为Java应用程序提供认证和授权功能。开发者可以轻松地为应用程序添加强大的安全性，以满足各种复杂的安全需求。 SpringSecurity完整流程 JwtAuthenticationTokenFilter：这里是我们自己定义的过滤器，主要负责放行不携带token的请求（如注册或登录请求），并对携带token的请求设置授权信息 UsernamePasswordAuthenticationFilter：负责处理我们在登陆页面填写了用户名密码后的登陆请求。入门案例的认证工作主要由它负责 ExceptionTranslationFilter：处理过滤器链中抛出的任何AccessDeniedException和AuthenticationException FilterSecurityInterceptor：负责权限校验的过滤器。 一般认证工作流程 Authentication接口：它的实现类表示当前访问系统的用户，封装了用户相关信息。 AuthenticationManager接口：定义了认证Authentication的方法 UserDetailsService接口：加载用户特定数据的核心接口。里面定义了一个根据用户名查询用户信息的方法。 UserDetails接口：提供核心用户信息。通过UserDetailsService根据用户名获取处理的用户信息要封装成UserDetails对象返回。然后将这些信息封装到Authentication对象中。 数据库 数据库的采用RBAC权限模型（基于角色的权限控制）进行设计。 RBAC至少需要三张表：用户表–角色表–权限表（多对多的关系比较合理） 用户表（user）：存储用户名、密码等基础信息，进行登录校验 角色表（role）：对用户的角色进行分配 权限表（menu）：存储使用不同功能所需的权限 注册流程 配置匿名访问 在配置类中允许注册请求可以匿名访问 编写实现类 registerDTO中存在字符串roleId和实体类user，先取出user判断是否存在相同手机号。若该手机号没有注册过用户，对密码进行加密后即可将用户存入数据库。 创建register方法映射，保存用户的同时也要将roleId一并存入关系表中，使用户获得对应角色。如下图。 登录流程 配置匿名访问 在配置类中允许登录请求可以匿名访问 调用UserDetailsServiceImpl 登录流程一般对应认证工作流程 先看这段代码：UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(user.getUserPhone(), user.getUserPassword());这里先用用户手机号和密码生成UsernamePasswordAuthenticationToken 再看这段代码：Authentication authenticate = authenticationManager.authenticate(authenticationToken);利用authenticate调用自定义实现类UserDetailsServiceImpl，根据用户名判断用户是否存在（对应认证流程的1、2、3、4） 实现UserDetailsServiceImpl 由于试下的是UserDetailsService接口，所以必须实现其方法loadUserByUsername（根据用户名查询数据库是否存在）这里我传入的是手机号。数据库中若存在用户，则返回UserDetails对象（这里的权限信息暂且不看，对应认证流程的5、5.1、5.2、6） UserDetails对象返回后，authenticate方法会默认通过PasswordEncoder比对UserDetails与Authentication的密码是否相同。因为UserDetails是通过自定义实现类从数据库中查询出的user对象，而Authentication相当于是用户输入的用户名和密码，也就可以理解为通过前面自定义实现类利用用户名查询到用户后，再看这个用户的密码是否正确。如果用户名或密码不正确，authenticate将会为空，则抛出异常信息。（对应认证流程的7） 由于这里的登录流程不涉及8，9，10，所以不再叙述。 在剩下的代码中我们利用用userId生成了jwt的令牌token，将其存入Redis中并返回token给前端。 登出流程 编写过滤器 除login、register请求外的所有请求都需要携带token才能访问，因此需要设计token拦截器代码，如下。 对于不携带token的请求（如登录/注册）直接放行；对于携带token的请求先判断该用户是否登录，即redis中是否存在相关信息，若存在，将用户授权信息存入SecurityContextHolder，方便用户授权，最后直接放行。 此外，还需将token拦截器设置在过滤器UsernamePasswordAuthenticationFilter的前面。 编写实现类 获取SecurityContextHolder中的用户id后，删除redis中存储的值，即登出成功。 授权流程 确保实现类正确编写： 在token拦截器中，我们添加了这段代码。 这样非登录/注册请求都会被设置授权信息。 为对应接口添加注解@PreAuthorize，就会检验该请求是否存在相关请求。 完整代码 config类 controller类 dto类 entity类 UserDetails的实现类 filter类 handler类 service实现类 utils类 "},{"title":"中国大学生服务外包创新创业大赛赛题分析","date":"2024-01-08T12:53:12.000Z","url":"/2024/01/08/%E8%B5%9B%E9%A2%98/","tags":[["赛题分析","/tags/%E8%B5%9B%E9%A2%98%E5%88%86%E6%9E%90/"]],"categories":[["比赛","/categories/%E6%AF%94%E8%B5%9B/"]],"content":"【A01】 基于文心大模型的智能阅卷平台设计与开发 技术栈 算法部分：PaddleOCR+NLP+文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 问题及解决方案 评阅效率与成本 问题描述：传统评阅面临评阅速度较慢、人力需求大的问题，特别是在大量试卷需要评阅时，评阅及反馈时效性能否被满足，将是一个巨大的挑战； 关键点：大量试卷、评阅及反馈时效性 解决方案： 批量评阅，选择、填空、判断题等机器评阅，计算题、简答题等主观题需人工阅卷。主观题可尝试利用自然语言处理（NLP）技术，让机器能够更准确地理解和评价学生的文字表达，将题干和答案输入给文心大模型，由大模型给阅卷老师提出建议辅助阅卷。 分配任务，将试卷任务划分为多个子任务，由多个评阅者同时处理，以提高评阅速度；将试卷任务分配给多个评阅者，多个评阅者负责不同数量的试卷。 实时反馈，设计评阅者和学生两个角色，让评阅者或学生能够迅速获取评阅结果并及时进行反馈，如老师评阅完后生成时间等数据、试卷评阅完后发送信息给学生，除分数外还可以包括评语和建议。学生端可以设计往期的数据统计、教师端可以设计往期评阅趋势等。 有阅无评 问题描述：在实际评阅场景中。往往需要更具有针对性和个性化的评价和建议，而不仅仅是分数反馈。在这过程中，评阅者需要将关键概念，结合学生自身特点进行充分关联，提供更贴近学生需求的知识延展和学科建议； 关键点：针对性和个性化的评价和建议、关联学生特点、提供知识延展和学科建议 解决方案： 反馈设计，评阅者可以输入关键概念给文心大模型，基于文心大模型给出专业评价和建议，或者评阅者自行写出评价和建议。学生端收到反馈后，可以使用文心大模型提取反馈中的知识，并给出学习建议。 增加多样性，包括提供更多的学科相关资源链接、在线学习平台等。这可以帮助学生更全面地理解和拓展相关知识。 学情信息跟踪实际 问题描述：评阅者在实际操作中难以全面、数字化地记录学生的评价和进步情况，进而影响评估的针对性和指导性作用； 关键点：全面、数字化记录学生的评价和进步情况 解决方案： 学情数据可视化，评阅者可以看到该学生历届考试的试卷、评阅建议或成绩趋势图等。 实际评阅风格的单一 问题描述：在实际评阅过程中，评语描述表达方式相对统一，难以通过多样化的评语风格来更好地引导学生理解知识点和提高学业水平； 关键点：多样化的评语风格 解决方案： 通用评语库， 构建一个通用的评语库，包含常见的评价和建议，评阅者可以从中选择或修改，以提高评价的一致性和效率。还可以提供评阅者多样化的评语模板，包括鼓励性的、建议性的、肯定的、挑战性的等不同类型。评阅者可以根据学生的表现灵活选择合适的模板，使评语更富有变化。 学科限制与切换 问题描述：由于不同学科具有独特的评价标准和要求，传统评阅工具难以在不同学科间切换，需要更多的学科专业性和差异化的评阅方式。 关键点：更多的学科专业性和差异化的评阅方式 解决方案： 学科试卷分配给对应的学科老师 学科专业化的大模型支持， 集成不同学科领域的专业化大模型，使其能够理解和应用特定学科的术语、概念和评价标准。这有助于提高评阅的准确性和专业性？？？ 任务清单 （1）试卷图像快速采集与存储； （2）字符识别与提取； （3）内容理解与评阅内容生成； （4）评阅内容的二次编辑； （5）评阅结果的可视化、整理与导出； （6）学情数据可视化； （7）跨平台支持； （8）实时采集与分析（可选）； （9）其它拓展功能和创新方向，如软硬一体解决方案。 功能模块 试卷导入模块 评阅分配模块 智能评阅模块 学情分析模块 个人信息模块 【A15】基于知识图谱的大学生就业能力评价和职位推荐系统 技术栈 算法部分：知识图谱、NLP、推荐和预测模型、文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 参考网站 老鱼简历： 牛客： 数据集 链接： 提取码：vmeh 技术要求与指标 推荐有效性达到80% 以上（用户调查：电子或计算机类相关专业毕业生简历与岗位样例库进行匹配）； 系统数据库中个人隐私信息（姓名、手机、邮箱、通讯地址等至少4项）进行加密，只能被授权人员解密； 架构设计上可并发支持1000人以上同时在线使用，推荐响应时间在5s以内； 技术不限，开发工具不限，可采用开源技术。 业务背景 知识图谱构建 职位推荐系统需要构建一个包含相关职位、技能要求、行业信息等多个领域知识的知识图谱。 聚合和分析用户信息 与知识库相结合，对用户的个人简历、求职意向、工作经验等信息进行聚合和分析处理。根据用户提供的信息学习用户的兴趣和特征，并利用这些特征在知识图谱上匹配职位需求和其他相关信息。 建立推荐模型 根据用户信息设计推荐算法和预测模型。通常来说，推荐算法有协同过滤、关键因素模型、深度学习模型等，具体选择应该根据实际情况来确定。 推荐结果展示 将推荐结果呈现给用户，并支持用户进行选择和反馈。需要注意的是推荐结果的呈现方式也应该根据不同用户群体的需求和喜好来定制化。 问题及解决方案 实现一套大学生就业能力评价和智能岗位推荐系统，根据提供的岗位信息样例库，设计一套含智能算法的软件系统方案：基于知识图谱的岗位信息，利用职位和用户信息，结合推荐算法和相关技术，为用户提供符合其需求和兴趣的职位推荐结果和能力评价结果。 能力评价 问题描述：用户上传个人简历，并明确自己期望的职位，系统自动判断用户与期望职位间的契合度，差异性，给出提升建议，让其知晓对于意向岗位自身知识和技能上的缺陷，找到短板，有针对性提升自我能力。 关键点：判断用户与期望职位间的契合度、给出提升建议 解决方案： 信息提取，利用自然语言处理（NLP）技术提取用户简历和期望职位中的关键词和技能。设计算法计算技能匹配度分数，考虑关键技能的重要性和权重。 构建知识图谱，基于构建的知识图谱，将用户的技能与职位要求进行匹配。利用图谱中的关系和节点信息，评估用户对于职位所需知识的覆盖程度。 差异性分析，分析用户的简历和期望职位之间的差异，包括技能、工作经验、项目经历等方面。 提升建议生成，基于差异性分析和匹配度评估，生成个性化的提升建议。针对用户的短板提供培训建议、学习资源链接、实践项目建议等，以提高其在期望职位上的竞争力。 用户反馈机制，提供用户反馈功能，让用户对系统的匹配度评估和提升建议进行确认。 岗位推荐 问题描述：用户上传个人简历，系统自动分析简历内容，生成推荐职位，用户可以给出推荐是否有效的反馈。如果不满意，可修订简历部分内容，重新进行推荐。 关键点：推荐职位 解决方案： 简历内容分析，利用自然语言处理（NLP）技术对用户上传的简历进行内容分析。 用户兴趣，基于用户上传的历史简历数据，建立用户的兴趣和偏好模型。考虑用户之前的工作经验、求职意向、职业发展方向等信息。 职位推荐算法设计，选择合适的推荐算法，如协同过滤、基于内容的推荐、深度学习模型等。结合用户的兴趣模型和简历特征向量，计算与不同职位的匹配度。 修订简历，如果用户不满意推荐结果，系统应提供修订简历的功能。用户可以编辑、添加或删除简历中的信息，系统重新分析并生成更新后的推荐职位。 用户历史记录查看，用户可以查看自己历史上传的简历和推荐结果，了解职业发展轨迹。 招聘推荐 问题描述：企业招聘人员输入或上传职位要求，系统自动分析匹配求职人员简历，筛选出符合期望的人员列表。 关键点：招聘推荐 解决方案： 职位要求解析，利用自然语言处理技术，对企业输入或上传的职位要求进行解析。提取关键技能、经验要求、学历等信息，构建职位要求的特征向量。 求职人员匹配度计算，基于已有的用户简历数据，计算每个求职人员与职位要求的匹配度。利用算法综合考虑关键技能匹配、工作经验匹配等因素。 候选人员筛选，根据匹配度计算结果，筛选出符合职位要求的候选人员列表。设置合适的匹配度阈值，确保选出的人员满足企业的期望。可以设计系统推荐排名，推荐排名靠前的人员供企业参考。考虑推荐结果的多样性，确保涵盖不同技能和经验背景的候选人。 候选人员详细信息展示，提供候选人员的详细信息，包括简历、技能、工作经历等。支持企业预览候选人员的综合素质，以便更好地做出招聘决策。 用户期望 对开发的产品方案期望如下： （1）算法优化合理，求职与招聘推荐结果与用户期望一致性高； （2）胜任度能力评价结果合理，给出的提升建议符合用户短板； （3）保护简历中个人数据的安全，不侵犯用户隐私； （4）扩展功能：可以对注册用户的数据和操作行为进行统计分析，给出热门职位、热门技能、热门专业等的一些趋势图等。 功能模块 简历上传模块 能力评价模块 岗位推荐模块 招聘推荐模块 热门推荐模块 个人信息模块 队伍分配 A01：周锦辉+罗骏岚+李鑫+杨康庆+禹乐 A15：李翔+李慧聪+周亚+朱豪尔+郑国盛 "},{"title":"【IDEA结合Git实现项目管理实战】四、git冲突篇","date":"2024-01-04T10:21:22.000Z","url":"/2024/01/04/%E5%9B%9B%E3%80%81%E8%A7%A3%E5%86%B3%E4%BB%A3%E7%A0%81%E5%86%B2%E7%AA%81/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 在使用Git进行项目管理时，代码合并是一项常见而重要的操作。本文将重点探讨两种常用的代码合并操作：合并（merge）和变基（rebase）。在进行代码合并时，我们难免会遇到Git冲突的情况。本文也将通过举例详细介绍如何通过IDEA使用Git进行合并或变基操作时可能遇到的代码冲突情况，并提供解决方法。 什么是git冲突 在多分支并行处理时，每个分支可能基于不同版本的主干分支创建。如果每个分支都独立开发而没有进行代码合并，自然不会出现代码冲突。但是，当两个分支同时修改同一文件时，在代码合并时就会出现冲突。 下图为两个分支分别使用合并/变基操作解决冲突后的提交树。 解决git冲突 介绍完冲突出现的原因，那么如何解决冲突呢？在解决git冲突时，我们需要确定以哪个分支的文件版本为准，或者取两个分支的文件的部分片段进行整合。 IDEA提供了强大的冲突解决功能，供用户处理git冲突。下面将进行详细介绍。 当前分支dev1的代码： 目标分支dev的代码： 我们现在的目标是让两个分支合并后的代码中同时出现method1、method2、access和acess2这四个方法。 执行合并后，出现界面： 左侧为当前分支dev1的提交记录，中间为合并前的预览结果，右侧为目标分支dev的提交记录。 其中红色区域为代码存在差异的部分。 先来看第一块红色区域的中间部分的代码。大家一定会疑惑预览结果中出现这段代码是什么意思？为什么会出现报错呢？ 这里其实是git对于左右侧存在差异的代码的标记。符号&lt;&lt;&lt;&lt;&lt;&lt;&lt; xxx的下方是左侧存在差异的代码，符号&gt;&gt;&gt;&gt;&gt;&gt; xxx的上方是右侧存在差异的代码，比如&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD箭头所指方向也就是我们当前分支的方向（在左侧），在该箭头下面的部分是当前分支的与目标分支的差异代码，这里因为左侧比右侧少了一段代码，因此下面啥东西没有；=======代表分割符号，该分割符号的下面就是目标分支的代码，即import java.util.concurrent.TimeUnit；&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev也就代表目标分支的方向（在右侧） 那么如何解决冲突呢？对于我们的目标来说，我们的输出语句自然不需要导入这个包，因此把语句import java.util.concurrent.TimeUnit;给删除即可。 点击左侧的箭头符号，可以把中间区域被替换成左侧的红色区域（那根细线，也就是没有代码）。 点击后中间区域消失。 再来看第二个红色区域，根据我们的目标，我们要将这四个方法都添加进入中间区域。 先点击左侧的箭头。可以发现中间区域被替换为左侧代码，右侧向左箭头变成了向左下箭头。 这个向左下的箭头代表将右侧的代码添加到中间代码的下方。 点击后如图： 那么一切就大功告成了，冲突解决成功，点击应用按钮。 git提示还有冲突未处理，这是为什么？ 把界面翻到上面，发现这个红色区域还没有处理，我们点击那个查号，作用是将冲突标记为已解决。 这时IDEA提示所有变更已被处理，那么我们就可以放心大胆的合并了。 合并成功！ 合并/变基详解 合并（git merge） 当前分支和目标分支执行合并操作时，Git会将当前分支的最新提交记录与目标分支的最新提交记录合并，并在当前分支形成一个新的提交记录。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行合并操作，dev中的提交记录添加到了分支dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行合并操作，git给出提示（已是最新 删除dev），当前分支dev1没有变动。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行合并操作，在目标分支dev1中生成一个新的提交记录Merge branch 'dev' into dev1，该提交记录包含了这两个提交记录的变更，如图。 在提交树中，可以看到两个提交记录合并为一个记录。 变基（git rebase） 当前分支和目标分支执行变基操作时，Git会将目标分支的最新提交记录依次应用到当前分支的每个新的提交记录中。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行变基操作，dev中的两条记录添加到了dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行变基操作，没有发生变化。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行变基操作，dev分支的提交记录添加到了dev1分支中。 总结 可以发现，无论是对于合并还是变基操作的示例1和示例2，最终执行操作后的结果都是一样的。对于合并操作，git将两个分支进行合并，最后生成一个新的提交记录，提交树存在交叉。对于变基操作，git将目标分支的提交记录应用到当前分支，提交树仍然是线性的。如图所示。 至于在实际开发中选择合并还是变基，还是看个人喜好了。 代码冲突示例 注意：本文为方便理解，所有示例均简单的修改项目中的md文件，实际开发中可能存在对多个文件的冲突，但万变不离其宗，只要你具备了解决单个文件代码冲突的能力，那么多个文件的冲突也能轻松应对。 合并/变基分支1 分支情况，当前dev1的两个提交记录博文1和博文2都在dev的提交记录博文3之前，其余分支一样 时间顺序：博文1-&gt;博文2-&gt;博文3 合并 此时合并有代码冲突，解决这个冲突。 发现该冲突只针对博文2，也就是最后一个提交记录 变基 该冲突为博文1和博文3的冲突 该冲突为变基后的博文1和博文2的冲突 合并/变基分支2 分支情况： dev中的博文3在dev1中的博文1和博文2之间 时间顺序：博文1-&gt;博文3-&gt;博文2 合并 基于上述情况，合并分支存在代码冲突 在代码冲突中，存在博文2和博文3的冲突， 冲突解决后如图所示。 这里紫色因为博文3是属于别的分支过来的，其父提交是add README.md.。所以从add README.md.出发，与dev1原本的提交记录博文2结合形成一个新的提交记录Merge branch 'dev' into dev1 结论：分支以时间顺序进行排序，合并分支永远是两个分支的最后一个提交历史进行合并。 变基 博文1和博文3存在冲突 冲突解决后，选择提交消息不变 依然存在冲突 可以发现该冲突来自于已经变基的提交博文1和之后的博文2 得到变基后的提交树 合并/变基分支3 分支情况： dev1中的两个提交记录博文1和博文2在dev中的博文3提交之后 时间顺序：博文3-&gt;博文1-&gt;博文2 合并 博文2和博文3存在代码冲突 变基 冲突来自于博文3和博文1 冲突来自变基后的博文1和博文2 总结 通过这三个代码冲突的示例，看到区别了吗？ 在合并操作时，冲突通常发生在两个分支的最新提交记录上。这是因为合并是将两个不同的分支合并为一个，而最新的提交记录是两个分支的端点。如果两个分支都对同一文件进行了修改，Git 无法确定应该选择哪个更改，因此会产生冲突。 在变基操作时，冲突可能发生在当前分支的提交记录和目标分支的提交记录之间的每个提交记录上。这是因为变基是将一系列提交应用到另一个分支上，而不仅仅是最新的提交。如果两个分支都修改了相同的文件，冲突可能会在每个提交记录上发生，而不仅仅是最新的提交。 总的来说，冲突是由于两个分支都对同一文件进行了修改，而 Git 无法自动解决冲突的情况下发生的。在合并操作中，冲突通常发生在最新的提交记录上；在变基操作中，冲突可能发生在多个提交记录上。"},{"title":"【IDEA结合Git实现项目管理实战】三、实战篇","date":"2023-12-15T04:01:20.000Z","url":"/2023/12/15/%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的实战篇，将正式介绍如何使用IDEA结合Git进行项目管理。 注意：本文假设你已经成功在IDEA中配置了git 在配置篇中，当我们已经在本地推送/克隆了一个项目后，我们能看到如下两个功能块。 在本文中，我们称红色箭头所指为git日志，蓝色箭头所指为git工具栏。 设置默认添加文件 还记得基础篇我们提到的四个工作区域吗？如果我们想要将变更的文件推送到远程仓库，我们首先需要先保证变更的文件在暂存区中。 在基础篇中，我们提到每当创建一个文件时，idea都会询问我们是否将文件添加到git，即是否添加到暂存区中。为了避免这种麻烦，我们打开设置，按以下流程选择无提示添加，这样idea就会默认将我们创建的文件自动添加到git了。 配置完该设置后，相当于每次创建文件都会默认执行git命令git add，作用是将文件添加到暂存区中。 推送更改的文件到本地库 当我们在本地完成了我们的工作后，我们可以先将变更的文件提交到本地库中保存。 这里我们点击蓝色箭头所指git工具栏的第二个按钮，即提交按钮。 该按钮对应git命令git commit，作用是将文件提交到本地库中。 这里我简单的修改了下我自己的更改，填写完提交信息后点击提交。 如果点击提交并推送的话，就可以直接将变更推送到远程仓库，省去了一个步骤。 提交并推送按钮对应git命令git commit &amp; git push，作用是将暂存区的变更直接推送到远程仓库 提交完毕后，我们打开git日志，能在当前的本地分支中看到我们刚才提交记录： 将本地库的提交记录推送到远程库中 当我们已经确定本地库的工作已经结束后，我们就可以将本地库的记录推送到远程库中。 我们点击git工具栏的第三个按钮，即推送按钮。 该按钮对应git命令git push，作用是将本地的提交记录推送到远程库中。 这里我准备了两个提交记录，按ctrl + 鼠标左键可以多选提交记录。 点击推送按钮后，我们可以在git日志中看到远程仓库中出现了我们刚才推送的提交记录。 此时我们打开gitee，可以发现提交记录同样生效于gitee中。 拉取远程分支的变更 当你的远程仓库发生改变时（你的队友推送了提交记录到远程仓库中），而你自己的远程分支不会自动拉取别人的提交记录，也就是说你的远程分支不具备别人的提交历史。 此时我们需要点击提取所有远程按钮，这时候就可以把远程仓库的所有本更拉取到自己的远程分支中。 提取所有远程按钮对应git命令git fetch，作用是将远程仓库的变更拉取到远程分支中。 这里我在gitee中手动添加了一个README.md文件。 点击按钮后，可以看到远程分支中增加了这条记录，同时，我们可以注意到本地分支名后出现了向下的蓝色箭头，这代表着当前远程分支已经更新了，本地分支也应该进行更新。 我们选中对应的本地分支，点击更新所选内容按钮，就可以把远程分支的提交记录添加到我们的本地分支中。 这里我们也可以选择git工具栏的第一个按钮，这样就可以指定方式传入变更。 该按钮对应git命令git merge或git rebase，作用是传入变更到当前分支 结果如下 新建分支 正常来说我们开发项目肯定不可能只用一个本地分支，一般会在其他分支中开发完毕后再推送到主分支中，这时候就需要新建分支的操作了。 分支可以简单的理解为我想基于这个提交以及它所有的父提交进行新的工作。当我们新建分支时，Git就会将HEAD指向的提交记录以及该提交记录之前的所有提交记录保存到新分支中。 如果我们要新建分支，可以点击左侧工具栏的加号按钮，这样就得到了一个新的分支。 该按钮对应git命令git branch，作用是基于当前提交历史创建新的分支。 新建后我们得到了一个全新的分支dev，他包含了master的全部提交历史。 当然，由于HEAD当前指向的是最后一个提交记录，所以新分支dev就包含了master的全部提交记录。如果我们要基于测试1这个提交历史创建分支的话，需要右键该提交历史，点击新建分支后再输入分支名并创建即可。 这样我们就得到了一个包含测试1之前所有提交记录的分支。 回到指定的提交历史 假设你当前开发的代码出现了问题，如何找回之前的代码？这时候就可以通过签出这一git提供功能回到之前的提交历史。当你签出到一个提交历史时，你就获取到了这个提交历史的所有代码。 注意：在签出前确保你当前的代码已经提交到了本地或者远程分支中，签出到其他提交记录时IDEA不会帮你自动保存当前的代码。 在签出的操作中，我们必须要明白HEAD这一概念，HEAD 是一个指向当前所在分支的指针，或者是指向当前所在提交记录的指针。在IDEA中，HEAD的位置可以通过黄色便签来看到，比如在下图master的提交历史中，HEAD指向的分支就是master，同时指向提交消息为add README.md.的提交记录。这里我们将下图称为图1。 如果我们要切换到测试1的提交历史上，右键测试1，点击签出修订。 签出修订按钮对应git命令git checkout，作用是将HEAD指针转移到当前分支/提交记录。 可以看到黄色便签转移到了测试1上，证明我们当前在测试1的提交历史上，这时我们就可以基于测试1的代码进行修改了。这里我们将下图称为图2。 但是这里有个问题，为什么图2在master分支上的黄色便签消失了？而在图1中黄色便签既指向了master又指向了最后一个提交记录add README.md. 这里要明白一个原理，那就是分支本身也能看作是一个指针，这个指针恒指向该分支的最后一个提交记录。 那么在图1中，HEAD其实是通过指向分支进而指向了该分支的最后一个提交记录，即HEAD-&gt;master-&gt;add README.md.。而在图2中，HEAD被称作游离的HEAD，是因为它指向的并不是分支而是该分支的一个提交历史，自然就不会指向master分支了，即HEAD-&gt;测试1。 基于指定提交历史修改代码 在上一个回到指定的提交历史的操作中，我们通过签出的操作获取到了指定的提交历史的代码，这时我们就能够基于这个提交历史的代码进行开发。 举个例子，假设我们要回到之前刚添加md文件的提交历史上进行代码开发，给md文件进行修改。我们当前分支的最新提交为博文3，如何回到提交历史add README.md.上？ 第一步我们肯定要签出到add README.md.上，签出后可以发现项目文件回到了刚开始提交md文件的时候。 这时我们对md文件进行修改，这里我删除了一些段落。 然后点击提交按钮，将这个修改历史666提交到本地分支中。 此时Git给出了警告。如果我们无视警告，仍然点击提交按钮，就会发现原本存在于add README.md.提交记录的指针消失了，而且我们修改后的提交记录666也并没有被提交到本地分支中。 还记得之前我们提到的游离的HEAD吗？如果直接在特定提交上修改代码并运行 git commit，这实际上会在游离的 HEAD 状态下创建一个新的提交，而不会创建分支。这样的操作可能会导致出现一个游离的提交，Git 的垃圾回收机制可能会删除这些提交。 解决方法1 一种方法是：我们在指定提交历史上创建一个分支，然后在新分支上开发完毕后进行提交。这两步操作前文已经详细介绍了，不再赘述，如图所示。 解决方法2 第二种方法则是：对指定提交记录执行git revert命令，该命令会基于指定提交记录创建一个新的提交历史 我们可以右键一个提交记录，选择还原提交选项。 如果当前分支的提交记录和还原的提交记录的文件存在差异的话会出现代码冲突。这里因为我当前分支的第一个提交记录博文3和add README.md.在md文件上存在差异，所以出现了冲突。 由于我们当前的目标是回到add README.md.这个提交记录的代码中，所以我们选择忽略来自其他提交的变更，保留add README.md.提交记录的原本代码即可。 写好提交消息后点击提交 可以发现我们这样就创建了一个和add README.md.一模一样的提交记录了。 接着我们可以基于这个提交记录的代码进行开发，这里前文已经详细介绍了，不再赘述。 将远程分支拉取到本地 设想这样一个场景：你的同事创建了一个新的远程分支并做了一个新的功能，而这个分支是你本地没有的，你现在的工作要基于这个新的功能才能进行下去，那么我们就需要把这个远程分支拉取到本地来。 在这个例子中，你的同事创建了远程分支origin/dev，而我们本地并没有与其对应的分支。 我们只需要右键该分支，点击签出，即可将该远程分支拉取到本地。 代码整合 在项目合作中，将其他人的代码整合到自己的代码中是经常用到的操作，这时就需要利用到Git的合并或变基功能。 一般来说代码整合会遇到以下几种情况： 当前分支正好比其他分支少了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，当前dev1中没有dev中的add README.md.的修改记录。首先确保我们当前分支在dev1分支上。 我们右键dev分支，选择变基或合并均可。 可以看到dev的提交记录整合到了dev1中。 当前分支和其他分支都修改了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，dev1中我写的md文件的内容较少。 在dev中我的队友写的md文件的内容较多。 现在一样要确保当前分支在dev1中，并且右键dev分支选择合并/变基来整合代码。这个时候就出现了冲突，大家应该很容易就想明白了，因为我当前分支和我队友的分支都同时存在文件名相同、内容不同的文件，这个问题不解决的话自然无法整合。 解决冲突的话就需要你和队友进行协商。比如这里我的分支dev1就比较少，队友的分支dev写的比我详细，所以我可以进行\"妥协\"，直接用队友的md文件即可，点击接受他们的这个按钮就可以使用队友的md文件了。 当然，我们也可以不进行妥协，IDEA提供了强大的修订功能，通过点击合并按钮，我们可以选择整合该文件的特定部分。具体如何操作，大家自行练习吧~ 删除提交 我们在实际开发中难免会提交一些无用的提交记录，这时需要利用删除提交的操作。 右键一个提交记录，选择删除提交选项，即可删除。 很简单吧？但要注意已经推送到远程分支的提交是不可删除的。 这里的绿色提交记录代表还未推送到远程分支的提交记录，棕色代表已经推送到远程分支的提交记录。 "},{"title":"【IDEA结合Git实现项目管理实战】二、基础篇","date":"2023-12-10T08:20:10.000Z","url":"/2023/12/10/%E4%BA%8C%E3%80%81Git%E7%9A%84%E5%9B%9B%E4%B8%AA%E5%8C%BA%E5%9F%9F/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的基础篇，将简要介绍使用Git所应该知道的最基本的知识，因此不算深究原理，哈哈。 注意：本文假设你已经成功在IDEA中配置了git Git的四个工作区域 基本概念 Git本地有三个工作区域，分别是工作区、暂存区、本地仓库 Git远程有一个工作区域，叫做远程仓库 工作区：开发者当前工作的目录，主要包含项目的实际文件，其中可能包括未进行版本管理的新文件和已修改的文件。 暂存区：作为一个临时存储区域，用于存放工作区中已经修改的文件的快照，以便将它们作为一个逻辑单元提交到本地仓库。 本地库：包含了项目的提交历史，开发者可以对其进行各种操作，如分支合并、变基、签出等。本地库是在开发者本地计算机上的存储库。 远程库： 存放在代码托管平台（如Gitee、GitHub）上的仓库，包含了已推送的代码版本、分支、标签等信息。开发者可以通过推送和拉取操作与远程库进行交互，以保持代码同步。 概念解释 工作区 一下子引入了这么多概念，可能大家会有点接受不了，那么这里对以上概念进行详细解释。 具体来说，工作区可以看做你在IDEA中打开的项目目录，如下图： 这里的项目目录就可以看作是工作区，但是在这个工作区中，既存在正常颜色（白色）的文件名，也存在红色、绿色和蓝色的文件名，那么这些文件分别代表什么含义？这里的文件的颜色，其实也就对应着上文所谓未进行版本管理的新文件和已修改的文件。如下表： 颜色 含义 白色 在当前提交历史未改动或者已经提交到本地库的文件 红色 未进行版本管理的新文件，也被称为未跟踪的文件 绿色 已经进行版本管理的新文件，该文件已被添加到暂存区中 蓝色 当前工作区中被修改的文件，该文件已被添加到暂存区中，与绿色相似 红色文件名、绿色文件名实例 大家可以自行测试下，如果你直接在项目目录中创建一个文件的话，IDEA会做出提醒： 这时如果你选择添加的话，该文件就会变成绿色文件从而被Git进行版本管理，如果选择取消的话，说明你不希望该文件被Git管理，那么该文件就是红色文件而不能被提交到暂存区。 总结： 当你创建一个新文件时，它就是红色的，表示这是一个未跟踪的文件。通过运行 git add 命令，将文件添加到暂存区，此时文件变为绿色，表示它已经被 Git 管理并准备提交。 如果你不想将一个未跟踪的文件或已修改的文件纳入版本管理，你可以选择不使用 git add 命令，或者使用 git reset 命令来取消已经添加到暂存区的文件。取消后，文件会回到红色状态，表示它未被跟踪或未被修改。 但是对于我们日常开发来说，我们创建一个文件肯定是有用意的，一般都希望该文件被提交上去，所以我们一般都选择添加文件，这样才能通过添加到暂存区，再到本地库最终推送到远程仓库中。 白色、蓝色文件名实例 比如这里我删除了md文件中的其中一行，md文件的文件名由白色转为了蓝色 该文件一般在该项目已经提交到本地库中后，我们准备开发新的功能时才会出现，也就是我们修改了当前已经存在于本地库中的文件，该文件就会被转化成蓝色文件。因为该文件已经被Git版本管理过了，所以可以直接提交到本地库。 总结： 蓝色文件可以直接提交到本地库中，因为其已经被Git版本管理了 蓝色文件可以简单认为就是白色文件被修改后的文件 暂存区 经过前面的内容，大家多少应该可以感觉到，暂存区其实是我们看不见、摸不着的存在，在暂存区中存在的文件一般是绿色和蓝色文件，也就是我们已经使用git add命令添加的文件。 当我们开发完毕一个新的功能后，我们就会准备将这次修改记录提交到本地仓库，这时候使用git commit命令就可以将暂存区中的文件提交到本地仓库中。 提交实例 在该实例中，我们删除了md文件的其中一行（可以看到changes一栏中是蓝色文件，如果你把一个新的文件也添加到暂存区的话就是前文提到的绿色文件），这里我们要在下方框中填写相关介绍，以便帮助其他团队成员或日后的自己知道这次的提交做了什么。 本地库 具体来说，本地库包含了开发者本地完整的提交历史和所有的本地分支，本地库中的提交记录是暂存区通过执行git commit后得来的。 这里要注意本地分支和本地库的区别，本地分支只是本地库众多分支的其中一个分支，当然本地库也可以只包含一个本地分支（但是一般来说开发者不会这么做）。 本地分支实例 这里展示了本地库中的master分支，在这个分支中存在着该分支的提交历史和各种信息。 远程库 远程库是存放在代码托管平台（如Gitee、GitHub）上的仓库，本地库执行git push命令可将本地的提交记录推送到远程仓库中。 远程分支和远程库的区别同本地分支与本地库的区别一样 远程分支实例 同上本地分支实例。 需要注意的是，由于我们在团队开发中涉及到多人的共同协作，因此每个人都可以向远程仓库中推送自己的代码，也就意味在当你在开发新的代码时，远程仓库可能已经发生了变动，那么远程分支也就不能实时和远程仓库的提交记录保持同步了，这时我们需要使用git fetch命令，将远程仓库的提交记录拉取到的远程分支中。 总结 通过对于工作区域的认识，大家想必已经了解了一个文件如何从工作区一步步提交到远程仓库中，当然这里存在着很多操作中的细节，如切换分支，合并来自其他分支的结果等，而我们是使用IDEA结合Git实现版本管理，同样存在着如何具体使用的问题。由于本篇只讲理论，就不过多介绍了，本系列将会继续更新下去，大家敬请期待~"},{"title":"【IDEA结合Git实现项目管理实战】一、配置篇","date":"2023-12-07T02:30:29.000Z","url":"/2023/12/07/%E4%B8%80%E3%80%81IDEA%E9%85%8D%E7%BD%AE/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的配置篇，将介绍如何使用IDEA整合git，从而实现项目管理。 本文将使用Gitee作为项目管理工具。 注意：本文假定你已经拥有了一个Gitee账号并已经配置了密钥。 下载插件 打开设置 在插件中搜索Gitee并下载安装，安装完毕后IDEA会提醒重启IDE，重启后插件才会生效！ 添加账号 IDEA重启后，再次打开设置，在版本控制中可以看到Gitee这一栏，点击加号添加账号 点击加号后我们选择Log in via Gitee 授权完毕后点击应用。 项目管理 最后一个步骤就是正式的实现项目的版本控制了，实现这一步骤有两种操作。 第一种是将IDEA本地的项目上传到gitee中 第二种是从远程clone一个仓库到IDEA本地中 下面我们来逐个介绍这两种操作，并简述这两者之间的区别和使用场景。 1.将IDEA本地的项目上传到Gitee中 现在我们打开你想要托管给Gitee的项目，打开工具栏的VCS，点击Share Project on Gitee 这里我们可以设置仓库名（Repository name），Remote是远程分支名，可以不用修改，Description是这个仓库的描述，这里自己填写即可。 填写完毕后点击share，弹出这个窗口。 我们需要在本窗口中添加需要进行版本管理的文件以进行初始化，可以看到，本项目的所有文件都是理论篇提到的红色文件名的文件，这是因为这个项目还没有上传到远程仓库，也就不存在被Git跟踪的文件，所以都是红色文件名。 这里我们填写下提交信息并点击添加按钮，就可以上传成功了。 上传完毕后，我们可以观察到IDEA中出现了这三个功能块，至于这些功能块有何具体作用，我们将在基础篇详细介绍，这里不过多解释了。 这时候我们可以打开gitee的网站，点击顶部工具栏的头像，选择我的仓库，就可以看到刚刚创建的仓库了! 2.从远程clone一个仓库到IDEA本地中 这一步我们将从远程仓库中clone一个项目到本地中。 我们在gitee中选择一个想要clone的远程仓库，这里我使用的是我自己的远程仓库：  点击VCS，选择从版本控制中获取。 在仓库URL中，在URL中粘贴我们刚才复制的HTTPS地址，在目录中选择我们想要放置远程仓库代码的本地地址，填写完毕后点击克隆。 注意：目录必须是一个空目录 等待克隆完成后项目会自动跳转到你选择的目录。 总结 通过以上的介绍，我们了解到实现版本控制有两种操作： 第一种操作适用于以下场景： 本地你已经开发好了项目，需要将项目托管给远程仓库。 第二种操作适用于以下场景： 团队已经有了远程仓库（有人已经将远程仓库创建好了），这时候我们直接clone即可。 在gitee或github中看到了优质项目，我们clone本地进行学习 "},{"title":"Redis持久化问题排查","date":"2022-10-29T12:49:50.000Z","url":"/2022/10/29/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Redis持久化问题排查 前言：在学习黑马的Redis持久化课程时因为老师用的MobaXterm软件与我使用的VMware存在差异，同时，老师上传的视频的redis配置与我大为不同而导致了一系列的问题，在进行了一天的排查（死杠）后终于解决了问题，感慨良多。在遇到问题解决不了的时候还是要多回头看，没准在出发点就能找到问题了。 下面就将展示问题的描述和解决方案 问题描述 老师在使用MobaXterm软件时使用redis-server redis.conf命令指定配置文件在前台启动Redis 然而我在使用redis-server命令却出现了下面的两种情况： 第一种：不指定配置文件启动Redis 发现监听6379端口失败 第二种：指定配置文件启动Redis 这里要先cd到自己的Redis安装路径下，然后使用命令redis-server redis.conf 发现没有产生日志，这里使用ps -ef | grep redis查看后端端口占用情况 显然，后台端口已经被占用了，这里是因为Redis配置了开机自启动和后台启动，导致端口占用而无法使用redis-server命令 问题解决 在执行以下操作前建议使用VMware创建一个快照，防止后续操作不当导致虚拟机设置出现问题 先使用以下命令关闭后台端口 此时cd到Redis的安装目录下，使用命令ps -ef | grep redis查看后台端口占用情况，发现后台端口关闭 然后在当前路径下使用vi redis.conf关闭Redis后台启动 将 daemonize 修改为 no 这样Redis就不会在后台启动了！ 接着可以开心的使用redis-server redis.conf指定配置文件启动redis了！ 然而，如果你使用该命令后又出现了如下情况 发现当前窗口确实是像前台启动一样阻塞的，但是却没有任何日志信息输出，这时我们打开RESP等图形化界面看一下连接是否成功 嗯，确实成功了，说明Redis服务确实启动了，但还是没有日志文件，那么这是怎么回事呢？ 哈哈，不用着急，我们先使用ctrl + c停止当前端口，然后再回到RESP上发现连接终止，这就说明我们之前做的操作没有问题！现在我们回到终端，使用vi redis.conf命令进入到配置文件中 找到logfile，发现logfile后面的引号内为一个日志文件 这个时候一切都真相大白了，因为在一开始配置Redis时，我们将在logfile的引号内写入了一个日志文件名称（一开始的logfile内的引号默认是空的），这就导致了我们使用redis-server命令输出的日志都进入到这个日志文件中，所以启动时当然看不到任何信息了！现在我们将引号内的文件命删除，保存并退出 再次使用redis-server redis.conf命令启动Redis 启动成功，大功告成！"},{"title":"VMWare演示Redis持久化","date":"2022-10-28T12:49:50.000Z","url":"/2022/10/28/VMWare%E6%BC%94%E7%A4%BARedis%E6%8C%81%E4%B9%85%E5%8C%96/","tags":[["学习笔记","/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"]],"categories":[["Linux","/categories/Linux/"]],"content":"VMware演示Redis持久化 Redis持久化的作用 Redis是内存存储，如果出现服务器宕机、服务重启等情况可能会丢失数据，利用Redis持久化可以将数据写入磁盘中，这样Redis就可以利用持久化的文件进行数据恢复，数据安全性得以大大提升。 RDB RDB全称为Redis Database Backup file(Redis数据备份文件)，也被叫做Redis数据快照。简单来说就是把内存重的所有数据都记录道磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。 RDB实现数据持久化有两种命令：save和bgsave save命令适合用于准备将Redis停机的情况。 在执行save命令时会占用Redis的主进程，阻塞所有命令，因此不适用于Redis正在运行的情况。 而bgsave命令会执行fork操作开启一个子进程，避免主进程收到影响。 此外，当Redis停机时会执行一次RDB操作进行数据备份。 下面将对该情况进行验证 RDB演示 使用redis-server redis.conf指令在任意路径下运行Redis 接下来创建一个新的终端创建连接，存入键值对 接着回到上一个终端使用ctrl + c关闭Redis，可以看到DB saved on disk表示文件正常保存 在当前路径下使用ll查看所有文件，看到dump.rdb快照文件已经生成 接着再次在当前终端使用redis-server redis.conf启动Redis，此时数据会自动恢复，回到另一个终端窗口 此时将使用ctrl + c关闭当前命令行客户端，重新使用redis-cli开启一个新的命令行 这时使用get num命令发现数据返回为789，（如果get num返回错误Error：Server closed the connection要将redis.conf的protected-mode设置为No关闭保护模式）表明数据备份成功 Redis停机自动执行RDB证明完毕！ 实现RDB的相关配置 这里需要 cd 到redis的安装目录下，我的安装目录在 /usr/local/src/redis-6.2.6下 这里老师用的MobaXterm可以直接使用第三方文件打开配置文件进行修改，因为我用的是VMware所以只能使用Linux命令进行修改配置 使用 vi redis.conf修改配置文件 这里有一些小技巧： 按下INSERT键进入修改状态，此时才可以对文件进行修改 按下ESC键可以使用一些命令：/xxx可以查找跳跃到当前文件中的xxx名称，:wq保存当前文件并退出，:q不保存并退出当前文件，:q!不保存并强制退出当前文件 下面就可以对配置文件进行修改了！ 修改为五秒内执行一次操作就出发RDB备份 接着 再查找rdbcompression后修改为yes即可 使用:wq保存并退出 rdb文件名修改后使用终端再次运行Redis，发现此时没有DB文件录入消息，之前的dump.rdb文件已不能被读取 到另一个终端重新使用redis-cli开启命令行，使用get num命令发现结果为空，再set 一个新的键值，回到Redis窗口发现出现Background saving started字段即表示成功！ AOF AOF全称为Append Only File (追加文件)。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 开启AOF需要在Redis的配置文件中修改配置，AOF记录命令的频率有三种:always 、everysec和no always配置胜在能记录每一次Redis执行的写命令，几乎不会丢失数据，但对性能会有很大的影响 everysec配置每隔一秒将缓冲区里的数据写入到AOF文件，避免直接对AOF文件操作而性能上有了提升，但是在数据间隔的一秒内如果出现服务宕机的情况会丢失最多一秒的数据 no配置执行频率最低，性能最好，但是可靠性很低，不推荐使用 AOF演示 先使用 cd 命令进入到Redis的安装路径下 使用vi redis.conf命令修改redis配置文件 将之前的save 5 1注释掉，写入save \"\"，表示禁用RDB 将appendonly修改为yes，开启AOF AOF执行频率的命令配置默认为appendfsync everysec，不需要修改 修改完毕后回到终端，使用rm -rf *.rdb删除之前的RDB文件 可以看到当前目录下rdb文件已删除 使用redis-server redis.conf命令重启Redis 发现日志中出现没有对RDB的读取，修改成功 在另一终端打开命令行，输入如下指令 返回空集合，说明所有数据已被清除 img set一个键值对 这时我们打开文件夹进入本机的Redis的安装路径下，发现AOF文件已经被创建成功，打开AOF文件，set命令已经被写入成功，说明AOF已经生效了 接下来验证AOF的重启恢复，先重启Redis服务，红框内的日志表示数据已经从AOF文件加载完毕 这时候我们回到命令行使用keys *命令，返回num，依然有数据，说明AOF重启能保证数据的恢复，证明完毕 RDB和AOF的比较 "},{"title":"VMware软件安装及问题解决","date":"2022-10-08T03:26:00.000Z","url":"/2022/10/08/VMware%E5%AE%89%E8%A3%85/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"VMware软件安装 前言：最近学习Redis时需要使用Linux系统，导致不得不去安装一个虚拟机了，在准备安装VMware软件时遇到了一些问题，这里给出记录和解决方案，以做参考学习使用 准备工作 本文将在Windows系统下使用VMware软件配置Linux虚拟机，下文将会详解VMware软件的安装步骤 VMware软件下载地址： Linux CentOS 7版本下载映像文件地址： 按图片所示下载即可 这里我选择的是CentOS-7.0-x86_64-DVD-2009.iso 版本 在下载路径下右键以管理员身份运行 在这里要敲重点！！！如果你没有以管理员身份运行这个选项，那么就看接下来的问题解决部分，如果你有这个选项请点击跳转自行无视。 问题解决 按住快捷键Win+R打开运行窗口，输入“regedit”， 这样就打开了注册表编辑器 在编辑器左侧依次找到HKEY_CURRENT_USER 然后将RestrictToPermittedSnapins的值设置为0 上面的方法如果输入路径后发现MMC不存在，那么下面这个方法就派上用场了！ 按下Win + R键打开运行，输入gpedit.msc打开组策略编辑器，这里可能你又会惊讶的发现，gpedit.msc打不开了！别着急，下面还有解决方案。当然，如果你能打开组策略编辑器的话可以点击这里继续往下看 对于无法打开组策略编辑器的情况，win + r 键打开运行后，输入notepad打开记事本，复制粘贴以下内容 记事本左上角文件另存为到任意路径下 文件名写为gpedit.cmd，保存类型选择所有文件，编码选择ANSI 点击保存后在你保存的路径下右键以管理员身份运行 等待运行结束后发现win + R键打开的gpedit.msc可以正常打开了 接下来进入组策略编辑器，双击计算机配置 --&gt; Windows设置 --&gt; 安全设置 双击本地策略 进入安全选项找到下面两项选择已启用 大功告成！！！ 安装VMware 当解决上述问题后开始准备下载VM软件了，右键以管理员身份运行后等待片刻来到安装界面 一路下一步来到这个界面，安装路径自己选择，我选择安装在D盘下的目录里 勾选取消“启动时检查产品更新” 一路下一步，点击安装 点击许可证 这里要输入许可证密钥（这里大家自行网络上获取），然后点击输入，安装结束，注意不要误点到跳过了。 输入密钥确认安装后就可以使用VMware了！"}]