[{"title":"【并发编程】全面解析volatile和synchronized关键字","date":"2024-09-10T07:12:31.000Z","url":"/2024/09/10/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90volatile%E5%92%8Csynchronized%E5%85%B3%E9%94%AE%E5%AD%97/","tags":[["volatile","/tags/volatile/"],["synchronized","/tags/synchronized/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"volatile 可见性问题 Java内存模型： 在Java内存模型（JMM）中，每个线程有自己的工作内存（这是一个抽象概念，不同于物理内存中的缓存），用于存储从主内存中读取的变量副本。线程对变量的操作（如读取、写入）通常在其工作内存中进行，而不是直接在主内存中操作。 当一个线程修改了工作内存中的变量副本后，新的值可能不会立即刷新回主内存。这意味着其他线程在其工作内存中读取该变量时，可能仍然看到旧值，而不是最新值。 CPU缓存机制： 在物理层面上，现代CPU使用多级缓存（如L1、L2、L3）来加速内存访问。每个处理器核心可能会将主内存中的数据加载到自己的缓存中，并在缓存中进行操作。 当一个线程运行在一个处理器核心上，并修改了缓存中的数据，其他处理器核心的缓存可能不会立即同步这些修改，导致其他线程看到的是旧数据。这种情况在多核处理器系统中尤为常见。 指令重排序： 为了优化性能，编译器和CPU可能会对指令进行重排序。例如，编译器可能会将一些操作的顺序调整，使得对变量的读取和写入操作不按程序的逻辑顺序执行。 这种重排序可能导致一个线程的操作在另一个线程中以错误的顺序被观察到，特别是在没有适当的同步措施时。例如，线程A可能先执行了对变量的写操作，而线程B却在此之前读取了这个变量的旧值。 变量可见性 volatile 关键字可以强制将变量的更新立即写入主内存，并确保其他线程读取时能看到最新的值。如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 指令重排序 在 Java 中，volatile 关键字除了可以保证变量的可见性，还有一个重要的作用就是防止 JVM 的指令重排序。 如果我们将变量声明为 volatile ，在对这个变量进行读写操作的时候，会通过插入特定的 内存屏障 的方式来禁止指令重排序。 指令重排序的分类： JVM重排序：JVM在将字节码转换为机器码时，可能会进行指令的重新排列。 CPU重排序：处理器在执行机器码指令时，也可能会根据当前的硬件状态（如缓存、流水线等）对指令进行重新排列。 编译器重排序：编译器在编译代码时可能会调整代码的顺序，以生成更高效的字节码。 指令重排序导致的问题 指令重排序在单线程环境中通常不会导致问题，因为编译器和处理器都会确保程序的最终结果与代码的顺序一致。然而，在多线程环境下，指令重排序可能会导致数据可见性和线程安全问题。 可见性问题：由于指令重排序，一个线程可能在另一个线程未完成操作前看到中间状态的变量值，导致读取到不正确的数据。 线程安全问题：当多个线程同时访问和修改共享变量时，如果没有适当的同步机制，指令重排序可能会使得线程间的操作顺序不同于预期，从而引发竞态条件，导致程序行为异常。 synchronized synchronized 是 Java 中的一个关键字，翻译成中文是同步的意思，主要解决的是多个线程之间访问资源的同步性，可以保证被它修饰的方法或者代码块在任意时刻只能有一个线程执行。 使用方法 1.修饰实例方法 （锁当前对象实例） 给当前对象实例加锁，进入同步代码前要获得 当前对象实例的锁 。 2.修饰静态方法 （锁当前类） 给当前类加锁，会作用于类的所有对象实例 ，进入同步代码前要获得 当前 class 的锁。 这是因为静态成员不属于任何一个实例对象，归整个类所有，不依赖于类的特定实例，被类的所有实例共享。 3.修饰代码块 （锁指定对象/类） 对括号里指定的对象/类加锁： synchronized(object) 表示进入同步代码库前要获得 给定对象的锁。 synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁 构造方法可以使用synchronized吗？ 不能，因为构造方法本来就是线程安全的，不存在同步的构造方法一说。 静态 synchronized 方法和非静态 synchronized 方法之间的调用互斥吗？ 不互斥。因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例的锁。 加锁方式 synchronized关键字对方法加锁是通过方法的flags标志来实现的，而对同步代码块加锁则是通过monitorenter和monitorexit指令来实现的。 同步方法的flags里面多了一个ACC_SYNCHRONIZED标志。当某个线程要访问某个方法是，会检查是否存在ACC_SYNCHRONIZED标识，如果有设置，则需要先获取监视器锁，方法执行后再释放监视器锁。 同步代码块是由monitorenter获取锁，然后monitorexit释放锁。在执行monitorenter之前需要尝试获取锁，如果这个对象没有被锁定，或者当前线程已经拥有了这个对象的锁，那么就把锁的计数器加一。当执行monitorexit指令时，锁的计数器也会减一。当获取锁失败时会被阻塞，一直等待锁被释放。 对象结构分析 要深入理解 synchronized 的实现原理，必须先了解 Java 对象在内存中的布局，尤其是对象头中的 Mark Word。 对象内存结构 在 Hotspot 虚拟机中，对象在内存中的布局可以分为 3 块区域：对象头、实例数据和对齐填充。 对象头包括两部分信息： 标记字段（Mark Word）：用于存储对象自身的运行时数据， 如哈希码（HashCode）、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。synchronized就是依靠Mark Word字段来进行锁升级、获取锁等操作的。 类型指针（Klass Word）：对象指向它的类元数据的指针，JVM可以通过该指针来确定这个对象是哪个类的实例。 实例数据是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为 Hotspot 虚拟机的自动内存管理系统要求对象起始地址必须是 8 字节的整数倍，换句话说就是对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 Mark Word Mark Word 不仅存储了对象的基本信息，还承担着锁状态的管理职责，包括锁的获取、升级、释放等操作。正是通过对 Mark Word 的精细控制，Java 虚拟机（JVM）才能高效地实现线程同步机制，从而保障并发环境下的数据一致性和程序的正确性。 在Java中，锁的状态分为四种，分别是无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态。 Mark Word的低两位用于表示锁的状态，分别为\"01\"(无锁状态)、\"01\"(偏向锁状态)、\"00\"(轻量级锁状态)和\"10\"(重量级锁状态)。但是由于无锁状态和偏向锁都是”01\"，所以在低三位引入偏向锁标记位用\"0\"表示无锁，\"1\"表示偏向。 锁升级过程 对于synchronized关键字，一共有四种状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态（级别从低到高）。 偏向锁 偏向锁提升性能的经验依据是：对于绝大部分锁，在整个同步周期内不仅不存在竞争，而且总由同一线程多次获得。偏向锁会偏向第一个获得它的线程，如果接下来的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程不需要再进行同步。这使得线程获取锁的代价更低。 触发条件：如果JVM启动参数没有禁用偏向锁，那么首次进入synchronized块时会自动开启。 偏向锁的获取过程： 初始状态：当一个对象刚创建时，它处于无锁状态。此时，Mark Word 中包含对象的哈希码和其他一些标记信息。 线程尝试加锁：当线程第一次访问这个对象并进入 synchronized 块时，JVM 会将 Mark Word 中的锁标志位设置为偏向锁，并将该线程的 ID 记录在 Mark Word 中。这样，锁就“偏向”于这个线程。 偏向锁加锁成功：如果同一线程再次访问该对象的 synchronized 块，JVM 检查 Mark Word 中的线程 ID，如果与当前线程的 ID 匹配，则认为该线程已经持有了锁，无需执行任何额外操作，直接进入同步块。这避免了加锁和解锁的开销。 偏向锁的释放过程： 解锁时：当持有偏向锁的线程退出 synchronized 块时，锁不会立即释放，Mark Word 中的线程 ID 和偏向标记位保持不变，仍然表示锁偏向于该线程。 锁重入：如果偏向锁的线程再次进入 synchronized 块，可以直接重用这个锁而无需加锁操作。 锁撤销（偏向锁失效）：如果另一线程尝试访问同一个对象的synchronized块，JVM 发现Mark Word 中的线程 ID 与当前线程不符，偏向锁就会被撤销。偏向锁撤销的过程包括： 偏向锁升级为轻量级锁：JVM 会暂停持有偏向锁的线程，撤销偏向锁，将 Mark Word 更新为指向轻量级锁的状态，并将锁记录在新的线程的栈中。 新线程继续尝试获取锁：如果当前没有其他线程竞争锁，新的线程将获取轻量级锁；否则，锁将进一步升级为重量级锁。 轻量级锁 轻量级锁是相对基于OS的互斥量实现的重量级锁而言的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用OS的互斥量而带来的性能消耗。 轻量级锁提升性能的经验依据是：对于绝大部分锁，在整个同步周期内都是不存在竞争的。如果没有竞争，轻量级锁就可以使用 CAS 操作避免互斥量的开销，从而提升效率。 触发条件：当有另一个线程尝试获取已被偏向的锁时，偏向锁会升级为轻量级锁。 轻量级锁的加锁过程： 创建锁记录（Lock Record）：当线程进入同步代码块时，JVM 会在当前线程的栈帧中创建一个锁记录（Lock Record）。这个锁记录用于存储对象当前的 Mark Word 的副本，称为 Displaced Mark Word。此时，锁记录中的 _owner 指针会指向对象的 Mark Word。 CAS 尝试加锁：JVM 使用 CAS（Compare-And-Swap）操作尝试将对象头中的 Mark Word 更新为指向线程栈中 Lock Record 的指针。如果 CAS 操作成功，则表明当前线程获取了轻量级锁，锁的标志位（lock state bits）将更新为 00，表示轻量级锁。 加锁成功：若 CAS 操作成功，表示该线程已经拥有了该对象的锁，可以安全地进入同步代码块执行。 加锁失败（锁竞争）：如果 CAS 操作失败，JVM 会检查对象的Mark Word 是否指向当前线程的栈帧中的锁记录： 当前线程已拥有锁： 如果 Mark Word 已经指向当前线程的锁记录，说明该线程已拥有锁，可以直接进入同步代码块继续执行。 锁被其他线程持有： 如果 Mark Word 指向的是其他线程的锁记录，说明锁被其他线程占用。此时，当前线程会尝试通过 自旋 一定次数获取锁。如果在多次自旋后 CAS 仍然失败，轻量级锁会升级为 重量级锁（标志位更新为 10），并将 Mark Word 指向重量级锁的指针。之后，未获得锁的线程将进入阻塞状态，等待锁的释放。 轻量级锁的解锁过程： CAS 释放锁：当线程退出同步代码块时，JVM 会使用 CAS 操作，将线程中锁记录的 Displaced Mark Word 恢复到对象的 Mark Word，即通过 CAS 将对象头的 Mark Word 替换为最初的值。 解锁成功：如果 CAS 操作成功，意味着锁成功释放，整个同步过程完成。 解锁失败（锁竞争）：如果 CAS 操作失败，说明有其他线程正在尝试获取该锁，或者锁已经升级为重量级锁。在这种情况下，JVM 会在释放锁的同时，唤醒等待该锁的阻塞线程。 重量级锁 监视器锁实现 synchronized 关键字用于实现线程同步，而它的底层是通过监视器锁（Monitor）来实现的。无论是对方法加锁还是对同步代码块加锁，JVM 都是依赖 monitor 机制来保证同步。在进入同步代码块之前，线程必须先获取监视器锁。成功获取锁后，锁的计数器增加 1；执行完同步代码后，计数器减少 1。如果线程无法获取锁，它会进入阻塞状态，直到锁被释放。 在Hotspot中，对象的监视器（monitor）锁对象由ObjectMonitor对象实现（C++），其跟同步相关的数据结构如下： 当多个线程同时访问一段同步代码时，首先会进入_EntryList队列中，当某个线程获取到对象的monitor后进入_owner区域并把 monitor 中的_owner变量设置为当前线程，同时 monitor 中的计数器count加一，即获得对象锁。 若持有 monitor 的线程调用wait()方法，将释放当前持有的 monitor，_owner变量恢复为null，_count自减一，同时该线程进入_WaitSet集合中等待被唤醒。若当前线程执行完毕也将释放 monitor(锁)并复位变量的值，以便其他线程进入获取 monitor(锁)。 加解锁 监视器锁monitor本质上是依赖操作系统的 Mutex Lock 互斥量 来实现的，我们一般称之为重量级锁。因为 OS 实现线程间的切换需要从用户态转换到核心态，这个转换过程成本较高，耗时相对较长，因此synchronized效率会比较低。 重量级锁的锁标志位为'10'，指针指向的是monitor对象的起始地址。 只有在使用重量级锁时，才会涉及到 monitor 及线程状态的控制。线程的生命周期分为五个状态，分别是初始状态（new）、运行状态（runnable）、等待状态（waiting）、阻塞状态（blocking）和终止状态（terminated）。 触发条件：当轻量级锁的CAS操作失败，轻量级锁升级为重量级锁。 重量级锁的加锁过程： 线程进入阻塞队列：如果锁已被其他线程持有，后续尝试获取该锁的线程会进入 ObjectMonitor 对象的 _EntryList 队列，处于阻塞状态等待锁。 线程获取锁进入运行状态：成功获取锁的线程会从 _EntryList 中移出，进入运行状态。此时，ObjectMonitor 的 _owner 字段指向该线程，_count 加一。如果线程重入锁，_count 递增。 调用 wait() 方法进入等待状态：线程调用 wait() 时，释放 monitor 锁，进入等待状态，_owner 置为 null，_count 减一。线程进入 _WaitSet 队列，等待 notify() 或 notifyAll() 唤醒。 线程被唤醒后重新竞争锁：被唤醒的线程从 _WaitSet 移至 _EntryList 队列，重新竞争锁。获取锁后，进入运行状态，_owner 指向该线程。 重量级锁的解锁过程： 线程正常退出同步代码块：线程退出同步代码块时，_count 减一。若 _count 为零，表示锁完全释放。 释放锁和唤醒其他线程：如果 _count 为零，_owner 置为 null。如果 _EntryList 队列中有其他线程，JVM 会唤醒其中一个，让其尝试获取锁。 线程退出或等待结束：线程执行完同步代码或被唤醒后，若锁可用，获取锁并继续执行，否则重新进入阻塞队列。锁释放时，_owner 置为 null，其他线程有机会获取锁。 锁优化 自旋锁 获取轻量级锁时：即当一个线程尝试获取一个被其他线程持有的轻量级锁时，会自旋等待锁的持有者释放锁。 在OpenJDK 8中，轻量级锁的自旋默认是开启的，最多自旋15次，每次自旋的时间逐渐延长。如果15次自旋后仍然没有获取到锁，就会升级为重量级锁。 获取重量级锁时：即当一个线程尝试获取一个被其他线程持有的重量级锁时，它会自旋等待锁的持有者释放锁。 在OpenJDK 8中，默认情况下不会开启重量级锁自旋。如果线程在尝试获取重量级锁时，发现该锁已经被其他线程占用，那么线程会直接阻塞，等待锁被释放。如果锁被持有时间很短，可以考虑开启重量级锁自旋，避免线程挂起和恢复带来的性能损失。 自适应自旋：在JDK6之后的版本中，JVM引入了自适应的自旋机制。该机制通过监控轻量级锁自旋等待的情况，动态调整自旋等待的时间。 如果自旋等待的时间很短，说明锁的竞争不激烈，当前线程可以自旋等待一段时间，避免线程挂起和恢复带来的性能损失。如果自旋等待的时间较长，说明锁的竞争比较激烈，当前线程应该及时释放CPU资源，让其他线程有机会执行。 锁消除 锁消除是JVM在编译或即时编译（JIT）过程中通过逃逸分析判断锁对象是否只在单个线程中使用。如果确定该锁对象不会逃逸到其他线程，即它只在当前线程中被使用，JVM会自动移除这些不必要的同步锁，从而减少锁的开销。 锁粗化 锁粗化是指将多个连续的、临近的小范围锁操作合并为一个更大的锁操作，以减少加锁和解锁的频率，从而提高性能。锁粗化的主要目的是避免在短时间内频繁进行锁的获取和释放操作，因为每次加锁和解锁都带有一定的开销。 两者比较 synchronized 关键字和 volatile 关键字是两个互补的存在，而不是对立的存在！ volatile 关键字是线程同步的轻量级实现，所以 volatile性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量而 synchronized 关键字可以修饰方法以及代码块。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 "},{"title":"【并发编程】从AQS机制到同步工具类","date":"2024-08-30T10:14:20.000Z","url":"/2024/08/30/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E4%BB%8EAQS%E6%9C%BA%E5%88%B6%E5%88%B0%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7%E7%B1%BB/","tags":[["AQS机制","/tags/AQS%E6%9C%BA%E5%88%B6/"],["ReentrantLock","/tags/ReentrantLock/"],["CountDownLatch","/tags/CountDownLatch/"],["CyclicBarrier","/tags/CyclicBarrier/"],["Semaphore","/tags/Semaphore/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"AQS机制 Java 中常用的锁主要有两类，一种是 Synchronized 修饰的锁，被称为 Java 内置锁或监视器锁。另一种就是在 JUC 包中的各类同步器，包括 ReentrantLock（可重入锁）、Semaphore（信号量）、CountDownLatch 等。 所有的同步器都是基于AQS机制来构建的，而 AQS 类的核心数据结构是一种名为CLH锁的变体。 CLH锁 CLH锁是一种基于链表的自旋锁，它通过维护一个隐式的等待队列来实现线程的公平性和高效性。CLH锁的核心思想是每个线程在进入临界区时都会在队列尾部排队，并且自旋等待前驱节点的状态变化。CLH 锁的特点是，它将等待线程的状态信息保存在前驱节点中，而不是在本线程中，这样就避免了过多的缓存一致性流量。 隐式双向链表 加锁过程： 初始化：CLH 锁初始化时，Tail 指向一个状态为 false 的空节点。 线程入队： 线程尝试获取锁时，创建一个状态为 true 的新节点，表示正在等待锁。 线程通过 CAS 操作将新节点插入队列尾部，并更新 Tail 指针。 轮询前驱节点状态：线程不断轮询其前驱节点的状态，直到前驱节点的状态变为 false，表示可以获取锁。 解锁过程： 释放锁：线程完成临界区访问后，将当前节点的状态设置为 false，表示释放锁。 后继节点获取锁：后继节点检测到前驱节点状态变化，获取锁并进入临界区。 AQS对CLH锁的改造 CLH锁存在缺点： 自旋操作，当锁持有时间长时会带来较大的 CPU 开销。 基本的 CLH 锁功能单一，不改造不能支持复杂的功能。 Java 的 AbstractQueuedSynchronizer（AQS）借鉴了 CLH 锁的思想，并在此基础上做了诸多改进，使其更适合构建高效、可扩展的同步器。以下是 AQS 对 CLH 锁所做的一些主要改造： 显式双向链表 AQS 使用了显式的双向链表来维护等待队列，而不是隐式的单向链表。这样改进的好处是，它允许 AQS 更方便地处理队列中的节点操作，比如取消、唤醒特定节点等。 AQS 加锁过程： 初始化：AQS 初始化时，等待队列为空，head 和 tail 指针均为 null。 线程入队： 线程尝试获取锁时，会检查当前锁的状态（state）。如果锁已被占用，线程会创建一个新的节点（Node），表示自己需要等待锁。 线程通过 CAS 操作将新节点原子性地插入到等待队列的尾部，并更新 tail 指针指向新节点。 如果队列为空，当前节点会成为队列中的第一个节点。 线程阻塞与等待：如果当前线程无法立即获取锁（state 不为 0），线程会进入阻塞状态，调用 LockSupport.park() 挂起自己，直到被唤醒为止。 AQS 解锁过程： 释放锁：持有锁的线程完成临界区的操作后，会调用 release(int arg) 方法将 state 变量设置为 0，表示锁已释放。 唤醒后继节点：AQS 会调用 LockSupport.unpark(Thread) 来唤醒后继节点的线程，使其从 park() 的阻塞状态中恢复。 后继节点获取锁：被唤醒的后继节点线程会重新尝试获取锁，通过 CAS 操作将 state 从 0 更新为 1。如果成功获取锁，线程将进入临界区执行任务。 多种同步模式 AQS 提供了独占锁（exclusive）和共享锁（shared）两种模式。例如，ReentrantLock 使用的是独占模式，而 Semaphore 和 CountDownLatch 使用的是共享模式。 一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现钩子方法中的tryAcquire-tryRelease或tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。 同步状态变量 AQS 使用一个 int 型变量（称为同步状态 state）来表示锁的状态，而不是像 CLH 锁那样依赖前驱节点的布尔变量。AQS 通过 CAS 操作来修改这个状态，确保线程安全。 ReentrantLock 的可重入性：ReentrantLock 通过内部的 state 变量表示锁的占用状态。初始 state 为 0，表示未锁定。当线程 A 调用 lock() 时，通过 tryAcquire() 方法尝试获取锁并将 state 加 1。若获取成功，线程 A 可以多次获取同一锁，state 会累加，体现可重入性。释放时，state 减 1，直到回到 0，锁才真正释放，其他线程才有机会获取锁。 CountDownLatch 的倒计时：CountDownLatch 使用 state 变量表示剩余的倒计时数。初始 state 为 N，表示 N 个子线程。每个子线程执行完任务后调用 countDown()，state 减 1。所有子线程执行完毕（state 变为 0）后，主线程被唤醒，继续执行后续操作。 实现同步器 AQS 的设计：AQS 提供了一个基础的框架和队列管理功能，但具体的同步逻辑并没有在 AQS 中实现，而是留给具体的同步器来定义。这就是模板方法模式的典型应用：AQS 提供了模板方法，这些模板方法依赖于子类实现的钩子方法。 重写钩子方法：具体的同步器，如 ReentrantLock、Semaphore，会通过其内部定义的 Sync 类（继承自 AQS）来重写这些钩子方法。这些重写的方法决定了锁的行为（如是否公平、是否可重入、许可的数量等）。 什么是钩子方法？ 钩子方法是一种被声明在抽象类中的方法，一般使用 protected 关键字修饰，它可以是空方法（由子类实现），也可以是默认实现的方法。模板设计模式通过钩子方法控制固定步骤的实现。 除了这些钩子方法，AQS类中其他方法都是final关键字修饰的，无法被重写。 常见同步工具类 ReentrantLock ReentrantLock是一种可重入的互斥锁，允许同一个线程在持有锁的情况下多次获取锁。它提供了更灵活的锁机制，可以显式地获取和释放锁，还支持公平锁和非公平锁的选择。通常用来实现线程间的同步，防止多个线程同时访问共享资源。 ReentrantLock 有一个内部类 Sync，Sync 继承 AQS，添加锁和释放锁的大部分操作实际上都是在 Sync 中实现的。Sync 有公平锁 FairSync 和非公平锁 NonfairSync 两个子类。 公平锁/非公平锁 抽象类Sync继承自AbstractQueuedSynchronizer，实现了AQS的部分方法； NonfairSync继承自Sync，实现了Sync中的方法，主要用于非公平锁的获取； FairSync继承自Sync，实现了Sync中的方法，主要用于公平锁的获取。 可以通过构造方法实现公平锁或非公平锁。 公平锁和非公平锁只有两处不同： 公平锁：在调用lock()后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。但是公平锁会先判断等待列中是否有处于等待状态的线程，如果有的话，就乖乖加入到等待线程中去排队，而不能直接插队获取锁。 非公平锁：在调用lock()中的第一次CAS 失败后，调用的是nonfairTryAcquire()非公平方法，如果发现锁这个时候被释放了（state == 0），非公平锁就会直接 CAS 抢锁，不会管当前等待队列中有没有等待线程。但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 可中断锁 可中断锁与不可中断锁的区别在于：线程尝试获取锁操作失败后，在等待过程中，如果该线程被其他线程中断了，它是如何响应中断请求的。lock方法会忽略中断请求，继续获取锁直到成功；而lockInterruptibly则直接抛出中断异常来立即响应中断，由上层调用者处理中断。 lock()适用于锁获取操作不受中断影响的情况，此时可以忽略中断请求正常执行加锁操作，因为该操作仅仅记录了中断状态（通过Thread.currentThread().interrupt()操作，只是恢复了中断状态为true，并没有对中断进行响应)。 如果要求被中断线程不能参与锁的竞争操作，则应该使用lockInterruptibly方法，一旦检测到中断请求，立即返回不再参与锁的竞争并且取消锁获取操作（即finally中的cancelAcquire操作） 可重入锁 可重入是指任意线程在获取到锁之后能够再次获取该锁而不会被锁阻塞，该特性的首先需要解决以下两个问题： 线程再次获取锁：所需要去识别获取锁的线程是否为当前占据锁的线程，如果是，则再次获取成功； 锁的最终释放：线程重复n次获取了锁，随后在第n次释放该锁后，其它线程能够获取到该锁。锁的最终释放要求锁对于获取进行计数自增，计数表示当前线程被重复获取的次数，而被释放时，计数自减，当计数为0时表示锁已经成功释放。 首先会通过compareAndSetState(int, int)方法来尝试修改同步状态，如果修改成功则表示获取到了锁，然后调用setExclusiveOwnerThread(Thread)方法来设置获取到锁的线程。 该方法继承自AbstractOwnableSynchronizer类，它的主要作用就是记录获取到独占锁的线程，AOS类的定义很简单： CountDownLatch CountDownLatch是一个计数器，它允许一个或多个线程等待其它线程完成操作后再继续执行，通常用来实现一个线程等待其它多个线程完成操作之后再继续执行的操作。 CountDownLatch内部维护了一个计数器，该计数器通过CountDownLatch的构造方法指定。当调用await()方法时，它将一直阻塞，直到计数器变为0。当其它线程执行完指定的任务后，可以调用countDown()方法将计数器减一。当计数器减为0，所有的线程将同时被唤醒，然后继续执行。 常用方法 CountDownLatch(int count)：CountDownLatch的构造方法，可通过count参数指定计数次数，但是要大于等于0，小于0会抛IIegalArgumentException异常。 void await()：如果计数器不等于0，会一直阻塞（在线程没被打断的情况下）。 boolean await(long timeout,TimeUnit unit)：除非线程被中断，否则会一直阻塞，直至计数器减为0或超出指定时间timeout，当计数器为0返回true，当超过指定时间，返回false。 void countDown()：调用一次，计数器就减1，当等于0时，释放所有线程。如果计数器的初始值就是0，那么就当没有用CountDownLatch吧。 long getCount()：返回当前计数器的数量，可以用来测试和调试。 使用实例 定义线程任务，实现Runnable接口 定义测试类，使用for循环执行任务，知道任务结束完毕后打印结果。 执行结果： 可以发现，当所有任务执行完毕后，才执行了测试类后续的打印任务。 但是如果使用构造函数创建了4个计数new CountDownLatch(4)，但实际只有3个线程，则测试类阻塞，无法打印结果。 CyclicBarrier CyclicBarrier是一个同步屏障，它允许多个线程相互等待，直到到达某个公共屏障点，才能继续执行。通常用来实现多个线程在同一个屏障处等待，然后再一起继续执行的操作。 CyclicBarrier也维护了一个类似计数器的变量，通过CyclicBarrier的构造函数指定，需要大于0，否则抛IllegalArgumenException异常。当线程到达屏障位置时，调用await()方法进行阻塞，直到所有线程到达屏障位置时，所有线程才会被释放，而屏障将会被重置为初始值以便下次使用。 常用方法 CyclicBarrier(int parties)：CyclicBarrier的构造方法，可通过parties参数指定需要到达屏障的线程个数，但是要大于0，否则会抛IllegalArgumentException异常。 CyclicBarrier(int parties,Runnable barrierAction)：另一个构造方法，parties作用同上，barrierAction表示最后一个到达屏障点的线程要执行的逻辑。 int await()：表示线程到达屏障点，并等待其它线程到达，返回值表示当前线程在屏障中的位置（第几个到达的）。 int await(long timeout,TimeUnit unit)：与await()类似，但是设置了超时时间，如果超过指定的时间后，仍然还有线程没有到达屏障点，则等待的线程会被唤醒并执行后续操作。 void reset()：重置屏障状态，即将屏障计数器重置为初始值。 int getParties()：获取需要同步的线程数量。 int getNumberWaiting()：获取当前正在等待的线程数量。 使用实例 定义线程执行的任务，当线程执行完打印任务后，阻塞等待其他线程。 定义最终执行的业务逻辑 定义测试类，当所有线程执行到屏障后，触发最终的业务逻辑。 执行结果： 对比CountDownLatch CyclicBarrier维护线程的计数，而CounDownLatch维护任务的计数。 可重用性：两者最明显的差异就是可重用性。CyclicBarrier所有线程都到达屏障后，计数会重置为初始值。而CountDownLatch永远不会重置。 Semaphore Semaphore是一个计数信号量，它允许多个线程同时访问共享资源，并通过计数器来控制访问数量。它通常用来实现一个线程需要等待获取一个许可证才能访问共享资源，或者需要释放一个许可证才能完成的操作。 Semaphore维护了一个内部计数器（许可permits），主要有两个操作，分别对应Semaphore的acquire和release方法。acquire方法用于获取资源，当计数器大于0时，将计数器减1；当计数器等于0时，将线程阻塞。release方法用于释放资源，将计数器加1，并唤醒一个等待中的线程。 常用方法 Semaphore(int permits)：构造方法，permits表示Semaphore中的许可数量，它决定了同时可以访问某个资源的线程数量。 Semaphore(int permits,boolean fair)：构造方法，当fair为ture，设置为公平信号量。 void acquire()：获取一个许可，如果没有许可，则当前线程被阻塞，直到有许可。如果有许可该方法会将许可数量减1。 void acquire(int permits)：获取指定数量的许可，获取成功同样将许可减去指定数量，失败阻塞。 void release()：释放一个许可，将许可数加1。如果有其他线程正在等待许可，则唤醒其中一个线程。 void release(int n)：释放n个许可。 int availablePermits()：当前可用许可数。 使用实例 信号量的构造方法传入参数为5，设置六个进程获取这5个资源。 返回结果： 可以发现同一时刻只有五个线程获取到资源，当有资源释放时（车牌号5 驶出），其他线程才能获取资源（车牌号6 进入）。 "},{"title":"【设计模式】单例模式详解","date":"2024-08-21T11:27:02.000Z","url":"/2024/08/21/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["单例模式","/tags/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 单例模式是一种创建型设计模式，它确保一个类在内存中只有一个实例，并提供全局访问点来共享该实例。在程序中，如果需要频繁使用同一个对象且它们的作用相同，单例模式可以避免频繁创建对象，减少内存开销。 单例模式主要有两种类型： 懒汉式：在需要使用时才去创建该对象的单例 饿汉式：在类加载时就已经创建好该单例对象，等待被程序使用 2.懒汉式 懒汉式创建对象的方法是在程序使用对象前，判断对象是否为空（是否被实例化），如果为空则创建该对象的实例，如果不为空直接返回该对象的实例即可。 非线程安全 使用 static 关键字修饰对象，表示该对象属于类本身，而不是类的某个实例。在应用程序的整个生命周期中，该类只会存在一个这样的实例。当多线程访问该类时，可以通过类获取这个静态对象，从而避免重复创建新对象。 线程安全 上述方法在多线程创建对象的时候是有问题的。如果两个线程同时判断singleton为空，那么它们都会执行singleton = new Singleton();语句创建一个实例，这就变成双例了。 解决方法也很简单，给获取实例的方法getInstance()加锁即可。 但是这种方法也存在问题：每次去获取对象都需要获取互斥锁，在高并发场景下，可能会多线程阻塞的问题。 优化方案是：如果没有实例化对象则加锁创建，如果已经实例化了，则不需要加锁，直接获取实例。 双重校验锁 在单例模式中，双重校验锁可以确保只有在第一次访问单例对象时才会进行实例化，并且在多线程环境下能够保证单例对象的唯一性。 详细解释下这段代码的执行流程： 多个线程执行getInstance()方法后，进入if (singleton == null)语句块，判断类是否已经被实例化了，如果已经被实例化，直接返回对象，无需继续创建对象。 如果 singleton 为 null，多个线程争抢该锁进行实例化，同一时刻只有一个线程可以进入同步代码块去创建对象。 再次判断该类是否被实例化，这一步确保了上一个线程在进入同步块后创建了对象，当前线程也不会重复创建实例。 防止指令重排 尽管我们使用双重校验锁的方案保证了线程安全并提升了性能，但由于JVM指令重排序的存在，在多线程创建单例时仍然可能存在问题。 当JVM执行new Singleton()创建一个对象时，会经过三个步骤： 为singleton分配内存空间。 初始化singleton对象，将对象的成员变量赋值。 将内存空间的引用赋值给 singleton 变量。 当发生指令重排序时，这个顺序可能会发生变化，导致未完成初始化（即对象的构造方法可能尚未执行完毕，某些属性可能未被赋值）的对象被其他线程访问，如果这些线程使用了该对象的成员变量，就会导致NPE。 具体场景： 当线程A进入同步块并执行singleton= new Singleton()时，由于指令重排，创建对象的顺序变成： 为singleton分配内存空间。 将内存空间的引用赋值给 singleton 变量。 初始化singleton对象，将对象的成员变量赋值。 这时候线程B在第一次检查singleton == null时得到的结果是false，因为instance已经不再是null（已经指向了内存地址），于是直接返回了这个未初始化的半成品对象。 由于singleton指向的对象尚完成初始化，当线程B在访问该对象尚未初始化的成员变量时，就会导致空指针异常NullPointerException。 解决方案： 只需加上volatile关键字即可，使用volatile修饰变量，可以保证其指令执行的顺序与程序指明的顺序一致，不会发生顺序变换。 完整代码如下： 3.饿汉式 饿汉式在类加载时就已经创建好单例对象，因此在程序调用时可以直接返回该对象。也就是说，我们在调用时直接获取单例对象即可，而不需要在调用时再去创建实例。 由于饿汉式在类加载过程中就由JVM创建了单例，因此不存在线程安全问题。 4.破坏单例模式 无论是使用了双重校验锁+volatile关键字的懒汉式还是饿汉式，都无法防止反射和序列化破坏单例模式（创建多个对象），具体演示代码如下： 反射破坏单例模式 这个原理很简单，利用反射可以强制访问类的私有构造器，从而创建另一个对象 序列化破坏单例模式 首先使用将单例对象序列化为文件流或其他形式，再使用反序列化的手段从流中读取对象。 当使用readObject()方法时，一定会创建一个新的对象。 5.枚举类 枚举类不仅能确保线程安全性，还可以防止反序列化破坏和反射攻击。 枚举类的优点： 线程安全：枚举类的实例在 JVM 加载时由 JVM 保证，只会实例化一次，因此枚举的单例实现天然是线程安全的。 防止反序列化破坏：默认情况下，反序列化一个枚举类型时，会通过类加载器加载枚举类，从而保证每个枚举类型在 JVM 中仅存在一个实例。因此枚举类型可以防止反序列化导致的单例破坏。 防止反射攻击：反射攻击是指通过反射机制来调用私有构造器从而创建多个实例的情况，但在枚举类型中，这种操作会抛出 IllegalArgumentException，从而有效地防止了反射攻击。 6.对比 懒汉式 实例化时机：在第一次调用getInstance()方法时，才会创建单例实例。这意味着如果程序一直没有调用这个方法，单例实例将永远不会被创建。 优点：延迟加载（Lazy Loading），只有在需要时才创建实例，节省了系统资源。 缺点：首次创建实例时可能会有性能开销。在多线程环境下，如果没有适当的同步机制，可能会导致线程安全问题，如重复创建实例。 饿汉式 实例化时机：在类加载时就创建单例实例。无论是否调用getInstance()方法，类加载时都会创建实例。 优点：实现简单，类加载时就完成了实例化，避免了多线程同步问题，因为JVM在类加载时确保了线程的安全性。 缺点：即使单例实例从未被使用，也会在程序启动时创建，可能会浪费系统资源，尤其是在实例化过程较重或单例实例不常用的情况下。 枚举类 实例化时机：在枚举类被加载时创建单例实例，与饿汉式类似，枚举类型的实例会在类加载时被创建，并且JVM会确保只有一个实例存在。 优点： 线程安全：枚举类型在Java中是天然线程安全的，JVM保证了枚举实例的唯一性，无需额外的同步控制。 防止反序列化破坏单例：枚举类在反序列化时不会创建新的实例，保证了单例的唯一性。 防止反射攻击：枚举类在Java中不允许通过反射创建实例，因此可以防止反射攻击破坏单例。 缺点：如果单例类需要继承其他类或实现某些接口，枚举类不太适合。 "},{"title":"【Redis集群】集群原理最全解析","date":"2024-08-12T03:00:01.000Z","url":"/2024/08/12/%E3%80%90Redis%E9%9B%86%E7%BE%A4%E3%80%91%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%E9%9B%86%E7%BE%A4%E5%8E%9F%E7%90%86/","tags":[["主从集群","/tags/%E4%B8%BB%E4%BB%8E%E9%9B%86%E7%BE%A4/"],["哨兵机制","/tags/%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/"],["分片集群","/tags/%E5%88%86%E7%89%87%E9%9B%86%E7%BE%A4/"]],"categories":[["Redis","/categories/Redis/"]],"content":"主从集群 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。 数据同步概念 Replication Id和offset 在从节点发起数据同步的请求中，有两个重要的属性： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid。 offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id和offset，master才可以判断到底需要同步哪些数据。 当从节点发起主从同步请求时，主节点会判断从节点的replid是否一致，如果不一致，说明是第一次请求数据同步。 repl_baklog缓冲区 repl_baklog缓冲区是主从同步的重要机制，主节点在生成RDB文件期间会将命令记录到这个环形缓冲区中。 该缓冲区用于增量同步，确保从节点的偏移量与主节点保持一致。然而，如果从节点宕机且重启时缓冲区的数据已被覆盖，从节点就无法通过缓冲区恢复全部数据，导致数据不一致。 第一次数据同步 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点。由于生成RDB文件的过程是异步的，主节点同时会持续记录在生成RDB文件期间产生的所有命令。 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。加载完成后，从节点获取缓存区的命令，执行命令同步数据。 全量同步 全量同步：是指主节点生成RDB文件并发送给从节点，从节点清空本地数据并加载RDB文件的过程。它通常发生在从节点首次连接到主节点或数据不一致的情况下。 全量同步的发生场景有两种： 从节点首次连接主节点： 当一个新的从节点第一次连接到主节点时，它没有任何数据副本。因此，需要进行全量同步来获取主节点的完整数据集。全量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。 主节点缓冲区超出容量： 主节点的repl_baklog缓冲区大小有上限，写满后会覆盖最早的数据。如果从节点断开时间过久，导致尚未备份的数据被覆盖，则主节点不能基于缓冲区做数据同步，只能再次使用全量同步获取RDB的完整数据集。 全量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果不一致，说明从节点是第一次申请数据同步，主节点需要生成RDB文件并将RDB文件发送给从节点 从节点获取RDB文件后，清空本地数据并加载RDB文件进行数据同步。 增量同步 增量同步：是指主节点在RDB文件生成期间记录的所有命令（写操作）被存储在replication backlog缓冲区中，并在全量同步完成后发送给从节点，从节点执行这些命令的过程。增量同步用于保持主从节点之间的数据一致性。 增量同步的发生场景有两种： 全量同步后的持续增量同步： 在从节点完成初次的全量同步之后，主节点和从节点之间需要保持数据一致性。从节点会不断接收主节点的增量数据以更新其自身的数据状态。 从节点宕机重启后的增量同步： 当从节点因为故障、宕机或其他原因暂时失联，然后重新启动并重新连接到主节点时，主节点会尝试通过增量同步来恢复数据同步的状态。 增量同步过程： 从节点发起数据同步请求，请求中携带replid和offset两个属性。 主节点需要判断replid是否与自己一致，如果一致，说明从节点不是第一次申请数据同步了（即从节点之前进行了全量同步），主节点返回continue，允许从节点获取repl_baklog缓冲区的命令 从节点持续获取缓存区的命令，执行命令同步数据。 优化策略 可以从以下几个方面来优化Redis主从集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘I/O Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘I/O 适当提高repl_baklog缓存区的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 哨兵机制 在主从集群中，slave节点即使宕机，也可以从master节点上恢复数据。然而，如果master节点宕机，即使master节点做了持久化处理，在其重启后虽然能够恢复部分数据，但在重启和故障恢复的过程中，仍然可能会丢失大量数据，这对系统来说是不可接受的。 因此，为了解决上述问题，在主从集群的基础上引入了哨兵机制。哨兵机制的核心作用是监控主从集群中的各个节点，并在检测到master节点宕机时，自动从slave节点中选举一个新的master节点。 哨兵（Sentinel）机制的作用： 服务状态监控： Sentinel会不断检查集群中的master和slave节点是否按预期工作 自动故障恢复： 如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知Redis客户端： Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 服务状态监控 Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： 主观下线： 如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 客观下线： 若超过指定数量(quorum)的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 自动故障恢复 选举新master节点 当sentinel检测到master节点客观下线时，需要在集群中选择一个slave节点作为新的master，选择依据是这样的： 节点断开时间长短：首先会判断slave节点与master节点断开时间长短，如果超过指定值down-after-milliseconds * 10则会排除该slave节点 优先级判断：slave从节点有slave-priority参数，越小优先级越高，如果是0则永不参与选举 数据同步状态：如果从节点优先级一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 节点 ID 大小：最后是判断slave节点的运行id大小，越小优先级越高， 进行故障转移 当节点2（master节点）故障后，sentinel选举节点1为新的master节点，故障转移步骤如下: sentinel给备选的节点1发送slaveof no one命令，让节点1成为master sentinel给其它所有的slave节点发送slaveof 192.168.150.101 7002命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当节点2恢复后会自动成为新的master的slave节点 通知Redis客户端 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。 在pom文件中引入redis的starter依赖 然后在配置文件application.yml中指定sentinel相关信息 配置读写分离 这里的ReadFrom是配置Redis的读取策略，是一个枚举，包括下面选择: MASTER： 从主节点读取 MASTER_PREFERRED： 优先从master节点读取，master不可用才读取replica REPLICA： 从slave(replica)节点读取 REPLICA_PREFERRED： 优先从slave(replica)节点读取，所有的slave都不可用才读取master 分片集群 主从复制和哨兵机制虽然解决了Redis的高可用性和高并发读的问题，但仍然面临以下两个挑战： 海量数据存储的问题：单个Redis实例的内存和存储容量有限，无法处理海量数据。 高并发写入的问题：单个主节点在高并发写入的场景下容易成为性能瓶颈。 Redis中的分片集群（Sharded Cluster）是一种将数据分布在多个Redis节点上的方式。通过将数据水平分片，分片集群能够在数据量增加时提升集群的存储容量，同时将写入压力分散到多个master节点上，提升整体性能。 Redis 分片集群的核心作用： 数据水平扩展： 通过将数据分片存储在多个节点上，Redis 集群能够扩展到多个实例，以应对大规模数据存储和高并发请求。 负载均衡： 将请求均匀分布到不同的分片节点上，避免单点压力过大，确保系统性能的稳定性。 高可用性： 通过主从复制和自动故障恢复机制，Redis 集群能够在某个节点发生故障时，继续提供服务，确保系统的高可用性。 重要概念 散列插槽 Redis 集群通过哈希槽（Hash Slot）机制来分配数据到不同的分片节点上。整个哈希空间分为 16384 个槽，每个键根据其哈希值被分配到一个特定的槽中，而槽则由集群中的各个matser节点持有。 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： 如果key中包含&#123;&#125;，且&#123;&#125;中至少包含1个字符，&#123;&#125;中的部分是有效部分 如果key中不包含&#123;&#125;，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{modox}num，则根据modox计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是插槽的slot值。 Redis客户端如何进行数据访问？ 根据键的哈希值确定数据所在的分片节点，然后直接与该节点通信。 如何将同一类数据保存在同一个Redis节点上？ 只需设置一个统一的有效部分，如{shopId} 配置纪元 配置纪元的作用是标识和跟踪集群配置的版本，确保集群中的所有节点在主节点故障转移和配置变更时保持一致。 1.配置纪元是只增不减的整数： 每个主节点都有一个自身维护的配置纪元 (clusterNode.configEpoch)，表示该主节点的版本。这个配置纪元是集群变更时用于标识和协调的关键因素。 每个主节点的配置纪元都不同，以确保集群内的节点可以正确识别和处理最新的配置变更。 2.从节点会复制主节点的配置纪元： 当从节点与其对应的主节点同步时，它会复制该主节点的配置纪元。这样在主节点发生故障时，从节点可以使用这个配置纪元参与选举并成为新的主节点。 3.全局配置纪元： 整个集群维护一个全局的配置纪元 (clusterState.currentEpoch)，记录集群内所有主节点的配置纪元中的最大版本号。这个全局纪元会在集群发生关键事件（如故障转移、添加/删除节点）时增加，以确保集群状态的一致性。 4.选举时选择纪元数最大的从节点： 在故障转移过程中，集群会优先选择配置纪元最大的从节点作为新的主节点。因为这个从节点的数据更可能是最新的，并且它在选举中更有可能获得其他主节点的支持。 服务状态监控 Redis分片集群的各个节点通过ping/pong进行消息通信，转播槽的信息和节点状态信息，故障发现也是通过这个动作实现的，类似于sentinel，有主观下线和客观下线。 主观下线（PFAIL）： 集群中的每个节点都会定期通过 PING-PONG 消息与其他节点通信。如果一个节点在指定时间内没有响应其他节点的 PING 请求，该节点会被标记为主观下线。 客观下线（FAIL）： 如果多个节点都将同一个节点标记为 PFAIL，那么通过投票机制，该节点将被标记为客观下线（FAIL）。这个状态会在集群中广播，所有节点都认同该节点已不可用。 故障恢复 选举新的master节点 Redis 分片集群和 Sentinel 机制在选举新的 master 节点时规则基本相同，唯一的区别在于节点断开时间的处理方式不同。 节点断开时间长短：每个从节点检查与故障主节点的断线时间，断开时间超过cluster-node-timeout * cluster-slave-validity-factor则取消资格。cluster-slave-validity-factor : 默认是10 优先级判断：slave从节点有slave-priority参数，越小优先级越高，如果是0则永不参与选举 数据同步状态：如果从节点优先级一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 配置纪元： 在故障转移过程中，集群会优先选择配置纪元最大的从节点作为新的主节点。因为配置纪元越大的从节点，数据更可能是新的。 进行故障转移 当新的 master 节点选举完成后，Redis 集群会自动进行故障转移，具体包括以下步骤： 提升新的 master 节点：Redis 集群通过内部命令 SLAVEOF NO ONE 将选中的从节点提升为新的 master 节点。 更新哈希槽映射：Redis 集群会自动更新哈希槽与节点的映射关系，新的 master 节点将执行 CLUSTER DELSLOTS 操作撤销故障主节点负责的槽，并执行 CLUSTER ADDSLOTS 把这些槽委派给自己。 重新配置和广播：Redis 集群将剩余的从节点重新配置为新 master 节点的从节点，并广播新的 master 信息给所有节点，确保集群内所有节点都更新哈希槽映射，并将新 master 的信息同步到其他节点。 节点重连：如果故障的 master 节点恢复上线，它通常会被重新配置为新的 master 的从节点，并同步数据以确保与新 master 保持数据一致性。 通知Redis客户端 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致。 引入redis的starter依赖 配置分片集群地址 配置读写分离 与哨兵模式相比，只有yaml配置文件的配置方式存在差异，如下： 集群伸缩 Redis 集群提供了灵活的节点扩容和收缩方案。在不影响集群对外服务的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容，对节点进行灵活上下线控制，原理可抽象为槽和对应数据在不同节点之间灵活移动。 集群扩容 1.添加节点： Redis分片集群提供了为现有集群添加新节点的，命令如下： 如果需要直接指定新节点为某一master的从节点，可使用如下命令： 2.迁移插槽： 可通过reshard命令将当前节点的散列插槽分配给其他节点。 接着Redis会提示需要移动多少插槽，自行输入即可。 然后需要输入接收插槽的节点ID，确认后即可实现插槽的迁移 3.添加从节点： 由于新的master节点相比其他主节点目前还没有从节点，因此该节点不具备故障转移的能力。 可以在从节点下使用cluster replicate命令为主节点添加对应从节点（在分片集群下slaveof命令添加从节点的操作不再支持）。 从节点内部除了对主节点发起全量复制之外，还需要更新本地节点的集群相关状态。 集群缩容 1.迁移插槽： 缩容操作需要非常谨慎，因为如果下线的节点持有插槽，直接删除可能会引起数据一致性问题，因此需要将槽迁移给其他节点后才能安全下线，流程同上。 接着Redis会提示需要移动多少插槽，自行输入即可。 然后需要输入接收插槽的节点ID，确认后即可实现插槽的迁移 2.忘记节点： 在一个可用的节点上执行删除节点的命令： 手动故障转移 在 Redis 集群中，手动故障转移允许管理员主动介入，以便在发现主节点故障时，迅速将其替换为副本节点，确保系统的持续可用性和稳定性。 此外，手动数据迁移还支持三种不同模式： 缺省： 默认的流程，如图1~6步 force： 省略了对offset的一致性校验 takeover： 直接执行第5步，忽略数据一致性、忽略master状态和其它master的意见 "},{"title":"【JVM】深入JIT优化机制","date":"2024-08-08T12:31:45.000Z","url":"/2024/08/08/%E3%80%90JVM%E3%80%91%E6%B7%B1%E5%85%A5JIT%E4%BC%98%E5%8C%96%E6%9C%BA%E5%88%B6/","tags":[["JIT","/tags/JIT/"]],"categories":[["JVM","/categories/JVM/"]],"content":"1.JIT优化技术 在将高级语言转化为计算机可识别的机器语言时，常用的两种方式是编译和解释。Java在编译过程中，首先将代码编译成字节码。但是，字节码并不能直接在机器上执行。因此，JVM中内置了解释器（Interpreter），它在运行时将字节码逐行翻译成机器码并执行。 然而，解释器的执行方式是一边翻译，一边执行，导致执行效率较低。为了提高效率，HotSpot JVM引入了JIT（Just-In-Time）编译技术。 有了JIT技术后，JVM仍然通过解释器进行初始执行。但当JVM发现某个方法或代码块被频繁执行时，它将其标记为“热点代码”（Hot Spot Code）。JIT随后将这些热点代码编译为机器码，并进行优化。优化后的机器码被缓存起来，以便下次直接使用，从而显著提升执行效率。 2.热点检测 上面我们说过，要想触发JIT，首先需要识别出热点代码。目前主要的热点代码识别方式是热点探测，有以下两种 基于采样的方式探测： 周期性检测各个线程的栈顶，发现某个方法经常出现在栈顶，就认为是热点方法。好处就是简单，缺点就是无法精确确认一个方法的热度。容易受线程阻塞或别的原因千扰热点探测。 基于计数器的热点探测： 采用这种方法的虚拟机会为每个方法，甚至是代码块建立计数器，统计方法的执行次数，某个方法超过阀值就认为是热点方法，触发JIT编译。 在HotSpot虚拟机中使用的是第二种一一基于计数器的热点探测方法，因此它为每个方法准备了两个计数器: 方法调用计数器和回边计数器。 方法计数器。顾名思义，就是记录一个方法被调用次数的计数器 回边计数器。是记录方法中的for或者while的运行次数的计数器 3.编译优化 逃逸分析 全局逃逸：对象超出了方法或线程的范围，比如被存储在静态字段或作为方法的返回值。 如我们新建的staticObject就是全局逃逸的。以及下面的方法中的sb对象，也是全局逃逸的。 参数逃逸： 对象被作为参数传递或被参数引用，但在方法调用期间不会全局逃逸。 如传递到methodB中的param对象，就是发生了参数逃逸的。因为他从methodA中逃逸到了methodB中 无逃逸： 对象可以被标量替换，意味着它的内存分配可以从生成的代码中移除。 如上面的sb，就没有发生逃逸，因为这个对象本身没有作为参数传递，也没有被当做方法返回值，并没有赋值给静态变量。 在Java中，不同的逃逸状态影响JIT (即时编译器)的优化策略： 全局逃逸： 由于对象可能被多个线程访问，全局逃逸的对象一般不适合进行栈上分配或其他内存优化。但JIT可能会进行其他类型的优化，如方法内联或循环优化。 参数逃逸： 这种情况下，对象虽然作为参数传递，但不会被方法外部的代码使用。JIT可以对这些对象进行一些优化，例如锁消除。 无逃逸： 这是最适合优化的情况。JIT可以采取多种优化措施，如在栈上分配内存，消除锁甚至完全消除对象分配 (标量替换)。这些优化可以显著提高性能，减少垃圾收集的压力。 方法内联 方法内联是Java中的一个优化技术，即时编译器JIT用它来提高程序的运行效率。在Java中，方法内联意味着将一个方法的代码直接插入到调用它的地方，从而避免了方法调用的开销。这种优化对于小型且频繁调用的方法特别有用。 锁消除 锁消除是 JIT 编译器在编译期间通过分析代码的同步块，判断是否存在锁竞争的可能性。如果某个锁在多线程环境下不存在竞争，那么它就可以在生成的机器码中消除这些锁操作，以减少不必要的开销。 栈上分配 栈上分配的好处： 减少GC压力：对象分配在栈上，当方法执行完毕后，栈上的内存会自动释放，不需要垃圾回收（GC）来管理，从而减少了GC的压力。 提高性能：栈上的内存分配和释放非常高效，因为它只是对栈指针进行简单的移动操作，而堆上的内存管理相对复杂，需要垃圾回收器的参与。 Java中的对象一定在堆上分配内存吗? 不一定，在HotSpot虚拟机中，存在JIT优化的机制，JIT优化中可能会进行逃逸分析，当经过逃逸分析发现某个对象不会逃逸出当前方法（即它只在方法内部使用），那么这个对象就不会被分配到堆上，而是进行栈上分配。 标量替换 标量是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量，Java中的对象就是聚合量，因为他可以分解成其他聚合量和标量。 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。"},{"title":"【JVM】Java内存区域图文详解","date":"2024-08-07T15:41:59.000Z","url":"/2024/08/07/%E3%80%90JVM%E3%80%91Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3/","tags":[["Java内存区域","/tags/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/"]],"categories":[["JVM","/categories/JVM/"]],"content":"1.JVM运行时区域总览 Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。 JVM运行时区域也成为Java内存区域。 在讨论Java内存模型时，通常将其分为线程共享区域和线程私有区域： 2.线程私有区域 2.1.程序计数器 程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完成。 程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 ⚠️ 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2.Java虚拟机栈 JVM栈可以说是 JVM 运行时数据区域的一个核心，除了一些 Native 方法调用是通过本地方法栈实现的，其他所有的 Java 方法调用都是通过JVM栈来实现的（也需要和其他运行时数据区域比如程序计数器配合）。 从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 局部变量表 局部变量表 主要存放了编译期可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference 类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置） 操作数栈 操作数栈 主要作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。另外，计算过程中产生的临时变量也会放在操作数栈中。 动态链接 动态链接 主要服务一个方法需要调用其他方法的场景。Class 文件的常量池里保存有大量的符号引用，比如方法引用的符号引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用，这个过程也被称为 动态连接 。 方法返回地址 方法返回地址是当前方法执行完成后，线程应该跳转到的下一条指令的地址。这个返回地址实际上是保存在栈帧中的一个特殊位置，用于在方法执行完毕后恢复程序的执行流程。 2.3.本地方法栈 和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种错误。 3.线程共享区域 3.1.堆 Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作 GC 堆（Garbage Collected Heap）。从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代；再细致一点有：Eden、Survivor、Old 等空间。进一步划分的目的是更好地回收内存，或者更快地分配内存。 JDK 8 版本之前，堆内存被通常分为下面三部分： 新生代(Young Generation) 老生代(Old Generation) 永久代(Permanent Generation) JDK 8 版本之后 PermGen(永久代) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存。 下图所示的 Eden 区、两个 Survivor 区都属于新生代，中间一层属于老年代，最下面一层属于元空间。 字符串常量池 字符串常量池 是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 JDK1.7 之前，字符串常量池存放在永久代，JDK1.7之后字符串常量池、静态变量和常量从永久代移动了 Java 堆中。 JDK 1.7 为什么要将字符串常量池移动到堆中？ 主要是因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。 3.2.方法区 方法区是JVM规范定义的一块用于存储类的元数据、常量、静态变量、即时编译器(JIT编译器)编译后的代码等数据的内存区域。 当虚拟机要使用一个类时，它需要读取并解析 Class 文件获取相关信息，再将信息存入到方法区。 方法区会存储已被虚拟机加载的类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。 方法区和永久代以及元空间是什么关系呢？ 方法区和永久代以及元空间的关系很像 Java 中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口可以看作是方法区，也就是说永久代以及元空间是 HotSpot 虚拟机对虚拟机规范中方法区的两种实现方式。并且，永久代是 JDK 1.8 之前的方法区实现，JDK 1.8 及以后方法区的实现变成了元空间。 为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢? 1、永久代有 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。 当元空间溢出时会得到如下错误：java.lang.OutOfMemoryError: MetaSpace 你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义初始元空间大小，如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 2、元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了。 在JDK 1.8中，运行时常量池和字符串常量池逻辑上属于方法区，但是实际存放在堆内存中。 运行时常量池 JVM的运行时常量池是方法区的一部分，它主要用于存放编译期生成的各种字面量和符号引用。这些字面量和符号引用是在类加载的过程中，从Class文件的常量池中加载到方法区的运行时常量池中的。 字面量是源代码中的固定值的表示法，即通过字面我们就能知道其值的含义。字面量包括整数、浮点数和字符串字面量。常见的符号引用包括类符号引用、字段符号引用、方法符号引用、接口方法符号。 常量池会在类加载后存放到方法区的运行时常量池中。 运行时常量池的功能类似于传统编程语言的符号表，尽管它包含了比典型符号表更广泛的数据。 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 错误。 Class常量池： 可以理解为是Class文件中的资源仓库。 Class文件中除了包含类的版本、字段、方法、接口等描述信息外，还有就是常量池(constant pool table)，用于存放编译器生成的各种字面量和符号引用。 Class是用来保存常量的一个媒介场所，并且是一个中间场所。Class文件中的常量池部分的内容，会在运行期被运行时常量池加载进去。 "},{"title":"【Redis】持久化机制最全解析","date":"2024-08-07T03:57:33.000Z","url":"/2024/08/07/%E3%80%90Redis%E3%80%91%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E6%9C%80%E5%85%A8%E8%A7%A3%E6%9E%90/","tags":[["RDB","/tags/RDB/"],["AOF","/tags/AOF/"]],"categories":[["Redis","/categories/Redis/"]],"content":"RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件）。通过将Redis数据集的快照保存到磁盘上的二进制文件中来实现。生成 RDB 文件的过程可以通过手动命令或自动触发。 实现原理 开始 BGSAVE： Redis 主进程接收到 BGSAVE 命令后，调用 fork 创建一个子进程，子进程会先复制主进程的页表（记录虚拟地址与物理地址的映射关系），而不是立即复制实际的内存数据。 在 fork 完成后，父子进程共享相同的内存页，子进程负责生成 RDB 文件，主进程可以继续处理客户端请求。 生成 RDB 文件： 子进程根据页表，扫描 Redis 数据集，将每个键值对序列化为二进制格式，并写入到临时 RDB 文件中。 序列化后的数据被写入到一个临时 RDB 文件中，以避免影响现有 RDB 文件的使用。 写时复制（Copy-On-Write, COW）： 在生成 RDB 文件的过程中，如果主进程执行了写操作（如插入、更新、删除数据），操作系统会触发写时复制机制。COW 机制会为被修改的内存页创建一个副本，并将写操作应用到这个副本上，而不是直接修改共享的内存页。 主进程在读取被修改的数据时，会从生成的副本中读取，而子进程则继续读取原始的内存页。这种机制确保了子进程在生成 RDB 文件时看到的是一致且稳定的数据快照，而主进程可以不受影响地继续处理新的写操作。 完成 RDB 文件： 子进程生成 RDB 文件后，将临时 RDB 文件重命名为正式的 RDB 文件，确保文件替换过程是原子的。 手动启用 Redis 提供了以下两个命令来手动生成 RDB 快照文件： 这里说 Redis 主线程而不是主进程的主要是因为 Redis 启动之后主要是通过单线程的方式完成主要的工作。如果你想将其描述为 Redis 主进程，也没毛病。 定时启用 可以通过配置文件中的 save 选项进行定时触发RDB持久化。 AOF持久化 AOF全称为Append Only File（追加文件）。通过将每次写操作记录到AOF文件中来实现。这种方式的特点是将 Redis 接收到的每个写命令都追加到文件末尾。 实现原理 AOF 持久化功能的实现可以简单分为 5 步： 命令追加（append）：所有的写命令会追加到 AOF 缓冲区中。 文件写入（write）：将 AOF 缓冲区的数据写入到 AOF 文件中。这一步需要调用write函数（系统调用），write将数据写入到了系统内核缓冲区之后直接返回了（延迟写）。 文件同步（fsync）：AOF 缓冲区根据对应的持久化方式（ fsync 策略）向硬盘做同步操作。这一步需要调用 fsync 函数（系统调用）， fsync 针对单个文件操作，对其进行强制硬盘同步，fsync 将阻塞直到写入磁盘完成后返回，保证了数据持久化。 文件重写（rewrite）：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。 重启加载（load）：当 Redis 重启时，可以加载 AOF 文件进行数据恢复。 启用AOF 与快照持久化相比，AOF 持久化的实时性更好。默认情况下 Redis 没有开启 AOF（Redis 6.0 之后默认开启），可以通过 appendonly 参数开启： 刷盘策略 开启 AOF 持久化后，每执行一条会更改 Redis 中数据的命令，Redis 就会将该命令写入到 AOF 缓冲区 server.aof_buf 中，然后再写入到系统内核缓冲区中，最后根据刷盘策略的配置来决定何时将系统内核缓冲区中的数据同步到硬盘中。 AOF的命令记录的频率也可以通过redis.conf文件来配置。 配置项 刷盘时机 优点 缺点 always 同步刷盘 可靠性高，几乎不丢数据 性能影响大 everysec 每秒刷盘 性能适中 最多丢失1秒数据 no 操作系统控制 性能最好 可靠性较差，可能丢失大量数据 文件重写 随着时间的推移，AOF 文件会因为不断追加写命令而变得越来越大，可能会导致磁盘空间不足和恢复速度变慢。可以通过重新生成一个新的 AOF 文件，将当前的数据集以最少的命令集记录下来，从而缩小文件大小。 AOF 文件重写期间，Redis 还会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。 开启 AOF 重写功能，可以调用 bgrewriteaof 命令手动执行，也可以设置阈值进行自动执行。 auto-aof-rewrite-min-size：如果 AOF 文件大小小于该值，则不会触发 AOF 重写。默认值为 64 MB auto-aof-rewrite-percentage：执行 AOF 重写时，当前 AOF 大小和上一次重写时 AOF 大小的比值。如果当前 AOF 文件大小增加了这个百分比值，将触发 AOF 重写。将此值设置为 0 将禁用自动 AOF 重写。默认值为 100。 AOF校验机制了解吗 AOF 校验机制是 Redis 在启动时对 AOF 文件进行检查，以判断文件是否完整，是否有损坏或者丢失的数据。 这个机制的原理其实非常简单，就是通过使用一种叫做 校验和（checksum） 的数字来验证 AOF 文件。这个校验和是通过对整个 AOF 文件内容进行 CRC64 算法计算得出的数字。如果文件内容发生了变化，那么校验和也会随之改变。因此，Redis 在启动时会比较计算出的校验和与文件末尾保存的校验和（计算的时候会把最后一行保存校验和的内容给忽略掉），从而判断 AOF 文件是否完整。如果发现文件有问题，Redis 就会拒绝启动并提供相应的错误信息。 类似地，RDB 文件也有类似的校验机制来保证 RDB 文件的正确性，这里就不重复进行介绍了。 如何选择RDB和AOF RDB（Redis Database File） 优点： 持久化速度快：RDB 文件在持久化时会生成一个数据快照，可以快速地恢复大规模的数据集。 文件较小：RDB 文件是二进制格式的，通常比 AOF 文件更小，占用的磁盘空间较少。 恢复速度快：RDB 文件在恢复数据时，加载速度通常比 AOF 快，因为 RDB 文件是一个完整的数据快照，不需要逐条回放写操作。 对性能影响小：RDB 持久化是通过子进程完成的，主进程可以继续处理客户端请求，因此对 Redis 性能影响较小。 缺点： 数据丢失风险高：因为 RDB 持久化是定期执行的（如每隔几分钟或根据配置的触发条件），在上一次持久化到下一次持久化之间的数据可能会丢失。 生成过程消耗资源：生成 RDB 文件需要 fork 子进程并占用一定的 CPU 和内存资源，特别是数据量大时会影响性能。 AOF（Append-Only File） 优点： 数据丢失风险低：AOF 通过追加日志记录每次写操作，可以更频繁地持久化数据（甚至可以做到每秒持久化一次），因此数据丢失的风险较低。 可调的同步策略：AOF 提供了多种同步策略（如 always、everysec 和 no），可以根据需要在性能和数据安全性之间做权衡。 更人性化：AOF 文件是可读的日志文件，方便人类阅读和分析，有助于调试和故障排查。 缺点： 文件较大：AOF 文件记录了每个写操作，文件大小通常比 RDB 大很多，占用更多的磁盘空间。 恢复速度慢：AOF 恢复数据时需要回放所有的写操作日志，恢复速度通常比 RDB 慢。 性能开销较大：因为 AOF 需要在每次写操作后追加日志，频繁的磁盘 I/O 操作会带来一定的性能开销，特别是在同步策略设置为 always 时。 综合比较 数据安全性：AOF 提供更高的数据安全性，适用于对数据丢失敏感的场景；RDB 在数据持久化频率较低时有较高的数据丢失风险。 性能：RDB 对 Redis 性能影响较小，适用于性能要求高的场景；AOF 因为频繁的磁盘 I/O 操作，对性能有一定的影响。 恢复速度：RDB 恢复速度更快，适用于需要快速恢复大规模数据集的场景；AOF 需要回放日志，恢复速度较慢。 磁盘空间：RDB 文件较小，节省磁盘空间；AOF 文件较大，占用更多磁盘空间。 混合持久化 Redis 4.0 引入了混合持久化机制，结合了 RDB 和 AOF 的优点，以提高持久化的效率和可靠性。混合持久化在重启恢复数据时使用 RDB 文件的快照来快速加载数据，并且将 AOF 日志应用于此快照以实现更高的数据恢复精度。 工作原理 RDB 快照： Redis 生成 RDB 文件快照，保存整个数据库的二进制数据。 RDB 文件的生成是通过子进程完成的，主进程可以继续处理客户端请求。 AOF 日志： 除了生成 RDB 文件外，Redis 还会将写操作记录到 AOF 日志中。 在混合持久化模式下，AOF 文件的初始部分是一个 RDB 快照，后面紧接着的是增量的 AOF 日志。 持久化过程： 当 Redis 进行持久化操作时，它会首先生成一个 RDB 文件，并将这个文件内容写入 AOF 文件。 在 AOF 文件中，RDB 文件的内容作为初始部分，然后紧跟着追加的 AOF 日志。 数据恢复： 当 Redis 重启时，它会首先加载 AOF 文件中的 RDB 部分（即快照）来快速恢复数据。 然后，它会回放 AOF 文件中 RDB 部分后的增量日志，以确保数据的一致性和完整性。 优缺点 优点： 快速恢复：使用 RDB 部分可以快速加载大部分数据，而不需要逐条回放所有写操作日志，极大地提高了数据恢复的速度。 较高的数据安全性：结合了 RDB 和 AOF 的优点，RDB 部分确保了数据的快速恢复，而 AOF 部分提供了更高的数据持久化频率，降低了数据丢失的风险。 性能优化： 在持久化过程中，RDB 快照的生成是通过子进程完成的，对主进程处理客户端请求的性能影响较小。 AOF 的增量日志记录了自上次快照以来的所有写操作，减少了持久化过程中的 I/O 操作次数，提高了系统性能。 缺点： 配置复杂性：混合持久化的配置比单独使用 RDB 或 AOF 更加复杂，需要合理设置参数以实现最佳性能和数据安全性。 磁盘空间占用：混合持久化模式下，AOF 文件既包含 RDB 快照部分又包含增量日志，可能会占用更多的磁盘空间。 启用混合持久化 要启用混合持久化，可以在 Redis 配置文件 redis.conf 中设置以下参数： "},{"title":"【MySQL】慢sql查询优化","date":"2024-08-04T03:31:24.000Z","url":"/2024/08/04/%E3%80%90MySQL%E3%80%91%E6%85%A2sql%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"],["日志","/tags/%E6%97%A5%E5%BF%97/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"定位慢sql 工具排查慢sql 调试工具：Arthas 运维工具：Skywalking 通过以上工具可以看到哪个接口比较慢，并且可以分析SQL具体的执行时间，定位到哪个sql出了问题。 启用慢查询日志 慢查询日志记录了所有执行时间超过指定参数(long_query_time，单位:秒，默认10秒)的所有SQL语句的日志。 MySQL的慢查询日志默认没有开启，需要在MySQL的配置文件(/etc/my.cnf)中配置如下信息: 配置完成后，重启MySQL服务保证配置生效。 慢查询日志一般的返回结果如下： 需要关注以下内容： Query_time（查询时间）：查询执行的总时间，单位为秒。是关键的指标，用于判断查询的性能。 Lock_time（锁定时间）：表被锁定的时间，单位为秒。可以帮助判断是否存在锁等待问题。 Rows_sent（发送的行数）：查询返回的行数。 Rows_examined（检查的行数）：查询过程中检查的行数，用于判断查询的效率。 分析慢sql profile详情 SHOW PROFILE 是 MySQL 提供的一种用于查看查询语句执行的详细步骤和资源消耗的工具。使用 SHOW PROFILE 命令可以帮助找出查询语句的瓶颈，优化查询性能。 启用 Profiling 在使用 SHOW PROFILE 之前，需要先启用 Profiling： 执行查询 执行你想分析的查询语句： 查看 Profile 列表 使用以下命令查看刚才执行的查询的 Profile： 这将显示一个查询 ID 列表及其对应的查询语句和总执行时间。 查看详细的 Profile 信息 使用 SHOW PROFILE 查看某个查询 ID 的详细信息： 查看CPU信息 explain执行计划 explain 是 MySQL 提供的一种用于分析和调试 SQL 查询的工具。 通过使用 explain，可以了解 MySQL 在执行查询时采用的具体执行计划，包括访问数据表的方式、使用的索引、连接表的顺序等信息。这些信息对于优化查询性能至关重要。 基本概念 EXPLAIN 执行计划支持 SELECT、DELETE、INSERT、REPLACE 以及 UPDATE 语句。我们一般多用于分析 SELECT 查询语句，要获取一条sql语句的执行计划，只需要在语句前加上explain关键字即可。 执行计划的返回结果一般是这样的： 返回结果中各字段的含义解释如下： 列名 含义 id SELECT 查询的序列标识符 select_type SELECT 关键字对应的查询类型 table 用到的表名 partitions 匹配的分区，对于未分区的表，值为 NULL type 表的访问方法 possible_keys 可能用到的索引 key 实际用到的索引 key_len 所选索引的长度 ref 当使用索引等值查询时，与索引作比较的列或常量 rows 预计要读取的行数 filtered 按表条件过滤后，留存的记录数的百分比 Extra 附加信息 字段释意 id 查询的序列标识符，用于表示查询的执行顺序。值越大，优先级越低，执行顺序越靠后。 select_type 查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询，常见的值有： SIMPLE: 简单查询，不包含子查询或 UNION。 PRIMARY: 最外层的 SELECT 查询。 SUBQUERY: 子查询中的第一个 SELECT。 DERIVED: 派生表（子查询中的 FROM 子句）。 UNION: UNION 操作中的第二个或后续的 SELECT 查询。 UNION RESULT: UNION 的结果集。 table 查询用到的表名。 type（重要） 查询执行的类型，描述了查询是如何执行的。常见的类型如下，这些类型的性能从最优到最差排序为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL system：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。 const：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。 eq_ref：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了 system 与 const 之外最好的 join 方式，常用于使用主键或唯一索引的所有字段作为连表条件。 ref：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。 range：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。 index：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。 ALL：全表扫描。 possible_keys possible_keys 列表示 MySQL 执行查询时可能用到的索引。如果这一列为 NULL ，则表示没有索引可以使用。 key（重要） key 列表示 MySQL 实际使用到的索引。如果为 NULL，则表示未用到索引。 Extra（重要） 这列包含了 MySQL 解析查询的额外信息，通过这些信息，可以更准确的理解 MySQL 到底是如何执行查询的。常见的值如下： Using index：表明查询使用了覆盖索引，不用回表，查询效率非常高。 Using index condition：表示查询优化器选择使用了索引下推这个特性。 Using where：表明查询使用了 WHERE 子句进行条件过滤。一般在没有使用到索引的时候会出现。 Using filesort：在排序时使用了文件排序而不是索引排序，通常是因为无法使用索引进行排序。 Using temporary：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。 Using join buffer (Block Nested Loop)：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。 优化慢sql sql优化方案 根据explain执行计划的返回结果，我们可以根据以下字段进行sql优化： 通过key和key_len检査是否命中了索引（索引本身存在是否有失效的情况） 通过type字段查看sql是否有进一步的优化空间，是否存在全索引扫描或全表扫描 通过extra字段判断，是否出现了回表的情况，如果出现了，可以尝试添加索引或修改返回字段来修复 深分页优化查询 传统分页 传统分页通常使用 OFFSET 和 LIMIT 来实现 这种方法对于小数据集或页数较小时效果较好，但在数据量非常大的情况下，OFFSET 的值越大，数据库需要扫描的行数就越多，性能会急剧下降。 深分页 深分页通过避免使用 OFFSET 来提高性能 1.覆盖索引+子查询： 这种方法通过子查询使用覆盖索引快速定位到分页的起始位置，外部查询从该位置获取实际数据，避免大量数据扫描和回表操作。 如本例中通过子查询定位到了第100001页的起始位置，向后获取100行数据。 这种方法避免了大量数据扫描，适用于有索引列的情况。 2.存储分页结果： 另一种方法是将分页结果存储在缓存（如 Redis）或临时表中，从而避免频繁查询数据库。例如： 这种方法适用于需要多次访问相同分页结果的场景。"},{"title":"【MySQL】全面剖析索引失效、回表查询与索引下推","date":"2024-08-03T08:24:24.000Z","url":"/2024/08/03/%E3%80%90MySQL%E3%80%91%E5%85%A8%E9%9D%A2%E5%89%96%E6%9E%90%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E3%80%81%E5%9B%9E%E8%A1%A8%E6%9F%A5%E8%AF%A2%E4%B8%8E%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.索引失效的情况 以tb_user表举例，id为主键索引、name和phone字段上建立了一个普通索引，name和phone均为varchar类型。 索引列运算 当在 WHERE 子句或 JOIN 子句中对列使用函数或表达式时，索引会失效。 执行以下语句，可以发现执行计划中索引已经生效。 如果我们使用substring函数只取前三个字符，则索引失效。 可以发现type为ALL，key为null，说明本次查询没有执行索引，走的是全表扫描。 隐式类型转换 当列的类型和查询中的值类型不同时，MySQL 可能会进行隐式类型转换，导致索引失效。 执行以下语句，phone为varchar类型，如果等号右侧不加引号，则发生隐式转换，索引失效。 前导通配符查询 使用通配符查询时，如果通配符在字符串的前面，索引会失效。 执行以下语句，查询name字段后缀为ack的数据，索引失效。 or连接条件 当 or 条件中某个列没有索引时，索引会失效 执行以下语句，因为name和phone都是索引字段，索引正常生效。 执行以下语句，因为age字段没有设置索引，所以索引失效查询。 最左匹配原则 对于联合索引（多个列组成的索引），如果查询条件不包含索引的最左前缀部分，索引会失效。 TIPS：这里指的最左是联合索引中的顺序，而不是SQL语句查询条件的顺序。 在本例中，我们新建一个表table，给字段col1、col2、age建立联合索引（col1, col2, age） 遵循最左匹配发展 按照最左前缀法则查询数据。 可以发现，联合索引的总长度为107 不遵循最左匹配法则（查询条件中不包括联合索引的最左前缀部分） 如果不按照最左匹配法则，直接查询col2的数据 本次查询走的是index全索引扫描，性能上要低于ref。 不遵循最左匹配法则（查询条件中包含&gt; &lt;范围查询） 如果查询条件中使用了&gt; &lt;，则不遵循最左匹配法则（可以使用其他范围查询符号），范围查询右侧的索引失效。 执行以下语句，由于age在联合索引（col1, col2, age）中是最后一个，所以不存在其右侧索引失效的情况。 但是如果我们将col2和age调换顺序，改为（col1, age, col2），则col2索引失效。 数据分布情况 MySQL会根据表中数据的分布情况，决定是否使用索引 举一个简单的例子，如果表中的age字段最小值为10，查询条件为age &gt;= 10。则在查询时可能不会走索引，因为走索引和不走索引都需要查询表中的全部数据，不过判断一个语句是否走索引还是要根据explain关键字返回的结果进行判断。 2.回表查询 回表查询是指在使用辅助索引（二级索引）进行查询时，由于辅助索引中不包含查询所需的所有列数据，数据库必须通过索引找到对应的数据行位置，再去实际的数据表（即“回表”）中读取完整的数据行。这种操作会增加额外的 I/O 开销，因此回表查询通常比直接从索引中获取数据的查询更慢。 回表查询示例 假设有以下表数据，id为主键索引，name为普通索引。 主键索引（id）的索引结构如下图，在叶子节点中存储的是每一行的数据。如果我们直接根据id查询，就可以在遍历索引时直接拿到每一行的数据。 辅助索引（name）的索引结构如下，叶子节点存储的是该行的主键（id），如果需要查询该行的数据，则需要遍历索引后获得主键id，再根据这个主键id前往主键索引中查询，这个过程就是回表查询。 避免回表查询 避免回表查询很简单，只需要保证查询的列能够被索引结构覆盖即可。通过创建一个包含所有查询所需列的索引，数据库可以直接从索引中获取所有需要的数据，无需回表。 覆盖索引（Covering Index）是指查询所需的所有列都包含在同一个索引中，从而避免回表操作。这样可以显著提高查询性能。 比如我们直接使用以下语句，就可以避免回表查询，因为name索引中包含了name和id的数据，而无需回到数据库进行查询。 3.索引下推 索引下推（Index Condition Pushdown，ICP）是 MySQL 5.6 及以上版本中引入的一种优化技术，用于提高使用索引查询的效率。ICP 可以减少回表操作（即从索引表跳回数据表读取完整行数据）的次数，从而提高查询性能。 除了可以减少回表次数之外，索引下推还可以减少存储引擎层和 Server 层的数据传输量。 工作原理 在没有索引下推的情况下，MySQL 的查询执行流程通常是： 索引扫描：存储引擎使用索引查找满足索引条件的记录。 返回记录：将这些记录返回给 MySQL 服务器。 行过滤：MySQL 服务器根据剩余的查询条件进一步过滤这些记录。 使用索引下推后，MySQL 优化器会在索引扫描阶段尽可能多地应用查询条件，只有在通过索引扫描无法完全过滤的情况下，才进行回表操作。 适用场景 索引下推在以下场景中尤其有效： 范围查询：对索引列进行范围查询时，例如 BETWEEN、&lt;、&gt; 等。 联合索引查询：在联合索引的前缀列上进行查询，并且查询条件涉及非索引列时。 复杂条件查询：查询条件包含多个过滤条件时，例如 AND、OR 等。 示例 假设有一个包含联合索引 idx_name_age 的表 tb_user： 查询语句： 在没有索引下推的情况下，MySQL 会： 使用索引 idx_name_age 找到 name = 'John' 的所有记录。 回表读取每一条记录的实际数据。 对回表后的数据应用剩余条件 age &gt; 30 和 address LIKE '%Street%' 进行过滤。 在启用索引下推的情况下，MySQL 会： 使用索引 idx_name_age 找到 name = 'John' 且 age &gt; 30 的记录（在索引扫描阶段应用部分条件）。 仅对符合前两个条件的记录进行回表操作。 对回表后的数据应用剩余条件 address LIKE '%Street%' 进行最终过滤。 "},{"title":"【MySQL】一文吃透MVCC执行原理","date":"2024-08-02T08:06:41.000Z","url":"/2024/08/02/%E3%80%90MySQL%E3%80%91%E4%B8%80%E6%96%87%E5%90%83%E9%80%8FMVCC%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86/","tags":[["MVCC","/tags/MVCC/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.MVCC是什么？ MVCC全称Multi-Version Concurrency Control，即多版本并发控制。它通过维护数据的多个版本来实现高效的并发控制，用于在多个并发事务同时读写数据库时保持数据的一致性和隔离性。 在搞清楚MVCC的实现原理之前，还需要了解快照读和当前读的概念。 一致性非锁定读（快照读） 简单的select语句（不加锁）就是快照读，读取的是记录数据的可见版本，不加锁，是非阻塞读。 select ... 一致性锁定读（当前读） 读取的是记录的最新版本，读取时需要保证其他并发事务不能修改当前记录，会对读取的记录加锁。如果执行的是下列语句，就是锁定读。 select ... lock in share mode select ... for update insert、update、delete 操作 2.MVCC实现原理 MVCC 的实现依赖于：隐藏字段、Read View、undo log。 隐藏字段 在内部，InnoDB 存储引擎为每行数据添加了三个隐藏字段 隐藏字段 含义 DB_ROW_ID（6字节） 隐藏主键，如果当前表不存在主键，则将该隐藏字段作为主键 DB_TRX_ID（6字节） 最近修改事务ID，记录插入这条数据或最后一次修改该记录的事务ID DB_ROLL_PTR（7字节） 回滚指针，指向这条记录的上一个版本，用于配合undo log 假设有一个学生表，该表没有指定主键。 那么该表实际上的字段如下，如果存在主键则不存在DB_ROW_ID字段。 undo log undo log 分为两种类型：insert undo log 和 update undo log。 Insert undo log 是在事务进行插入操作时生成的日志。其主要作用是用于事务回滚时撤销插入操作。该日志只在回滚时需要，在事务提交后，可被立即删除。 Update undo log 是在事务进行更新或删除操作时生成的日志。其主要作用是用于事务回滚时撤销更新和删除操作。该日志不仅在回滚时需要，在快照读时也需要，不会被立即删除。 一条记录的每一次更新操作产生的 undo log 格式都有一个一个 DB_TRX_ID事务id 和 DB_ROLL_PTR 指针： 通过 DB_TRX_ID 可以知道该记录是被哪个事务修改的； 通过 DB_ROLL_PTR 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链； 举例说明： 事务1已经提前执行了INSERT INTO user (id, age, name) VALUES (10, 10, 'Jack');语句插入了一条记录。则DB_TRX_ID（插入这条数据或最后一次修改该记录的事务ID）为1，由于insert undo log在事务提交后自动删除，所以不存在undo log日志，DB_ROLL_PTR为null。 该示例中有四个并发事务，其他事务在不同的时刻将执行update语句修改记录。 所有事务执行完毕后，当前记录的DB_TRX_ID为4，且形成了一条Update Undo Log版本链，后续MVCC可以利用这条版本链获取旧数据。 Read View ReadView（读视图）是快照读执行时MVCC获取数据的依据，记录并维护系统尚未提交的事务（也称为活跃事务）id。 ReadView有以下四个重要字段： 字段 含义 m_ids 当前活跃事务的ID集合 min_trx_id 最小活跃事务ID max_trx_id 预分配事务ID，当前最大事务ID+1（事务ID自增） creator_trx_id ReadView创建者的事务ID Tips：m_ids的长度可不是max_trx_id - min_trx_id，因为m_ids是当前活跃事务的ID集合，在min_trx_id到max_trx_id即可能有活跃事务，也可能有非活跃事务。 当一个事务需要读取一条记录时，需要遵循以下四条规则进行读取（非常重要）： DB_TRX_ID == creator_trx_id时，说明该数据就是当前事务更改的，可以访问该版本。 DB_TRX_ID &lt; min_trx_id时，比最小活跃事务ID小，说明当前事务已经提交了，可以访问该版本。 DB_TRX_ID &gt; max_trx_id时，比预分配事务ID大，说明当前事务在ReadView生成后才开始，还没有提交不能访问该版本。 min_trx_id &lt;= DB_TRX_ID &lt;= max_trx_id时，如果DB_TRX_ID不在m_ids中，即当前事务已经提交了，可以访问该版本。 看完这些规则我们可以总结以下规律： 当前事务可以读取自己更改的记录，对应第一条规则 只有一个事务提交了，才能去读取该事务ID下的版本记录（保证事务的隔离性，防止脏读），对应第二、三、四条规则 3.MVCC的执行流程 这里承接第二部分举过的案例，来具体分析事务5在不同隔离级别两次查询id为10的记录时，分别会读取哪个版本的数据。学会这个案例之后，就能理解MVCC如何解决不可重复读和幻读的问题。 RC隔离级别 在RC隔离级别下，事务每一次执行快照读时都会生成一次ReadView。 在第一次查询时，还未提交的事务有3、4、5，那么m_ids（活跃事务ID集合）为{3,4,5}，min_trx_id（最小活跃事务ID）为3，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 在第二次查询时，还未提交的事务有4、5，那么m_ids（活跃事务ID集合）为{4,5}，min_trx_id（最小活跃事务ID）为4，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 RR隔离级别 在RR隔离级别下，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。 在第一次查询时，还未提交的事务有3、4、5，那么m_ids（活跃事务ID集合）为{3,4,5}，min_trx_id（最小活跃事务ID）为3，max_trx_id（预提交事务ID）为5+1=6，creator_trx_id（事务创建者ID）为5。 在第二次查询时，直接复用ReadView。 读取版本记录 在读取版本记录时，需要根据DB_TRX_ID匹配ReadView的读取规则，判断当前记录对DB_TRX_ID对应的事务是否可见，如果可见，直接读取当前版本，如果不可见，则读取前一个undo log记录继续进行匹配。 我们以第一个ReadView举例，当前undo log版本链和读视图如下： 当DB_TRX_ID为4，存在于活跃事务列表中，因此不可以读取该行数据，需要向前找DB_TRX_ID为3的记录。 当DB_TRX_ID为3时，同样存在于活跃事务列表，因此不可以读取该行数据，需要向前找DB_TRX_ID为2的记录。 当DB_TRX_ID为2时，发现DB_TRX_ID&lt;min_trx_id，符合规则，因此可以读取该行记录。 最后的读取结果为： 10 20 Jack 2 0x00001 4.MVCC小结 MVCC解决不可重复读 RC隔离级别 在RC读取已提交下，事务每一次执行快照读时都会生成一次ReadView，这也就造成了每次读取就有不同 ReadView，那么就会读到已提交的事务修改的内容，不能解决不可重复读的问题。 RR隔离级别 解决 RR 不可重复读主要靠 Readview，在隔离级别为可重复读时，仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。由于后续复用了 ReadView，所以数据对当前事务的可见性和第一次是一样的，所以从 undo log 中读到的数据快照和第一次是一样的，即便过程中有其他事务修改也读不到。因此解决了不可重复读的问题。 MVCC解决幻读 InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock（临键锁） 来解决幻读问题： 1、执行快照读 在快照读的情况下，RR 隔离级别使用MVCC，只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”。 2、执行当前读 在当前读的情况下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读。InnoDB 使用Next-key Lock来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。"},{"title":"【MySQL】索引概念解析","date":"2024-08-01T06:05:13.000Z","url":"/2024/08/01/%E3%80%90MySQL%E3%80%91%E7%B4%A2%E5%BC%95%E6%A6%82%E5%BF%B5%E8%A7%A3%E6%9E%90/","tags":[["索引","/tags/%E7%B4%A2%E5%BC%95/"]],"categories":[["MySQL","/categories/MySQL/"]],"content":"1.什么是索引？ MySQL中的索引是一种数据结构，用于帮助MySQL数据库管理系统快速查询数据。索引的主要目的是提高数据检索的速度，减少数据库系统需要扫描的数据量。 优点： 索引可以极大的提高数据检索效率，降低数据库IO成本 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性 通过索引列对数据进行排序，降低数据排序的成本，减少CPU的消耗 缺点： 创建索引需要消耗物理空间。对于大型数据库，索引可能会占用相当大的磁盘空间。 创建索引和维护索引需要消耗时间，降低表的更新效率。对表中数据进行增删改操作时，那么索引也需要动态的修改，会降低 SQL 执行效率。 适用场景： 具有唯一性约束的字段，比如商品编码，可以适用唯一性索引 频繁使用的列，如主键、外键 经常用于WHERE查询条件的字段，如果查询条件不是⼀个字段，可以建⽴联合索引。 用于GROUP BY或ORDER BY中的字段，由于索引基于B+树实现，会自动维护数据的有序性，降低数据排序的成本。 不适用场景： WHERE、GROUP BY或ORDER BY用不到的字段，索引的作用是快速定位，用不到的话会额外占用空间 存在大量重复元素的字段，如性别，无论怎么搜索可能只会得到一半的数据。 表数据太少的时候，无需创建索引。 频繁修改的列，当对表中数据进行增删改操作时，由于索引需要维护B+树的有序性，会频繁的创建索引，影响数据库的性能。 2.索引结构选型 B+ 树非常适合作为数据库索引结构，特别是在处理大量数据的场景下，能够提供高效的查询、插入和删除操作，并且支持范围查询和顺序扫描。这些特性使得 B+ 树成为 MySQL 等数据库系统中首选的索引数据结构。 下面将针对不同的数据结构进行分析，以说明B+树为何能在众多数据结构中脱颖而出。 Hash表 MySQL 的 InnoDB 存储引擎不直接支持常规哈希索引，但有一种自适应哈希索引（Adaptive Hash Index）。这种索引结合了 B+ 树和哈希索引的特点，适应实际数据访问模式和性能需求。自适应哈希索引的每个哈希桶实际上是一个小型的 B+ 树结构，存储多个键值对，减少了哈希冲突，提高了效率。 MySQL 没有采用哈希索引作为主要索引结构，主要因为哈希索引不支持顺序和范围查询。此外，每次 IO 只能取一个值，限制了查询性能。 二叉查找树(BST) 二叉查找树的性能非常依赖于它的平衡程度。 平衡时：查询时间复杂度为 O(log N)，效率较高。 不平衡时：最坏情况下退化为线性链表，查询效率降至 O(N)。 AVL树 AVL 树是一种高度平衡二叉树，保证任何节点的左右子树高度之差不超过 1，查找、插入和删除的时间复杂度均为 O(log N)。AVL 树通过四种旋转操作（LL、RR、LR、RL）保持平衡，但频繁的旋转操作增加了计算开销，降低了数据库写操作的性能。 每个 AVL 树节点仅存储一个数据，每次磁盘 IO 只能读取一个节点的数据，需要多次 IO 查询多个节点的数据，影响了性能。 红黑树 红黑树是一种自平衡二叉查找树，通过颜色变换和旋转操作保持平衡，具有以下特点： 每个节点非红即黑； 根节点总是黑色； 每个叶子节点是黑色的空节点（NIL）； 红色节点的子节点必须是黑色； 从任意节点到叶子节点的每条路径包含相同数量的黑色节点。 红黑树追求的是大致平衡，查询效率略低于 AVL 树，因为红黑树的平衡性较弱，可能导致树的高度较高，需要多次磁盘 IO 操作。这也是 MySQL 没有选择红黑树的原因之一。但红黑树的插入和删除操作效率高，因为只需进行 O(1) 次数的旋转和变色操作，保持基本平衡状态。 红黑树广泛应用于 TreeMap、TreeSet 和 JDK1.8 的 HashMap 底层，在内存中的表现非常优异。 B树&amp;B+树 B 树也称 B-树,全称为 多路平衡查找树 ，B+ 树是 B 树的一种变体。B 树和B+ 树中的 B 是 Balanced （平衡）的意思。 目前大部分数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。 B 树&amp;B+ 树两者有何异同呢？ 存储方式不同 B 树的所有节点既存放键（key）也存放数据（data），而B+树只有叶子节点存放 key 和 data，非叶子节点只存放 key。 单点查询稳定性不同 B 树的查询波动较大，因为每个节点既存放索引又存放记录，有时访问到非叶子节点就能找到数据，有时需要访问叶子节点才能找到。 B+ 树的非叶子节点仅存放索引，因此可以存放更多的索引，使得 B+ 树比 B 树更「矮胖」，查询底层节点的磁盘 I/O 次数更少。 插入和删除效率不同 在B树中，当内部节点需要删除或插入时，可能会涉及到多个子节点的调整。由于B树的非叶子节点也存储数据，因此分裂或合并操作需要确保数据的完整性和树的平衡。 相比之下，B+树的非叶子节点只存储键信息，不存储实际的数据。因此，在分裂或合并非叶子节点时，只需要处理键信息，这使得操作相对简单且高效。并且，B+树的叶子节点包含所有实际的数据，并且它们之间通过指针相连。这使得在删除节点时，可以更容易地重新组织数据以保持树的平衡。 范围查询效率不同 B+ 树支持范围查询。进行范围查找时，从根节点遍历到叶子节点即可，因为数据都存储在叶子节点上，且叶子节点通过指针连接，便于范围查找。 3.索引的类型 按照底层存储方式角度划分： 聚簇索引（聚集索引）：索引结构和数据一起存放的索引，只有InnoDB 中的主键索引属于聚簇索引。 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，二级索引(辅助索引)就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。 按照应用维度划分： 主键索引 加速查询 + 列值唯一 + 不可以有NULL + 表中只有一个。 普通索引 加速查询 + 列值可以重复 + 可以有NULL。 唯一索引 加速查询 + 列值唯一 + 可以有NULL。 联合索引 多个列组成一个索引，专门用于组合搜索，其效率大于多个单列索引的合并效率。 覆盖索引 一个索引包含所有需要查询的字段的值。 全文索引 对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。 "},{"title":"【设计模式】工厂模式详解","date":"2024-07-28T08:21:10.000Z","url":"/2024/07/28/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["工厂模式","/tags/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 工厂模式是一种创建型设计模式，通过提供一个接口或抽象类来创建对象，而不是直接实例化对象。工厂模式的主要思想是将对象的创建与使用分离，使得创建对象的过程更加灵活和可扩展。 工厂模式主要包括以下角色： 抽象工厂（Abstract Factory）：定义了一个创建产品对象的接口，可以包含多个方法来创建不同类型的产品。 具体工厂（Concrete Factory）：实现抽象工厂接口，负责实例化具体的产品对象。 抽象产品（Abstract Product）：定义了产品的接口或抽象类，是工厂方法和抽象工厂模式中的基础。 具体产品（Concrete Product）：实现抽象产品接口，具体定义产品的功能和行为。 2.简单工厂模式 简单工厂模式（Simple Factory Pattern）：由一个工厂类根据传入的参数决定创建哪一种产品类的实例。它通常包含一个静态方法，这个方法根据参数创建相应的对象。 定义一个简单的例子：电脑有很多品牌，如惠普电脑、联想电脑，如果需要创建这两个对象时，主动new出来，使用了简单工厂模式后，可以把创建的动作交给工厂类，只需要指定参数即可获取对应的对象。 实现方法 编写产品类 首先创建一个Computer接口，不同的产品实现这一接口 编写工厂类 简单工厂模式不存在抽象工厂，只需编写一个工厂类即可。 测试类使用工厂创建产品 输出结果如下： 小结 简单工厂模式虽然实现比较简单，但是工厂类的职责过重，增加新的产品类型需要修改工厂类，违背了开闭原则。 开闭原则：软件实体（类、模块、函数等）应该对扩展开放，对修改关闭。 对扩展开放（Open for extension）：软件实体应该允许在不改变其现有代码的情况下，通过增加新功能来对其进行扩展。也就是说，当软件的需求发生变化时，我们应该能够通过添加新代码来满足这些需求，而不需要修改已有的代码。 对修改关闭（Closed for modification）：一旦软件实体被开发完成并投入使用，其源代码就不应该再被修改。这可以防止对现有功能的破坏，减少引入新的错误的风险，并使软件更加稳定和可维护。 3.工厂方法模式 工厂方法模式（Factory Method Pattern）：定义一个创建对象的接口，但由子类决定实例化哪个类。工厂方法将对象的创建推迟到子类。 实现方法 编写产品类 编写工厂类 需要定义一个抽象工厂，然后由具体工厂创建对应的产品。 测试类使用不同的具体工厂创建产品 输出结果如下： 小结 优点： 遵循开闭原则，新增产品时不需要修改现有系统代码，只需要添加新的具体工厂和具体产品类。 更符合单一职责原则，每个具体工厂类只负责创建一种产品。 缺点： 增加了系统复杂度，需要增加额外的类和接口。 4.抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。适用于产品族的场景，即多个产品等级结构中相关的产品需要一起创建和使用。 产品等级结构：指产品的继承结构，例如一个电脑抽象类，它有HP电脑、Lenovo电脑等实现类，那么这个电脑抽象类和他的实现类就构成了一个产品等级结构。 产品族：产品族是指由同一个工厂生产的，位于不同产品等级结构中的一组产品。比如，Lenovo除了生产电脑还可以生产打印机等其他产品。 实现方法 编写产品类 编写工厂类 定义一个抽象工厂，该工厂可以创建多个产品。 测试类使用不同的具体工厂创建产品 输出结果如下： 小结 优点： 符合开闭原则，新增产品族时无需修改现有系统代码。 符合单一职责原则，每个具体工厂类只负责创建一类产品族。 保证产品族的一致性，同一个工厂创建的产品是属于同一个产品族的。 缺点： 增加了系统的复杂度。修改产品族时，需要修改所有具体工厂类，扩展性稍差。 5.总结 适用场景： 简单工厂模式：适用于产品种类较少，客户端只需根据参数获得具体产品的简单场景。适合产品种类不经常变化的场合。 工厂方法模式：适用于产品种类较多，每个产品有相应的具体工厂类。适合需要扩展新产品，且不希望修改现有代码的场合。 抽象工厂模式：适用于产品族较多，每个产品族中包含多个相关产品。适合创建一系列相关或相互依赖的产品，且希望统一管理产品族的场合。 "},{"title":"【设计模式】代理模式详解","date":"2024-07-28T03:16:29.000Z","url":"/2024/07/28/%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","tags":[["代理模式","/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"1.简介 代理模式是常用的Java设计模式，该模式的特点是代理类与委托类共享相同的接口。代理类主要负责预处理消息、过滤消息、将消息转发给委托类，并在事后处理消息等。代理类与委托类之间通常存在关联关系，一个代理类对象与一个委托类对象关联。代理类对象本身不真正实现服务，而是通过调用委托类对象的相关方法来提供特定的服务。 代理模式主要包括以下角色： 抽象主题（Subject）：定义代理类和委托类（RealSubject）的共同接口。这个接口规定了代理类和委托类必须实现的方法，代理类可以通过这个接口来调用委托类的方法。 真实主题（RealSubject）：实现抽象主题，定义委托类的操作。它包含了实际的业务逻辑，是客户端实际需要调用的对象。 代理类（Proxy）：实现抽象主题，持有对委托类的引用，并在其方法被调用时进行控制。代理类在调用委托类的方法前后可以添加一些额外的功能，如日志记录、权限控制、事务处理等。 2.静态代理 静态代理：在编译时期确定代理类和目标类的关系，代理类和目标类都要实现同一个接口。 定义一个简单的例子：假如一个租客需要租房子，他可以直接通过房东（委托类）去租房，也可以经过中介（代理类）去租房。房东（realsubject）和中介（proxy）都需要实现subject接口实现房子出租。 确定接口具体行为 首先创建一个Person接口。这个接口是房东和中介的共同接口，租房行为可以被中介代理。 编写委托类业务逻辑 创建一个委托类，实现subject接口，并编写业务逻辑 代理类增强方法 创建一个代理类，同样实现subject接口，对委托类的方法进行增强 测试类使用代理对象 输出结果如下，可以发现对方法进行了增强 3.动态代理 动态代理：在程序运行时动态生成代理类（subject的实现类）。 相比于静态代理， 动态代理的优势在于其较高的灵活性和代码复用性。同一个动态代理处理器可以代理多个目标对象，而静态代理则需要创建大量的代理类。 在Java中，可以通过JDK和CGLIB实现动态代理。 3.1.JDK动态代理 实现原理 JDK动态代理：在java的java.lang.reflect包下提供了Proxy类和InvocationHandler接口，利用这两个类和接口，可以在运行时动态生成指定接口的实现类。 Proxy类就是用来创建一个代理对象的类，在JDK动态代理中我们需要使用其newProxyInstance方法。 这个方法的作用就是创建一个代理类对象，它接收以下三个参数： loader：一个ClassLoader对象，指定哪个ClassLoader将加载生成的代理类。 interfaces：一个Interface对象数组，定义代理对象实现的一组接口，代理类可以调用这些接口中声明的所有方法。 h：一个InvocationHandler对象，指定代理对象的方法调用将关联到哪个InvocationHandler对象，由它处理实际的方法调用。 InvocationHandler接口提供了一个invoke方法，当代理对象调用方法时，invoke方法会被调用。通过实现这个接口，可以在方法调用前后添加自定义逻辑。 代码实现 确定接口具体行为 这里我们设计两个接口 编写委托类业务逻辑 委托类实现这两个接口，并且定义具体的业务逻辑 编写代理工厂代码 代理工厂负责在运行时动态生成代理类，需要实现InvocationHandler接口重写invoke方法来做方法增强，使用Proxy类创建代理对象。 测试类使用代理对象 在实际使用时，只需要将工厂类生成的代理对象转为需要的代理类，即可实现同时代理多个接口的方法。 返回结果： 3.2.CGLIB动态代理 实现原理 CGLIB动态代理：依赖于ASM下的Enhancer类和MethodInterceptor接口，可以在运行时动态生成目标类的子类。 Enhancer类是用来创建代理对象的类。在CGLIB动态代理中，我们需要使用其create方法。 这个方法的作用是创建一个代理类对象，通常还需要设置以下几个属性： setSuperclass：设置被代理的目标类，CGLIB通过生成目标类的子类来实现代理。 setCallback：设置回调接口，用于处理代理对象的方法调用。 MethodInterceptor接口提供了一个intercept方法，当代理对象调用方法时，intercept方法会被调用。通过实现这个接口，可以在方法调用前后添加自定义逻辑。 与JDK动态代理不同，CGLIB代理不需要目标类实现接口。CGLIB通过生成目标类的子类并重写方法来实现代理，因此它可以代理没有实现接口的类。 代码实现 首先导入依赖 编写委托类业务逻辑（无需实现接口） 测试类使用代理对象 返回结果： 4.总结 静态代理 实现方式： 由程序员显式编写代理类。代理类在编译期确定，编译前就存在代理类的字节码文件。 需要实现与目标对象相同的接口，且在代理类中显式调用目标对象的方法。 优点： 结构简单，容易理解。 缺点： 每增加一个接口，都需要编写对应的代理类，代码量大，维护成本高。静态代理类在编译期生成，灵活性差。 JDK动态代理 实现方式： 使用java.lang.reflect.Proxy类和java.lang.reflect.InvocationHandler接口。 代理类在运行时动态生成，不需要显式编写代理类。 优点： 代理类在运行时生成，增加了代码的灵活性和可维护性。 缺点： 只能代理实现了接口的类，不能代理没有实现接口的类。 CGLIB动态代理 实现方式： 使用CGLIB（Code Generation Library），依赖ASM字节码生成框架。 代理类在运行时动态生成，不需要显式编写代理类。 优点： 不要求目标类实现接口，可以代理普通的类。 性能通常比JDK动态代理更高，尤其在代理大量方法调用时更为显著。 缺点： 不能代理final类和final方法。 适用场景： 静态代理：需要手动编写代理类，适用于简单的场景，但不够灵活，维护成本高。 JDK动态代理：适用于实现了接口的类，代理类在运行时生成，灵活性高，但只能代理接口。 CGLIB动态代理：适用于没有实现接口的类，性能优于JDK动态代理，但不能代理final类和final方法，且使用复杂度稍高。 "},{"title":"【SpringBoot】SpringCache轻松启用Redis缓存","date":"2024-07-15T08:20:41.000Z","url":"/2024/07/15/%E3%80%90SpringBoot%E3%80%91SpringCache%E8%BD%BB%E6%9D%BE%E5%90%AF%E7%94%A8Redis%E7%BC%93%E5%AD%98/","tags":[["Redis","/tags/Redis/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"1.前言 Spring Cache是Spring提供的一种缓存抽象机制，旨在通过简化缓存操作来提高系统性能和响应速度。Spring Cache可以将方法的返回值缓存起来，当下次调用方法时如果从缓存中查询到了数据，可以直接从缓存中获取结果，而无需再次执行方法体中的代码。 2.常用注解 @Cacheable：在方法执行前查看是否有缓存对应的数据，如果有直接返回数据，如果没有调用方法获取数据返回，并缓存起来； @CacheEvict：将一条或多条数据从缓存中删除； @CachePut：将方法的返回值放到缓存中； @EnableCaching：开启缓存注解功能； @Caching：组合多个缓存注解。 3.启用缓存 3.1.配置yaml文件 3.2.添加注解 在启动类上添加注解@EnableCaching： 3.3.创建缓存 使用@CachePut注解。当方法执行完后，如果缓存不存在则创建缓存；如果缓存存在则更新缓存。 注解中的value属性可指定缓存的名称，key属性则可指定缓存的键，可使用SpEL表达式来获取key的值。 这里result表示方法的返回值UserInfo，从UserInfo中获取id属性。 3.4.更新缓存 同样使用@CachePut注解。当方法执行完后，如果缓存不存在则创建缓存；如果缓存存在则更新缓存。 3.5.查询缓存 使用@Cacheable注解。在方法执行前，首先会查询缓存，如果缓存不存在，则根据方法的返回结果创建缓存；如果缓存存在，则直接返回数据，不执行方法。 这里使用request表示方法的参数UserQueryRequest。 3.6.删除缓存 使用@CacheEvict注解。当方法执行完后，会根据key删除对应的缓存。 这里可以使用condition属性，当返回结果为true（删除成功）后，才去删除缓存。 3.7.多重操作 使用@Caching注解，通过使用不同的属性进行相应操作。 创建/更新多个缓存： 删除多个缓存： "},{"title":"【RabbitMQ】一文详解消息可靠性","date":"2024-07-13T15:10:21.000Z","url":"/2024/07/13/%E3%80%90RabbitMQ%E3%80%91%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7/","tags":[["RabbitMQ","/tags/RabbitMQ/"],["消息可靠性","/tags/%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7/"]],"categories":[["消息队列","/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"]],"content":"1.前言 RabbitMQ 是一款高性能、高可靠性的消息中间件，广泛应用于分布式系统中。它允许系统中的各个模块进行异步通信，提供了高度的灵活性和可伸缩性。然而，这种通信模式也带来了一些挑战，其中最重要的之一是确保消息的可靠性。 影响消息可靠性的因素主要有以下几点： 发送消息时连接RabbitMQ失败 发送时丢失： 生产者发送的消息未送达交换机； 消息到达交换机后未到达队列； MQ 宕机，队列中的消息会丢失； 消费者接收到消息后未消费就宕机了。 2.生产者 2.1.生产者重连机制 生产者发送消息时，出现了网络故障，导致与MQ的连接中断。为了解决这个问题，RabbitMQ提供的消息发送时的重连机制。即：当RabbitTemplate与MQ连接超时后，多次重试。 在生产者yml文件添加配置开启重连机制 当网络不稳定的时候，利用重试机制可以有效提高消息发送的成功率。但是RabbitMQ提供的重试机制是阻塞式的重试。 如果对于业务性能有要求，建议禁用重试机制。如果一定要使用，就需要合理配置等待时长和重试次数，或者使用异步线程来执行发送消息的代码 2.2.生产者确认机制 RabbitMQ的生产者确认机制（Publisher Confirm）是一种确保消息从生产者发送到MQ过程中不丢失的机制。当消息发送到 RabbitMQ 后，系统会返回一个结果给消息的发送者，表明消息的处理状态。这个结果有两种可能的值： 返回结果有两种方式： publisher-confirm(发送者确认) 消息成功投递到交换机，返回ACK。 消息未投递到交换机，返回NACK。（可能是由于网络波动未能连接到RabbitMQ，可利用生产者重连机制解决） publisher-return(发送者回执) 消息投递到交换机了，但是没有路由到队列。返回ACK和路由失败原因。（这种问题一般是因为路由键设置错误，可以人为规避） 通过这种机制，生产者在发送消息后获取返回的回执结果，从而采取对应的策略，如消息重发或记录失败信息。 3.数据持久化 3.1.配置持久化 在默认情况下，RabbitMQ会将接收到的信息保存在内存中以降低消息收发的延迟。这样会导致两个问题 RabbitMQ宕机，存在内存中的消息会丢失。 内存空间有限，当消费者故障或处理过慢时，会导致消息积压，引发MQ阻塞。 为了提升性能，默认情况下MQ的数据都是在内存存储的临时数据，重启后就会消失。RabbitMQ可以通过配置数据持久化，从而将消息保存在磁盘，包括： 交换机持久化（确保RabbitMQ重启后交换机仍然存在） 队列持久化（确保RabbitMQ重启后队列仍然存在） 消息持久化（确保RabbitMQ重启后队列中的消息仍然存在） 由于Spring会在创建队列时默认将交换机和队列设置为持久化，发送消息时也默认指定消息为持久化消息，因此不需要额外配置。 3.2.惰性队列 从RabbitMQ的3.6.0版本开始，就增加了Lazy Queue的概念，也就是惰性队列。 在3.12版本后，所有队列都是Lazy Queue模式，无法更改。 惰性队列的特点如下： 接收到消息后直接存入磁盘而非内存(内存中只保留最近的消息，默认2048条) 消费者要消费消息时才会从磁盘中读取并加载到内存 支持数百万条的消息存储 对于低于3.12版本的情况，可以使用注解的arguments来指定 3.3.为什么需要数据持久化？ 数据持久化在 RabbitMQ 中有以下重要作用： 队列和交换机的持久化： 防止重启后丢失：将队列和交换机设置为持久化，可以防止 RabbitMQ 服务器重启后丢失这些队列和交换机，确保它们的存在和绑定关系保持不变。 消息的持久化： 安全性： 防止数据丢失：消息持久化后，可以防止 RabbitMQ 服务器重启或宕机时数据丢失，方便数据恢复，保证消息的可靠性和耐久性。 性能： 内存管理：未持久化的临时消息默认存储在内存中。内存空间有限，大量消息涌入时会导致内存占满，系统需要进行 page out 操作将消息写入磁盘。频繁的 page out 操作会严重影响性能。 预防内存溢出：通过持久化消息，可以缓解内存压力，防止因内存溢出导致的系统性能问题和崩溃。 4.消费者 4.1.消费者确认机制 为了确认消费者是否正确处理了消息，RabbitMQ提供了消费者确认机制。当消费者处理消息后，会返回回执信息给RabbitMQ。回执有三种值： ack：消息处理成功，RabbitMQ从队列中删除消息。 nack：消息处理失败，RabbitMQ需要再次投递消息。 reject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除消息。 在SpringBoot项目中，我们可以通过配置文件选择回执信息的处理方式，一共有三种处理方式： none：不处理。RabbitMQ 假定消费者获取消息后会一定会成功处理，因此消息投递后立即返回ack，将消息从队列中删除。 manual：手动模式。需要在业务代码结束后，调用SpringAMQP提供的API发送ack或reject，存在代码侵入问题，但比较灵活。 auto：自动模式。SpringAMQP利用AOP对我们的消息处理逻辑进行了环绕增强，返回结果如下： 如果消费者正常处理消息，自动返回ack并删除队列的消息。 如果消费者消息处理失败，自动返回nack并重新向消费者投递消息。 如果消息校验异常，自动返回reject并删除队列中的消息。 注意：手动模式返回回执消息时通常需要显式指定requeue参数，当requeue=true时，表明消息需要重新入队；当requeue=false时，RabbitMQ将从队列删除消息。 4.2.消息失败重试机制 当消费者出现异常后，消息会不断requeue（重新入队）到队列，再重新发送给消费者，然后再次异常，再次requeue无限循环，导致mq的消息处理飙升，带来不必要的压力。 可以通过设置yml文件开启失败重试机制，在消息异常时利用本地重试，而不是无限制的进行requeue操作。 4.3.消息失败处理策略 在开启重试模式后，重试次数耗尽，如果消息依然失败，则需要有 MessageRecoverer 接口来处理，它包含三种不同的实现： RejectAndDontRequeueRecoverer：重试次数耗尽后，直接reject，丢弃消息，这是默认采取的方式； ImmediateRequeueMessageRecoverer：重试次数耗尽后，返回nack，消息重新入队； RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机。 5.死信队列 尽管通过以上设置可以确保消息在生产者、消息队列和消费者之间的传递过程中不会丢失，但在某些情况下，消费者仍可能无法成功处理消息（如消息重试次数耗尽后仍无法被消费）。这时候，我们需要一个机制来妥善处理这些无法被正常消费的消息。死信队列便是用于解决这一问题的兜底机制。 5.1.死信 当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）： 消息被拒绝： 当消费者明确拒绝一个消息并且设置不再重新入队（requeue=false）时，这个消息会被标记为死信。 消息过期： 每个消息或队列可以设置一个TTL（Time-To-Live），即消息的存活时间。如果消息在队列中停留的时间超过了这个TTL，消息会被认为过期，并被转移到死信队列。 队列达到最大长度： 如果队列设置了最大长度并且达到了这个限制，那么新进入的消息会被转移到死信队列中。 5.2.创建死信队列 5.2.1.创建死信交换机和死信队列 正常使用注解，创建交换机和队列即可 5.2.2.绑定死信交换机 如果队列通过dead-letter-exchange属性指定了一个交换机，那么该队列中的死信就会投递到这个交换机中。这个交换机称为死信交换机（Dead Letter Exchange，简称DLX） 可以通过@Argument注解指定死信交互机和路由键，如下。 "},{"title":"【SpringBoot】随机盐值+双重SHA256加密实战","date":"2024-07-08T15:12:15.000Z","url":"/2024/07/08/%E3%80%90SpringBoot%E3%80%91%E9%9A%8F%E6%9C%BA%E7%9B%90%E5%80%BC+%E5%8F%8C%E9%87%8DSHA256%E5%8A%A0%E5%AF%86%E5%AE%9E%E6%88%98/","tags":[["密码加密","/tags/%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"1.SHA-256和Salt 1.1.什么是SHA-256 SHA-256是一种信息摘要算法，也是一种密码散列函数。对于任意长度的消息，SHA256都会产生一个256bit长的散列值（哈希值），用于确保信息传输完整一致，称作消息摘要。这个摘要相当于是个长度为32个字节的数组，通常用一个长度为64的十六进制字符串来表示。 SHA-256的具备以下几个关键特点： 固定长度输出：无论输入数据的大小，SHA-256都会产生一个256位（32字节）的固定长度散列值。 不可逆性：SHA-256的设计使得从生成的散列值无法还原原始输入数据。这种不可逆性在安全性上是非常重要的。 抗碰撞性：找到两个不同的输入数据具有相同的散列值（碰撞）是极其困难的。虽然理论上碰撞可能发生，但SHA-256被设计得非常抗碰撞。 除了SHA-256之外，还有一个密码散列函数MD5，过去也常被用于密码加密，但MD5在安全性上低于SHA-256，现在已经很少用于密码加密了，本文不做考虑。 SHA-256 和 MD5 的比较： 特性 SHA-256 MD5 输出长度 256 位（64 个十六进制字符） 128 位（32 个十六进制字符） 安全性 高 低 计算速度 较慢 快 抗碰撞能力 强 弱 应用场景 数据完整性校验、数字签名、密码存储、区块链 曾用于文件校验、密码存储 推荐使用 是 否 1.2.什么是随机盐值 盐值（salt）是一种在密码学和安全计算中常用的随机数据，用于增强密码散列的安全性。 随机盐值（random salt）是一种用于增强密码散列安全性的技术。它是一个随机生成的数据块，在将密码输入散列函数之前，将盐值与密码组合。通过引入随机盐值，可以有效地防止彩虹表攻击和相同密码散列值重复的问题。 盐值的作用： 防止彩虹表攻击： 彩虹表是一个预计算的哈希值数据库，用于快速查找常见密码的哈希值。通过在密码哈希之前加入随机盐值，即使密码相同，其最终的哈希值也会不同，从而使彩虹表无效。 避免散列值重复： 如果两个用户使用相同的密码，在没有盐值的情况下，他们的哈希值会相同。加入盐值后，即使密码相同，生成的哈希值也会不同，这有助于防止攻击者通过观察哈希值来推测用户是否使用了相同的密码。 增加攻击难度： 盐值增加了密码哈希的复杂性。即使攻击者获取了存储的哈希值和盐值，他们仍需对每个盐值进行单独的暴力破解，显著增加了破解的时间和计算成本。 1.3.如何进行加密操作 本文采用的加密方式是在前端采用md加密防止明文传输，后端对密码二次加密后再进行随机盐值的混入。 2.前端实现 引入md5.min.js 3.后端实现 3.1.导入Maven依赖 3.2.密码加密 3.2.1.密码加盐 首先使用Apache的RandomStringUtils工具类，生成16位的盐值。然后将盐拼接到明文后面，进行SHA256加密。 这个加密后的SHA256是个固定64长度的字符串。 3.2.2.随机盐值混合 加盐后的SHA256码长度为80位，这里我们采用的盐值混合规则：将SHA-256散列值的每四个字符中间插入一个盐值字符，依次交替排列。 这样就完成了加密的操作：密码加盐 + 盐值混合。 3.3.密码解密 3.3.1.提取盐值和加盐密码 按照加密时采用的规则：将SHA-256散列值的每四个字符中间插入一个盐值字符，依次交替排列。 我们可以将盐值和加盐后的SHA-256码 3.3.2.比较密码 最后，将取出的盐值与原始密码再次加盐，再次得到加盐密码，与sha256Hex比较即可判断密码是否相同。 3.4.完整工具类 "},{"title":"【消息队列】RabbitMQ基本概念","date":"2024-06-30T15:52:12.469Z","url":"/2024/06/30/%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91RabbitMQ%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","tags":[["RabbitMQ","/tags/RabbitMQ/"]],"categories":[["消息队列","/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"]],"content":"1.RabbitMQ的架构 Producer(生产者)：生产者是消息的发送方，负责将消息发布到RabbitMQ的交换器 (Exchange)。 Consumer(消费者)：消费者是消息的接收方，负责从队列中获取消息，并进行处理和消费。 Exchange(交换器)：交换器是消息的接收和路由中心，它接收来自生产者的消息，并将消息路由到一个或多个与之绑定的队列(Queue)中。 Queue(队列)：队列是消息的存储和消费地，它保存着未被消费的消息，等待消费者(Consumer) 从队列中获取并处理消息。 Binding(绑定)：绑定是交换器和队列之间的关联关系，它定义了交换器将消息路由到哪些队列中。 VHost(虚拟主机)：它类似于操作系统中的命名空间，用于将RabbitMQ的资源进行隔离和分组。每个VHost拥有自己的交换器、队列、绑定和权限设置，不同VHost之间的资源相互独立，互不干扰。VHost可以用于将不同的应用或服务进行隔离，以防止彼此之间的消息冲突和资源竞争。 2.RabbitMQ的工作模式 简单模式（Simple Mode）：在这种模式下，有一个生产者和一个消费者。生产者发送消息，消费者接收并处理这些消息。这种模式是最简单的RabbitMQ使用方式。 Work模式（Work Queues）：在这种模式下，有一个生产者和多个消费者。生产者发送消息到队列中，消费者从队列中取出消息进行处理。RabbitMQ通过轮询的方式将消息平均发送给消费者，确保一条消息只被一个消费者接收和处理。 发布/订阅模式（Publish/Subscribe）：在这种模式下，生产者发送消息到交换机，交换机将消息广播到所有与之绑定的队列中，然后消费者从队列中取出消息进行消费。这种模式允许消费者有选择性地接收消息。 路由模式（Routing）：路由模式与发布/订阅模式类似，但是生产者发送消息时需要指定一个路由键（routing key），交换机根据路由键将消息发送到匹配的队列中。消费者需要将其队列绑定到交换机，并指定路由键以便接收消息。 Topic模式：这种模式是路由模式的扩展，它使用更灵活的匹配规则。生产者发送消息时指定一个路由键，消费者可以将其队列绑定到交换机上，并指定一个模式（topic），该模式可以包含通配符，用于匹配路由键。这样，消费者可以接收符合特定模式的所有消息。 RPC模式：支持生产者和消费者不在同一个系统中，即允许远程调用的情况。通常，消费者作为服务端，放置在远程的系统中，提供接口，生产者调用接口，并发送消息。 3.RabbitMQ的作用 应用解耦：在生产者和消费者之间建立了一个缓冲区，使得两者之间的处理速度可以异步进行，不需要严格匹配。这大大增加了系统的灵活性和可扩展性。 削峰填谷：在高并发场景下，大量请求可能瞬间涌入系统，导致系统压力过大。RabbitMQ可以作为一个缓冲存储，将一部分请求暂时存入队列中，起到“削峰”的作用，保证系统平稳运行，在高峰期过去后，继续从队列中取出消息进行处理，直到积压的消息被完全消费。 异步通信：RabbitMQ支持异步通信，生产者将消息发送到队列后，不必等待消费者处理完消息再返回结果，而是可以继续执行其他任务。这大大提高了系统的并发处理能力和响应速度。 流量整形：RabbitMQ可以对消息进行优先级排序、延迟处理等，使得消息的处理更加有序和高效。 4.如何处理RabbitMQ的消息积压问题？ 增加消费者数量：当消息队列中的消息数量超出当前消费者的处理能力时，可以动态地增加消费者的数量。这样，更多的消费者可以并行地处理消息，从而加快消息的消费速度。 提高消费者的处理能力：优化消费者的代码逻辑，提升消费者的性能，例如通过多线程或其他并行处理技术，可以提高单个消费者的处理速度。 设置消息的过期时间：为了避免消息无限期地积压在队列中，可以为消息设置一个合理的过期时间。当消息在队列中等待时间过长且未被消费时，RabbitMQ可以自动将其丢弃或进行其他处理。 5.RabbitMQ的死信队列 死信队列介绍 RabbitMQ的死信队列(Dead Letter Queue，简称DLQ)是一种用于处理消息处理失败或无法路由的消息的机制。它允许将无法被正常消费的消息重新路由到另一个队列，以便稍后进行进一步的处理、分析或排查问题。 当消息队列里面的消息出现以下几种情况时，就可能会被称为”死信”: 消息处理失败：当消费者由于代码错误、消息格式不正确、业务规则冲突等原因无法成功处理一条消息时，这条消息可以被标记为死信。 消息过期：在RabbitMQ中，消息可以设置过期时间。如果消息在规定的时间内没有被消费，它可以被认为是死信并被发送到死信队列。 消息被拒绝：当消费者明确拒绝一条消息时，它可以被标记为死信并发送到死信队列。拒绝消息的原因可能是消息无法处理，或者消费者认为消息不符合处理条件。 消息无法路由：当消息不能被路由到任何队列时，例如，没有匹配的绑定关系或路由键时，消息可以被发送到死信队列。 当消息变成”死信”之后，如果配置了死信队列，它将被发送到死信交换机，死信交换机将死信投递到一个队列上，这个队列就是死信队列。但是如果没有配置死信队列，那么这个消息将被丢弃。 配置死信队列 在RabbitMQ中，死信队列通常与交换机(Exchange) 和队列(Queue) 之间的绑定关系一起使用。要设置死信队列，通常需要以下步骤: 创建死信队列：定义一个用于存储死信消息的队列。 创建死信交换机：为死信队列定义一个交换机，通常是一个direct类型的交换机。 将队列与死信交换机绑定：将主要队列和死信交换机绑定，以便无法处理的消息能够被转发到死信队列。 在主要队列上设置死信属性：通过设置队列的x-dead-letter-exchange和x-dead-letter-routing-key属性，来指定死信消息应该被发送到哪个交换机和路由键。 当消息被标记为死信时，它将被发送到死信队列，并可以由应用程序进一步处理、审查或记录。这种机制有助于增加消息处理的可靠性和容错性，确保不丢失重要的消息，并提供了一种处理失败消息的方式。 6.RabbitMQ如何实现延迟队列 死信队列 当RabbitMQ中的一条正常的消息，因为过了存活时间(TTL过期)、队列长度超限、被消费者拒绝等原因无法被 消费时，就会变成Dead Message，即死信。 实现方法 当一个消息变成死信之后，他就能被重新发送到死信队列中(其实是交换机-exchange)。基于这样的机制，就可以实现延迟消息了。那就是我们给一个消息设定TTL，但是并不消费这个消息，等他过期，过期后就会进入到死信队列，然后我们再监听死信队列的消息消费就行了。 而且，RabbitMQ中的这个TTL是可以设置任意时长的，这相比于RocketMQ只支持一些固定的时长而显得更加灵活一些。 死信队列实现延迟队列的缺点 但是，死信队列的实现方式存在一个问题，那就是可能造成队头阻塞，因为队列是先进先出的，而目每次只会判断队头的消息是否过期，那么，如果队头的消息时间很长，一直都不过期，那么就会阻塞整个队列，这时候即使排在他后面的消息过期了，那么也会被一直阻塞。 基于RabbitMQ的死信队列，可以实现延迟消息，非常灵活的实现定时关单，并且借助RabbitMQ的集群扩展性可以实现高可用，以及处理大并发量。他的缺点一是可能存在消息阻塞的问题；二是方案比较复杂，不仅要依赖RabbitMQ，而且还需要声明很多队列出来，增加系统的复杂度。 RabbitMQ插件 实现方法 基于插件的方式，消息并不会立即进入队列，而是先把他们保存在一个基于Erlang开发的Mnesia数据库中，然后通过一个定时器去查询需要被投递的消息，再把他们投递到x-delayed-message交换机中。 基于RabbitMQ插件的方式可以实现延迟消息，并且不存在消息阳塞的问题，但是因为是基于插件的，而这个插件 支持的最大延长时间是(232)-1 毫秒，大约49天，超过这个时就会被立即消费。 RabbitMQ插件实现延迟队列的缺点 不过这个方案也有一定的限制，它将延迟消息存在于 Mnesia 表中，并且在当前节点上具有单个磁盘副本，存在丢失的可能。 目前该插件的当前设计并不真正适合包合大量延迟消息(例如数十万或数百万)的场景，另外该插件的一个可变性来源是依赖于 Erlang 计时器，在系统中使用了一定数量的长时间计时器之后，它们开始争用调度程序资源，并且时间漂移不断累积。 7.保证RabbitMQ的消息可靠性 为了确保消息不丢失，可以采取以下措施： 持久化消息： 将消息持久化到磁盘中，这样即使系统崩溃，消息也不会丢失。常见的MQ如RabbitMQ、Kafka、ActiveMQ都支持消息持久化。 确认机制（Acknowledgment）： 发送方和接收方都需要确认消息的接收。发送方在发送消息后等待接收方的确认，接收方处理消息后需要确认已成功处理。 重复发送： 如果在设定时间内没有收到确认，发送方可以重试发送消息。这需要消息处理具备幂等性，即多次处理同一条消息不会造成副作用。 死信队列（Dead Letter Queue, DLQ）： 当消息无法被成功处理或无法被确认时，将其转移到死信队列中进行后续处理。 高可用集群： 通过MQ集群来实现高可用性，防止单点故障导致消息丢失。 "},{"title":"【并发编程】线程池参数及创建方法","date":"2024-05-27T15:43:21.000Z","url":"/2024/05/27/%E3%80%90%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E3%80%91%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8F%82%E6%95%B0%E5%8F%8A%E5%88%9B%E5%BB%BA%E6%96%B9%E6%B3%95/","tags":[["线程池","/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"]],"categories":[["并发编程","/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"]],"content":"1.什么是线程池 线程池就是管理一系列线程的资源池。当有任务要处理时，直接从线程池中获取线程来处理，处理完之后线程并不会立即被销毁，而是等待下一个任务。 为什么要使用线程池？ 使用线程池的好处： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控 2.线程池七大参数 2.1.核心线程数 corePoolSize：线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了allowCoreThreadTimeOut。当提交一个新任务时，如果线程池中的线程数小于corePoolSize，那么就会创建一个新线程来执行任务。 2.2.最大线程数 maximumPoolSize：线程池中允许的最大线程数。在当前线程数达到corePoolSize后，如果继续有任务被提交到线程池，会将任务存放到工作队列中。如果队列也已满，则会去创建一个新线程处理任务。 2.3.空闲线程存活时间 keepAliveTime：非核心线程的空闲存活时间。当线程数大于corePoolSize时，空闲时间超过keepAliveTime的线程将被终止。 这个参数在设置了allowCoreThreadTimeOut=true时对核心线程同样有效。 2.4.空闲线程存活时间单位 unit：keepAliveTime的计量单位。例如，TimeUnit.SECONDS、TimeUnit.MILLISECONDS等。 2.5.工作队列 workQueue：新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列： ①ArrayBlockingQueue 基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到corePoolSize后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到maxPoolSize，则会执行拒绝策略。 ②LinkedBlockingQuene 基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。由于该队列的近似无界性，当线程池中线程数量达到corePoolSize后，再有新任务进来，会一直存入该队列，而基本不会去创建新线程直到maxPoolSize（很难达到Interger.MAX这个数），因此使用该工作队列时，参数maxPoolSize其实是不起作用的。 ③SynchronousQuene 一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。 ④PriorityBlockingQueue 具有优先级的无界阻塞队列，优先级通过参数Comparator实现。 2.6.线程工厂 threadFactory：创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等 2.7.拒绝策略 handler：当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，线程池会执行拒绝策略。JDK中提供了4中拒绝策略： ①CallerRunsPolicy 该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。 ②AbortPolicy 该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。 ③DiscardPolicy 该策略下，直接丢弃任务，什么都不做。 ④DiscardOldestPolicy 该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列 3.如何设定线程池大小 一般来说，有两种类型的线程：CPU 密集型和 IO 密集型。 CPU 密集型的线程主要进行计算和逻辑处理，需要占用大量的 CPU 资源。 IO 密集型的线程主要进行输入输出操作，如读写文件、网络通信等，需要等待 IO 设备的响应，而不占用太多的 CPU 资源。 常见且简单的公式： CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。 I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。 4.如何创建线程池 方式一：通过ThreadPoolExecutor构造函数来创建（推荐）。 通过这种方式可以根据服务器硬件配置，灵活设定线程池参数。 方式二：通过 Executor 框架的工具类 Executors 来创建（不推荐）。 FixedThreadPool：该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 SingleThreadExecutor： 该方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。初始大小为 0。当有新任务提交时，如果当前线程池中没有线程可用，它会创建一个新的线程来处理该任务。如果在一段时间内（默认为 60 秒）没有新任务提交，核心线程会超时并被销毁，从而缩小线程池的大小。 ScheduledThreadPool：该方法返回一个用来在给定的延迟后运行任务或者定期执行任务的线程池。 Executors 返回线程池对象的弊端 FixedThreadPool 和 SingleThreadExecutor：使用的是无界的 LinkedBlockingQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 CachedThreadPool：使用的是同步队列 SynchronousQueue, 允许创建的线程数量为 Integer.MAX_VALUE ，如果任务数量过多且执行速度较慢，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool 和 SingleThreadScheduledExecutor : 使用的无界的延迟阻塞队列DelayedWorkQueue，任务队列最大长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。 "},{"title":"【SpringBoot】MapStruct实现优雅的数据复制","date":"2024-05-05T14:10:21.000Z","url":"/2024/05/05/%E3%80%90SpringBoot%E3%80%91MapStruct%E5%AE%9E%E7%8E%B0%E4%BC%98%E9%9B%85%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/","tags":[["数据复制","/tags/%E6%95%B0%E6%8D%AE%E5%A4%8D%E5%88%B6/"],["MapStruct","/tags/MapStruct/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":" 做项目时你是否遇到过以下情况： DTO（数据传输对象）与Entity之间的转换：在Java的Web应用中，通常不会直接将数据库中的Entity实体对象返回给前端。而是会创建一个DTO对象，这个DTO对象只包含需要返回给前端的字段。此时，就需要将Entity转换为DTO。 复杂对象的映射：当需要映射的对象包含大量的字段，或者字段之间存在复杂的依赖关系时，手动编写映射代码不仅繁琐，而且容易出错。 1.为什么选择MapStruct 1.1.常见的属性映射方法 一般来说，不使用MapStruct框架进行属性映射，常有的方法以下两种： Getter/Setter方法手动映射 这种方法最朴素，手动编写代码将源对象的属性存入目标对象，需要注意实体类中嵌套属性的判空操作以防止空指针异常。 BeanUtils.copyProperties()方法进行映射 BeanUtils底层使用的是反射机制实现属性的映射。反射是一种在运行时动态获取类信息、调用方法或访问字段的机制，无法利用JVM的优化机制，因此通常比直接方法调用慢得多。 此外，BeanUtils 只能同属性映射，或者在属性相同的情况下，允许被映射的对象属性少；但当遇到被映射的属性数据类型被修改或者被映射的字段名被修改，则会导致映射失败。 1.2.MapStruct的优势 MapStruct是一个基于注解的Java代码生成器，它通过分析带有@Mapper注解的接口，在编译时自动生成实现该接口的映射器类。这个映射器类包含了用于执行对象之间映射的具体代码。 与常规方法相比，MapStruct具备的优势有： 简化代码。对于对象内属性较多的情况，使用MapStruct框架无须手动对每个属性进行get/set和属性判空操作。MapStruct可以通过注解和映射接口来定义映射规则，自动生成映射代码，从而大大简化了这种复杂对象的映射过程。 性能优越。相较于反射这种映射方法，MapStruct在编译期生成映射的静态代码，可以充分利用JVM的优化机制，对于企业级的项目应用来说，这种方式能大大提高数据复制的性能。 类型安全。由于MapStruct在编译期生成映射代码，这意味着如果源对象和目标对象的映射存在错误，那么可以在编译时就发现错误。相比之下，BeanUtils在运行时使用反射来执行属性复制，这可能会导致类型不匹配的问题在运行时才发现。 灵活映射。MapStruct可以轻松处理嵌套对象和集合的映射。对于嵌套对象，MapStruct可以递归地应用映射规则；对于集合，MapStruct可以自动迭代集合中的每个元素并应用相应的映射规则。 有开发者对比过两者的性能差距，如下表。这充分体现了MapStruct性能的强大。 对象转换次数 属性个数 BeanUtils耗时 MapStruct耗时 5千万次 6 14秒 1秒 5千万次 15 36秒 1秒 5千万次 25 55秒 1秒 2.MapStruct快速入门 在快速入门中，我们的任务是将dto的数据复制到实体类中。 2.1.导入Maven依赖 2.2.创建相关对象 注意，实体类要具有get/set方法，这里我使用了lombok的@Data注解来实现。 dto类我使用了@Builder注解，可以快速为对象赋初始值。 2.3.创建转换器Converter 使用抽象类来定义转换器，只需中@Mapping注解中填写target和source的字段名，即可实现属性复制。 2.4.测试 在SpringBoot的测试类中测试，这里我使用DTO类的@Builder注解提供的方法为dto赋初值模拟实际开发，通过调用converter的方法实现属性映射。 结果如图： 最后，我们可以发现在target包的converter的相同目录下，生成了TestConverter的实现类 里面为我们编写好了映射的代码。 3.MapStruct进阶操作 如果仅是这种简单层级的对象映射，还不足以体现MapStruct的灵活性。下面将介绍MapStruct的进阶技巧。 3.1.嵌套映射 假设我们的Hotel实体类中嵌套了另外一个实体类Master dto对象为： 我们需要把personName和personAge映射到Hotel实体类的Master中，怎么做？ 很简单，只需要在target属性中加上Hotel实体类嵌套实体类的字段名，加字符.，再跟上嵌套类的字段名即可 结果如图： 3.2.集合映射 如果源对象和目标对象的集合的元素类型都是基本数据类型，直接在target和source中填写字段名即可。 若源对象和目标对象的集合元素类型不同，怎么做？ 这个案例我们需要把DTO的personList映射到masterList中。 编写converter，这次需要进行两层映射。 第一层将person集合映射到master集合上。 第二层将person对象的属性映射到master对象中。 结果如图： 查看target包下的代码，可以发现MapStruct除了两层映射外，还帮你自动生成了迭代集合添加元素的代码，从而实现集合元素的复制。 4.字段的逻辑处理 4.1.复杂逻辑处理（qualifiedByName和@Named） 这次我们需要把dto中的personName和personAge的list集合映射到实体类的masters集合中。常规的集合映射无法处理这种情况，这时需要使用到qualifiedByName和@Named进行特殊处理。 这就需要拿到两个list的数据，进行手动处理了。在@Mapping注解的qualifiedByName属性指定方法名定位处理逻辑的方法，@Named(\"dtoToMasters\")。 利用stream流进行处理。 返回结果： 4.2.额外逻辑处理（ignore和@AfterMapping） @Mappings的ignore属性，也可以对一个字段（不能是集合）进行额外逻辑处理。通常搭配@AfterMapping注解使用。 这个案例中，我们需要根据DTO的mount属性判断是否大于15，如果大于，则判断hotel实体类的isSuccess为true 编写converter，注意@AfterMapping注解下的方法的参数列表，需要使用@MappingTarget注解指明目标对象， 测试方法 返回结果 4.3.简单逻辑处理（expression） expression可以在注解中编写简单的处理逻辑 在这个案例中我需要在实体类的nowTime字段获取当前时间。 直接在expression属性中使用方法获取当前时间。 结果如下 "},{"title":"【项目开发】Java调用Python方法一文详解","date":"2024-04-26T08:23:20.000Z","url":"/2024/04/26/%E3%80%90%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E3%80%91Java%E8%B0%83%E7%94%A8Python%E6%96%B9%E6%B3%95/","tags":[["调用python","/tags/%E8%B0%83%E7%94%A8python/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"前言 在这个人工智能技术迅速发展的时代，对于我们学生而言，参加软件竞赛已不再是单纯的技术比拼。传统的纯Java编写项目，虽然有其稳定与高效的优势，但在面对日益复杂的算法需求时，其竞争力已逐渐减弱。因此，将Java与Python这两种编程语言的优势相结合，实现算法与软件的完美融合，已成为提升项目竞争力的关键。 本文将详细讲解使用Java调用Python的三大方法，并分析各个方法的优势。 1.jython库（不推荐） 首先在pom.xml中导入jython对应依赖 1.1.手动编写Python语句 这里我们编写一个简单的a + b函数的实现样例。 这样就可以得到返回结果 1.2.读取Python文件进行调用 编写一个jythonTest.py文件 使用PythonInterpreter.execfile方法调用py文件 总结： 上述两个方法的优点是其集成性，Jython允许你在Java中直接执行Python代码，使得Java和Python代码可以直接进行交互。 缺点也是明显的，首先，Jython说到底是Java的库，可能无法完全支持Python的所有库和功能，其次，Python工程师的工作高度耦合在Java代码中，如果你和你的Python同事不能够忍受这种开发方式，那么就不要用这种方法。 2.Java调用命令行（推荐） 这种方法的原理是通过Java代码调用操作系统的命令行接口，然后在命令行中执行Python脚本。 Java程序可以通过Runtime.getRuntime().exec()方法或者更高级的ProcessBuilder类来实现这一功能。执行Python脚本后，Java程序可以通过InputStream流来捕获并处理Python脚本的输出结果。 2.1.Runtime.getRuntime().exec()方法调用 首先需要编写执行Python脚本的命令行语句 使用Runtime.getRuntime().exec()方法执行命令 使用process.getInputStream()捕获InputStream流，从而获取执行结果 使用process.getErrorStream()捕获标准错误流（可选），如果Python语句报错会打印报错信息 等待进程结束 ，使用process.waitFor()方法获取返回码 完整代码为： 2.2.ProcessBuilder类调用（最推荐） 编写命令 使用ProcessBuilder启动进程 完整代码： 总结： 两种方法都是通过创建一个Process进程调用命令行获取Python脚本的执行结果。主要区别为第二种方法需要额外创建一个ProcessBuilder类，ProcessBuilder类接收的是String数组，相较于Runtime只接收一个String字符串，ProcessBuilder类编写命令更加灵活。 3.调用云端模型（最推荐） 这种方法需要Python工程师在百度云、腾讯云等云平台上开启接口，然后通过Hutool的工具类发送HTTP请求调用云端接口。 首先需要导入Hutool库 填写接口地址，使用Map作为表单数据（一般使用表单，比较灵活，文件和文本都可以传输） 使用HttpRequest发送请求 判断返回结果合法性，进行相关处理 获取返回结果 完整代码： 4.总结 jython库 优点： 直接在Java程序中执行，可以直接利用Java虚拟机（JVM）的性能优势，减少进程间通信的开销。 缺点： Jython可能无法完全支持Python的所有库和功能。 Java调用命令行 优点： Java和Python进程是独立的，这使得它们可以更容易地并行运行，而不会影响彼此的性能。 对于对硬件要求比较高的Python模型， 本地部署可能存在一定的困难。 对于开发人员比较友好，两者的开发工作分离。 缺点： 进程间通讯可能会引入额外的开销和复杂性。 调用云端模型 优点： 对于开发人员比较友好，两者的开发工作分离。 利于构建大型算法模型，如百度云等云端平台对于Python模型支持度很高，比起本地更容易部署。 缺点： 需要Java和Python工程师对HTTP请求有一定了解。 云端接口需要支付一定的费用。 "},{"title":"常见的设计模式","date":"2024-02-08T03:22:40.000Z","url":"/2024/02/08/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","tags":[["设计模式","/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"categories":[["设计模式","/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"]],"content":"单例模式 懒汉式 通常使用双重校验+锁的方式实现 volatile关键字可防止指令重排，比如创建一个对象，JVM会分为三步： 为singleton分配内存空间 初始化singleton对象 将singleton指向分配好的内存空间 指令重排：JVM在保证最终结果正确的情况下，可以不按照程序编码的顺序执行语句，尽可能提高程序的性能 而volatile关键字会防止指令重排，从而保证线程安全。 饿汉式 饿汉式在类加载时就创建好了对象，程序调用时可直接返回对象。 工厂模式 对于接口存在多个实现类时，如果使用if-else语句进行实例化接口的操作，不利于代码扩展，如果换用其他实现类很麻烦。 使用静态工厂模式可以灵活切换实现类，提高通用性。 这样只需调用工厂类的静态方法，传入指定参数即可实例化对象。 代理模式 实现原理： 实现被代理的接口 通过构造函数接收一个被代理的接口实现类 调用被代理的接口实现类，在调用前后增加对应的操作 比如，我们需要在调用代码沙箱前，输出请求参数日志；在代码沙箱调用后，输出响应结果日志，便于管理员去分析数据。 可以使用代理模式，提供一个Proxy，来增强代码沙箱的能力（代理模式的作用就是增强能力） 原本：需要用户自己去调用多次 使用代理后：不仅不用改变原本的代码沙箱实现类，而且对调用者来说，调用方式几乎没有改变，也不需要在每个调用沙箱的地方去写统计代码。 代理类： 使用方法： 策略模式 我们的判题策略可能会有很多种，比如: 我们的代码沙箱本身执行程序需要消耗时间，这个时间可能不同的编程语 言是不同的，比如沙箱执行 Java 要额外花 10 秒。 我们可以采用策略模式，针对不同的情况，定义独立的策略，便于分别修改策略和维护。而不是把所有的判题逻 辑、if ... else ...代码全部混在一起写. 定义一个策略接口，让代码更通用化 定义判题上下文对象，用于定义在策略中传递的参数 (可以理解为一种 DTO) 实现接口，实现各种判题策略 但是，如果选择某种判题策略的过程比较复杂，如果都写在调用判题服务的代码中，代码会越来越复杂，会有大量 if ... else ...，所以建议单独编写一个判断策略的类。 定义JudgeManager，目的是尽量简化对判题功能的调用，让调用方写最少的代码、调用最简单。对于判题 策略的选取，也是在JudgeManager 里处理的。 模板方法模式 定义一个模板方法抽象类。 先复制具体的实现类，把代码从完整的方法抽离成一个个子方法。 接着子类可以继承模板方法 java原生代码沙箱实现，可以直接复用模板方法定义好的方法实现。 docker代码沙箱实现，根据需求重写模板方法。 "},{"title":"【SpringSecurity】认证授权全流程实战","date":"2024-01-18T08:20:13.000Z","url":"/2024/01/18/%E3%80%90SpringSecurity%E3%80%91%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83%E5%85%A8%E6%B5%81%E7%A8%8B%E5%AE%9E%E6%88%98/","tags":[["身份校验","/tags/%E8%BA%AB%E4%BB%BD%E6%A0%A1%E9%AA%8C/"]],"categories":[["项目开发","/categories/%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91/"]],"content":"介绍 Spring Security是一个强大且灵活的身份验证和访问控制框架，用于Java应用程序。它是基于Spring框架的一个子项目，旨在为应用程序提供安全性。 Spring Security致力于为Java应用程序提供认证和授权功能。开发者可以轻松地为应用程序添加强大的安全性，以满足各种复杂的安全需求。 SpringSecurity完整流程 JwtAuthenticationTokenFilter：这里是我们自己定义的过滤器，主要负责放行不携带token的请求（如注册或登录请求），并对携带token的请求设置授权信息 UsernamePasswordAuthenticationFilter：负责处理我们在登陆页面填写了用户名密码后的登陆请求。入门案例的认证工作主要由它负责 ExceptionTranslationFilter：处理过滤器链中抛出的任何AccessDeniedException和AuthenticationException FilterSecurityInterceptor：负责权限校验的过滤器。 一般认证工作流程 Authentication接口：它的实现类表示当前访问系统的用户，封装了用户相关信息。 AuthenticationManager接口：定义了认证Authentication的方法 UserDetailsService接口：加载用户特定数据的核心接口。里面定义了一个根据用户名查询用户信息的方法。 UserDetails接口：提供核心用户信息。通过UserDetailsService根据用户名获取处理的用户信息要封装成UserDetails对象返回。然后将这些信息封装到Authentication对象中。 数据库 数据库的采用RBAC权限模型（基于角色的权限控制）进行设计。 RBAC至少需要三张表：用户表–角色表–权限表（多对多的关系比较合理） 用户表（user）：存储用户名、密码等基础信息，进行登录校验 角色表（role）：对用户的角色进行分配 权限表（menu）：存储使用不同功能所需的权限 注册流程 配置匿名访问 在配置类中允许注册请求可以匿名访问 编写实现类 registerDTO中存在字符串roleId和实体类user，先取出user判断是否存在相同手机号。若该手机号没有注册过用户，对密码进行加密后即可将用户存入数据库。 创建register方法映射，保存用户的同时也要将roleId一并存入关系表中，使用户获得对应角色。如下图。 登录流程 配置匿名访问 在配置类中允许登录请求可以匿名访问 调用UserDetailsServiceImpl 登录流程一般对应认证工作流程 先看这段代码：UsernamePasswordAuthenticationToken authenticationToken = new UsernamePasswordAuthenticationToken(user.getUserPhone(), user.getUserPassword());这里先用用户手机号和密码生成UsernamePasswordAuthenticationToken 再看这段代码：Authentication authenticate = authenticationManager.authenticate(authenticationToken);利用authenticate调用自定义实现类UserDetailsServiceImpl，根据用户名判断用户是否存在（对应认证流程的1、2、3、4） 实现UserDetailsServiceImpl 由于试下的是UserDetailsService接口，所以必须实现其方法loadUserByUsername（根据用户名查询数据库是否存在）这里我传入的是手机号。数据库中若存在用户，则返回UserDetails对象（这里的权限信息暂且不看，对应认证流程的5、5.1、5.2、6） UserDetails对象返回后，authenticate方法会默认通过PasswordEncoder比对UserDetails与Authentication的密码是否相同。因为UserDetails是通过自定义实现类从数据库中查询出的user对象，而Authentication相当于是用户输入的用户名和密码，也就可以理解为通过前面自定义实现类利用用户名查询到用户后，再看这个用户的密码是否正确。如果用户名或密码不正确，authenticate将会为空，则抛出异常信息。（对应认证流程的7） 由于这里的登录流程不涉及8，9，10，所以不再叙述。 在剩下的代码中我们利用用userId生成了jwt的令牌token，将其存入Redis中并返回token给前端。 登出流程 编写过滤器 除login、register请求外的所有请求都需要携带token才能访问，因此需要设计token拦截器代码，如下。 对于不携带token的请求（如登录/注册）直接放行；对于携带token的请求先判断该用户是否登录，即redis中是否存在相关信息，若存在，将用户授权信息存入SecurityContextHolder，方便用户授权，最后直接放行。 此外，还需将token拦截器设置在过滤器UsernamePasswordAuthenticationFilter的前面。 编写实现类 获取SecurityContextHolder中的用户id后，删除redis中存储的值，即登出成功。 授权流程 确保实现类正确编写： 在token拦截器中，我们添加了这段代码。 这样非登录/注册请求都会被设置授权信息。 为对应接口添加注解@PreAuthorize，就会检验该请求是否存在相关请求。 完整代码 config类 controller类 dto类 entity类 UserDetails的实现类 filter类 handler类 service实现类 utils类 "},{"title":"中国大学生服务外包创新创业大赛赛题分析","date":"2024-01-08T12:53:12.000Z","url":"/2024/01/08/%E8%B5%9B%E9%A2%98/","tags":[["赛题分析","/tags/%E8%B5%9B%E9%A2%98%E5%88%86%E6%9E%90/"]],"categories":[["比赛","/categories/%E6%AF%94%E8%B5%9B/"]],"content":"【A01】 基于文心大模型的智能阅卷平台设计与开发 技术栈 算法部分：PaddleOCR+NLP+文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 问题及解决方案 评阅效率与成本 问题描述：传统评阅面临评阅速度较慢、人力需求大的问题，特别是在大量试卷需要评阅时，评阅及反馈时效性能否被满足，将是一个巨大的挑战； 关键点：大量试卷、评阅及反馈时效性 解决方案： 批量评阅，选择、填空、判断题等机器评阅，计算题、简答题等主观题需人工阅卷。主观题可尝试利用自然语言处理（NLP）技术，让机器能够更准确地理解和评价学生的文字表达，将题干和答案输入给文心大模型，由大模型给阅卷老师提出建议辅助阅卷。 分配任务，将试卷任务划分为多个子任务，由多个评阅者同时处理，以提高评阅速度；将试卷任务分配给多个评阅者，多个评阅者负责不同数量的试卷。 实时反馈，设计评阅者和学生两个角色，让评阅者或学生能够迅速获取评阅结果并及时进行反馈，如老师评阅完后生成时间等数据、试卷评阅完后发送信息给学生，除分数外还可以包括评语和建议。学生端可以设计往期的数据统计、教师端可以设计往期评阅趋势等。 有阅无评 问题描述：在实际评阅场景中。往往需要更具有针对性和个性化的评价和建议，而不仅仅是分数反馈。在这过程中，评阅者需要将关键概念，结合学生自身特点进行充分关联，提供更贴近学生需求的知识延展和学科建议； 关键点：针对性和个性化的评价和建议、关联学生特点、提供知识延展和学科建议 解决方案： 反馈设计，评阅者可以输入关键概念给文心大模型，基于文心大模型给出专业评价和建议，或者评阅者自行写出评价和建议。学生端收到反馈后，可以使用文心大模型提取反馈中的知识，并给出学习建议。 增加多样性，包括提供更多的学科相关资源链接、在线学习平台等。这可以帮助学生更全面地理解和拓展相关知识。 学情信息跟踪实际 问题描述：评阅者在实际操作中难以全面、数字化地记录学生的评价和进步情况，进而影响评估的针对性和指导性作用； 关键点：全面、数字化记录学生的评价和进步情况 解决方案： 学情数据可视化，评阅者可以看到该学生历届考试的试卷、评阅建议或成绩趋势图等。 实际评阅风格的单一 问题描述：在实际评阅过程中，评语描述表达方式相对统一，难以通过多样化的评语风格来更好地引导学生理解知识点和提高学业水平； 关键点：多样化的评语风格 解决方案： 通用评语库， 构建一个通用的评语库，包含常见的评价和建议，评阅者可以从中选择或修改，以提高评价的一致性和效率。还可以提供评阅者多样化的评语模板，包括鼓励性的、建议性的、肯定的、挑战性的等不同类型。评阅者可以根据学生的表现灵活选择合适的模板，使评语更富有变化。 学科限制与切换 问题描述：由于不同学科具有独特的评价标准和要求，传统评阅工具难以在不同学科间切换，需要更多的学科专业性和差异化的评阅方式。 关键点：更多的学科专业性和差异化的评阅方式 解决方案： 学科试卷分配给对应的学科老师 学科专业化的大模型支持， 集成不同学科领域的专业化大模型，使其能够理解和应用特定学科的术语、概念和评价标准。这有助于提高评阅的准确性和专业性？？？ 任务清单 （1）试卷图像快速采集与存储； （2）字符识别与提取； （3）内容理解与评阅内容生成； （4）评阅内容的二次编辑； （5）评阅结果的可视化、整理与导出； （6）学情数据可视化； （7）跨平台支持； （8）实时采集与分析（可选）； （9）其它拓展功能和创新方向，如软硬一体解决方案。 功能模块 试卷导入模块 评阅分配模块 智能评阅模块 学情分析模块 个人信息模块 【A15】基于知识图谱的大学生就业能力评价和职位推荐系统 技术栈 算法部分：知识图谱、NLP、推荐和预测模型、文心大模型 后端部分：SpringBoot+Mybatis-Plus+Mysql+Redis+SpringSecurity 前端部分：Nginx+Vue 其他：Git 参考网站 老鱼简历： 牛客： 数据集 链接： 提取码：vmeh 技术要求与指标 推荐有效性达到80% 以上（用户调查：电子或计算机类相关专业毕业生简历与岗位样例库进行匹配）； 系统数据库中个人隐私信息（姓名、手机、邮箱、通讯地址等至少4项）进行加密，只能被授权人员解密； 架构设计上可并发支持1000人以上同时在线使用，推荐响应时间在5s以内； 技术不限，开发工具不限，可采用开源技术。 业务背景 知识图谱构建 职位推荐系统需要构建一个包含相关职位、技能要求、行业信息等多个领域知识的知识图谱。 聚合和分析用户信息 与知识库相结合，对用户的个人简历、求职意向、工作经验等信息进行聚合和分析处理。根据用户提供的信息学习用户的兴趣和特征，并利用这些特征在知识图谱上匹配职位需求和其他相关信息。 建立推荐模型 根据用户信息设计推荐算法和预测模型。通常来说，推荐算法有协同过滤、关键因素模型、深度学习模型等，具体选择应该根据实际情况来确定。 推荐结果展示 将推荐结果呈现给用户，并支持用户进行选择和反馈。需要注意的是推荐结果的呈现方式也应该根据不同用户群体的需求和喜好来定制化。 问题及解决方案 实现一套大学生就业能力评价和智能岗位推荐系统，根据提供的岗位信息样例库，设计一套含智能算法的软件系统方案：基于知识图谱的岗位信息，利用职位和用户信息，结合推荐算法和相关技术，为用户提供符合其需求和兴趣的职位推荐结果和能力评价结果。 能力评价 问题描述：用户上传个人简历，并明确自己期望的职位，系统自动判断用户与期望职位间的契合度，差异性，给出提升建议，让其知晓对于意向岗位自身知识和技能上的缺陷，找到短板，有针对性提升自我能力。 关键点：判断用户与期望职位间的契合度、给出提升建议 解决方案： 信息提取，利用自然语言处理（NLP）技术提取用户简历和期望职位中的关键词和技能。设计算法计算技能匹配度分数，考虑关键技能的重要性和权重。 构建知识图谱，基于构建的知识图谱，将用户的技能与职位要求进行匹配。利用图谱中的关系和节点信息，评估用户对于职位所需知识的覆盖程度。 差异性分析，分析用户的简历和期望职位之间的差异，包括技能、工作经验、项目经历等方面。 提升建议生成，基于差异性分析和匹配度评估，生成个性化的提升建议。针对用户的短板提供培训建议、学习资源链接、实践项目建议等，以提高其在期望职位上的竞争力。 用户反馈机制，提供用户反馈功能，让用户对系统的匹配度评估和提升建议进行确认。 岗位推荐 问题描述：用户上传个人简历，系统自动分析简历内容，生成推荐职位，用户可以给出推荐是否有效的反馈。如果不满意，可修订简历部分内容，重新进行推荐。 关键点：推荐职位 解决方案： 简历内容分析，利用自然语言处理（NLP）技术对用户上传的简历进行内容分析。 用户兴趣，基于用户上传的历史简历数据，建立用户的兴趣和偏好模型。考虑用户之前的工作经验、求职意向、职业发展方向等信息。 职位推荐算法设计，选择合适的推荐算法，如协同过滤、基于内容的推荐、深度学习模型等。结合用户的兴趣模型和简历特征向量，计算与不同职位的匹配度。 修订简历，如果用户不满意推荐结果，系统应提供修订简历的功能。用户可以编辑、添加或删除简历中的信息，系统重新分析并生成更新后的推荐职位。 用户历史记录查看，用户可以查看自己历史上传的简历和推荐结果，了解职业发展轨迹。 招聘推荐 问题描述：企业招聘人员输入或上传职位要求，系统自动分析匹配求职人员简历，筛选出符合期望的人员列表。 关键点：招聘推荐 解决方案： 职位要求解析，利用自然语言处理技术，对企业输入或上传的职位要求进行解析。提取关键技能、经验要求、学历等信息，构建职位要求的特征向量。 求职人员匹配度计算，基于已有的用户简历数据，计算每个求职人员与职位要求的匹配度。利用算法综合考虑关键技能匹配、工作经验匹配等因素。 候选人员筛选，根据匹配度计算结果，筛选出符合职位要求的候选人员列表。设置合适的匹配度阈值，确保选出的人员满足企业的期望。可以设计系统推荐排名，推荐排名靠前的人员供企业参考。考虑推荐结果的多样性，确保涵盖不同技能和经验背景的候选人。 候选人员详细信息展示，提供候选人员的详细信息，包括简历、技能、工作经历等。支持企业预览候选人员的综合素质，以便更好地做出招聘决策。 用户期望 对开发的产品方案期望如下： （1）算法优化合理，求职与招聘推荐结果与用户期望一致性高； （2）胜任度能力评价结果合理，给出的提升建议符合用户短板； （3）保护简历中个人数据的安全，不侵犯用户隐私； （4）扩展功能：可以对注册用户的数据和操作行为进行统计分析，给出热门职位、热门技能、热门专业等的一些趋势图等。 功能模块 简历上传模块 能力评价模块 岗位推荐模块 招聘推荐模块 热门推荐模块 个人信息模块 队伍分配 A01：周锦辉+罗骏岚+李鑫+杨康庆+禹乐 A15：李翔+李慧聪+周亚+朱豪尔+郑国盛 "},{"title":"【IDEA结合Git实现项目管理实战】四、git冲突篇","date":"2024-01-04T10:21:22.000Z","url":"/2024/01/04/%E5%9B%9B%E3%80%81%E8%A7%A3%E5%86%B3%E4%BB%A3%E7%A0%81%E5%86%B2%E7%AA%81/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 在使用Git进行项目管理时，代码合并是一项常见而重要的操作。本文将重点探讨两种常用的代码合并操作：合并（merge）和变基（rebase）。在进行代码合并时，我们难免会遇到Git冲突的情况。本文也将通过举例详细介绍如何通过IDEA使用Git进行合并或变基操作时可能遇到的代码冲突情况，并提供解决方法。 什么是git冲突 在多分支并行处理时，每个分支可能基于不同版本的主干分支创建。如果每个分支都独立开发而没有进行代码合并，自然不会出现代码冲突。但是，当两个分支同时修改同一文件时，在代码合并时就会出现冲突。 下图为两个分支分别使用合并/变基操作解决冲突后的提交树。 解决git冲突 介绍完冲突出现的原因，那么如何解决冲突呢？在解决git冲突时，我们需要确定以哪个分支的文件版本为准，或者取两个分支的文件的部分片段进行整合。 IDEA提供了强大的冲突解决功能，供用户处理git冲突。下面将进行详细介绍。 当前分支dev1的代码： 目标分支dev的代码： 我们现在的目标是让两个分支合并后的代码中同时出现method1、method2、access和acess2这四个方法。 执行合并后，出现界面： 左侧为当前分支dev1的提交记录，中间为合并前的预览结果，右侧为目标分支dev的提交记录。 其中红色区域为代码存在差异的部分。 先来看第一块红色区域的中间部分的代码。大家一定会疑惑预览结果中出现这段代码是什么意思？为什么会出现报错呢？ 这里其实是git对于左右侧存在差异的代码的标记。符号&lt;&lt;&lt;&lt;&lt;&lt;&lt; xxx的下方是左侧存在差异的代码，符号&gt;&gt;&gt;&gt;&gt;&gt; xxx的上方是右侧存在差异的代码，比如&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD箭头所指方向也就是我们当前分支的方向（在左侧），在该箭头下面的部分是当前分支的与目标分支的差异代码，这里因为左侧比右侧少了一段代码，因此下面啥东西没有；=======代表分割符号，该分割符号的下面就是目标分支的代码，即import java.util.concurrent.TimeUnit；&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev也就代表目标分支的方向（在右侧） 那么如何解决冲突呢？对于我们的目标来说，我们的输出语句自然不需要导入这个包，因此把语句import java.util.concurrent.TimeUnit;给删除即可。 点击左侧的箭头符号，可以把中间区域被替换成左侧的红色区域（那根细线，也就是没有代码）。 点击后中间区域消失。 再来看第二个红色区域，根据我们的目标，我们要将这四个方法都添加进入中间区域。 先点击左侧的箭头。可以发现中间区域被替换为左侧代码，右侧向左箭头变成了向左下箭头。 这个向左下的箭头代表将右侧的代码添加到中间代码的下方。 点击后如图： 那么一切就大功告成了，冲突解决成功，点击应用按钮。 git提示还有冲突未处理，这是为什么？ 把界面翻到上面，发现这个红色区域还没有处理，我们点击那个查号，作用是将冲突标记为已解决。 这时IDEA提示所有变更已被处理，那么我们就可以放心大胆的合并了。 合并成功！ 合并/变基详解 合并（git merge） 当前分支和目标分支执行合并操作时，Git会将当前分支的最新提交记录与目标分支的最新提交记录合并，并在当前分支形成一个新的提交记录。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行合并操作，dev中的提交记录添加到了分支dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行合并操作，git给出提示（已是最新 删除dev），当前分支dev1没有变动。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行合并操作，在目标分支dev1中生成一个新的提交记录Merge branch 'dev' into dev1，该提交记录包含了这两个提交记录的变更，如图。 在提交树中，可以看到两个提交记录合并为一个记录。 变基（git rebase） 当前分支和目标分支执行变基操作时，Git会将目标分支的最新提交记录依次应用到当前分支的每个新的提交记录中。 示例1 当前分支为dev1，目标分支为dev，目标分支dev中存在两条当前分支dev1分支没有的提交记录。 执行变基操作，dev中的两条记录添加到了dev1中。 示例2 当前分支为dev1，目标分支为dev，当前分支dev1中存在两条目标分支dev分支没有的提交记录。 执行变基操作，没有发生变化。 示例3 当前分支为dev1，目标分支为dev，当前分支dev1中有新的提交记录添加测试类，目标分子dev中有新提交记录添加新文件（该示例由于都是添加新文件，没有对同一文件进行更改，因此不存在代码冲突） dev1中添加了一个JavaTest文件。 dev分支中添加了一个test.lua文件。 执行变基操作，dev分支的提交记录添加到了dev1分支中。 总结 可以发现，无论是对于合并还是变基操作的示例1和示例2，最终执行操作后的结果都是一样的。对于合并操作，git将两个分支进行合并，最后生成一个新的提交记录，提交树存在交叉。对于变基操作，git将目标分支的提交记录应用到当前分支，提交树仍然是线性的。如图所示。 至于在实际开发中选择合并还是变基，还是看个人喜好了。 代码冲突示例 注意：本文为方便理解，所有示例均简单的修改项目中的md文件，实际开发中可能存在对多个文件的冲突，但万变不离其宗，只要你具备了解决单个文件代码冲突的能力，那么多个文件的冲突也能轻松应对。 合并/变基分支1 分支情况，当前dev1的两个提交记录博文1和博文2都在dev的提交记录博文3之前，其余分支一样 时间顺序：博文1-&gt;博文2-&gt;博文3 合并 此时合并有代码冲突，解决这个冲突。 发现该冲突只针对博文2，也就是最后一个提交记录 变基 该冲突为博文1和博文3的冲突 该冲突为变基后的博文1和博文2的冲突 合并/变基分支2 分支情况： dev中的博文3在dev1中的博文1和博文2之间 时间顺序：博文1-&gt;博文3-&gt;博文2 合并 基于上述情况，合并分支存在代码冲突 在代码冲突中，存在博文2和博文3的冲突， 冲突解决后如图所示。 这里紫色因为博文3是属于别的分支过来的，其父提交是add README.md.。所以从add README.md.出发，与dev1原本的提交记录博文2结合形成一个新的提交记录Merge branch 'dev' into dev1 结论：分支以时间顺序进行排序，合并分支永远是两个分支的最后一个提交历史进行合并。 变基 博文1和博文3存在冲突 冲突解决后，选择提交消息不变 依然存在冲突 可以发现该冲突来自于已经变基的提交博文1和之后的博文2 得到变基后的提交树 合并/变基分支3 分支情况： dev1中的两个提交记录博文1和博文2在dev中的博文3提交之后 时间顺序：博文3-&gt;博文1-&gt;博文2 合并 博文2和博文3存在代码冲突 变基 冲突来自于博文3和博文1 冲突来自变基后的博文1和博文2 总结 通过这三个代码冲突的示例，看到区别了吗？ 在合并操作时，冲突通常发生在两个分支的最新提交记录上。这是因为合并是将两个不同的分支合并为一个，而最新的提交记录是两个分支的端点。如果两个分支都对同一文件进行了修改，Git 无法确定应该选择哪个更改，因此会产生冲突。 在变基操作时，冲突可能发生在当前分支的提交记录和目标分支的提交记录之间的每个提交记录上。这是因为变基是将一系列提交应用到另一个分支上，而不仅仅是最新的提交。如果两个分支都修改了相同的文件，冲突可能会在每个提交记录上发生，而不仅仅是最新的提交。 总的来说，冲突是由于两个分支都对同一文件进行了修改，而 Git 无法自动解决冲突的情况下发生的。在合并操作中，冲突通常发生在最新的提交记录上；在变基操作中，冲突可能发生在多个提交记录上。"},{"title":"【IDEA结合Git实现项目管理实战】三、实战篇","date":"2023-12-15T04:01:20.000Z","url":"/2023/12/15/%E4%B8%89%E3%80%81%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%AE%9E%E6%88%98/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的实战篇，将正式介绍如何使用IDEA结合Git进行项目管理。 注意：本文假设你已经成功在IDEA中配置了git 在配置篇中，当我们已经在本地推送/克隆了一个项目后，我们能看到如下两个功能块。 在本文中，我们称红色箭头所指为git日志，蓝色箭头所指为git工具栏。 设置默认添加文件 还记得基础篇我们提到的四个工作区域吗？如果我们想要将变更的文件推送到远程仓库，我们首先需要先保证变更的文件在暂存区中。 在基础篇中，我们提到每当创建一个文件时，idea都会询问我们是否将文件添加到git，即是否添加到暂存区中。为了避免这种麻烦，我们打开设置，按以下流程选择无提示添加，这样idea就会默认将我们创建的文件自动添加到git了。 配置完该设置后，相当于每次创建文件都会默认执行git命令git add，作用是将文件添加到暂存区中。 推送更改的文件到本地库 当我们在本地完成了我们的工作后，我们可以先将变更的文件提交到本地库中保存。 这里我们点击蓝色箭头所指git工具栏的第二个按钮，即提交按钮。 该按钮对应git命令git commit，作用是将文件提交到本地库中。 这里我简单的修改了下我自己的更改，填写完提交信息后点击提交。 如果点击提交并推送的话，就可以直接将变更推送到远程仓库，省去了一个步骤。 提交并推送按钮对应git命令git commit &amp; git push，作用是将暂存区的变更直接推送到远程仓库 提交完毕后，我们打开git日志，能在当前的本地分支中看到我们刚才提交记录： 将本地库的提交记录推送到远程库中 当我们已经确定本地库的工作已经结束后，我们就可以将本地库的记录推送到远程库中。 我们点击git工具栏的第三个按钮，即推送按钮。 该按钮对应git命令git push，作用是将本地的提交记录推送到远程库中。 这里我准备了两个提交记录，按ctrl + 鼠标左键可以多选提交记录。 点击推送按钮后，我们可以在git日志中看到远程仓库中出现了我们刚才推送的提交记录。 此时我们打开gitee，可以发现提交记录同样生效于gitee中。 拉取远程分支的变更 当你的远程仓库发生改变时（你的队友推送了提交记录到远程仓库中），而你自己的远程分支不会自动拉取别人的提交记录，也就是说你的远程分支不具备别人的提交历史。 此时我们需要点击提取所有远程按钮，这时候就可以把远程仓库的所有本更拉取到自己的远程分支中。 提取所有远程按钮对应git命令git fetch，作用是将远程仓库的变更拉取到远程分支中。 这里我在gitee中手动添加了一个README.md文件。 点击按钮后，可以看到远程分支中增加了这条记录，同时，我们可以注意到本地分支名后出现了向下的蓝色箭头，这代表着当前远程分支已经更新了，本地分支也应该进行更新。 我们选中对应的本地分支，点击更新所选内容按钮，就可以把远程分支的提交记录添加到我们的本地分支中。 这里我们也可以选择git工具栏的第一个按钮，这样就可以指定方式传入变更。 该按钮对应git命令git merge或git rebase，作用是传入变更到当前分支 结果如下 新建分支 正常来说我们开发项目肯定不可能只用一个本地分支，一般会在其他分支中开发完毕后再推送到主分支中，这时候就需要新建分支的操作了。 分支可以简单的理解为我想基于这个提交以及它所有的父提交进行新的工作。当我们新建分支时，Git就会将HEAD指向的提交记录以及该提交记录之前的所有提交记录保存到新分支中。 如果我们要新建分支，可以点击左侧工具栏的加号按钮，这样就得到了一个新的分支。 该按钮对应git命令git branch，作用是基于当前提交历史创建新的分支。 新建后我们得到了一个全新的分支dev，他包含了master的全部提交历史。 当然，由于HEAD当前指向的是最后一个提交记录，所以新分支dev就包含了master的全部提交记录。如果我们要基于测试1这个提交历史创建分支的话，需要右键该提交历史，点击新建分支后再输入分支名并创建即可。 这样我们就得到了一个包含测试1之前所有提交记录的分支。 回到指定的提交历史 假设你当前开发的代码出现了问题，如何找回之前的代码？这时候就可以通过签出这一git提供功能回到之前的提交历史。当你签出到一个提交历史时，你就获取到了这个提交历史的所有代码。 注意：在签出前确保你当前的代码已经提交到了本地或者远程分支中，签出到其他提交记录时IDEA不会帮你自动保存当前的代码。 在签出的操作中，我们必须要明白HEAD这一概念，HEAD 是一个指向当前所在分支的指针，或者是指向当前所在提交记录的指针。在IDEA中，HEAD的位置可以通过黄色便签来看到，比如在下图master的提交历史中，HEAD指向的分支就是master，同时指向提交消息为add README.md.的提交记录。这里我们将下图称为图1。 如果我们要切换到测试1的提交历史上，右键测试1，点击签出修订。 签出修订按钮对应git命令git checkout，作用是将HEAD指针转移到当前分支/提交记录。 可以看到黄色便签转移到了测试1上，证明我们当前在测试1的提交历史上，这时我们就可以基于测试1的代码进行修改了。这里我们将下图称为图2。 但是这里有个问题，为什么图2在master分支上的黄色便签消失了？而在图1中黄色便签既指向了master又指向了最后一个提交记录add README.md. 这里要明白一个原理，那就是分支本身也能看作是一个指针，这个指针恒指向该分支的最后一个提交记录。 那么在图1中，HEAD其实是通过指向分支进而指向了该分支的最后一个提交记录，即HEAD-&gt;master-&gt;add README.md.。而在图2中，HEAD被称作游离的HEAD，是因为它指向的并不是分支而是该分支的一个提交历史，自然就不会指向master分支了，即HEAD-&gt;测试1。 基于指定提交历史修改代码 在上一个回到指定的提交历史的操作中，我们通过签出的操作获取到了指定的提交历史的代码，这时我们就能够基于这个提交历史的代码进行开发。 举个例子，假设我们要回到之前刚添加md文件的提交历史上进行代码开发，给md文件进行修改。我们当前分支的最新提交为博文3，如何回到提交历史add README.md.上？ 第一步我们肯定要签出到add README.md.上，签出后可以发现项目文件回到了刚开始提交md文件的时候。 这时我们对md文件进行修改，这里我删除了一些段落。 然后点击提交按钮，将这个修改历史666提交到本地分支中。 此时Git给出了警告。如果我们无视警告，仍然点击提交按钮，就会发现原本存在于add README.md.提交记录的指针消失了，而且我们修改后的提交记录666也并没有被提交到本地分支中。 还记得之前我们提到的游离的HEAD吗？如果直接在特定提交上修改代码并运行 git commit，这实际上会在游离的 HEAD 状态下创建一个新的提交，而不会创建分支。这样的操作可能会导致出现一个游离的提交，Git 的垃圾回收机制可能会删除这些提交。 解决方法1 一种方法是：我们在指定提交历史上创建一个分支，然后在新分支上开发完毕后进行提交。这两步操作前文已经详细介绍了，不再赘述，如图所示。 解决方法2 第二种方法则是：对指定提交记录执行git revert命令，该命令会基于指定提交记录创建一个新的提交历史 我们可以右键一个提交记录，选择还原提交选项。 如果当前分支的提交记录和还原的提交记录的文件存在差异的话会出现代码冲突。这里因为我当前分支的第一个提交记录博文3和add README.md.在md文件上存在差异，所以出现了冲突。 由于我们当前的目标是回到add README.md.这个提交记录的代码中，所以我们选择忽略来自其他提交的变更，保留add README.md.提交记录的原本代码即可。 写好提交消息后点击提交 可以发现我们这样就创建了一个和add README.md.一模一样的提交记录了。 接着我们可以基于这个提交记录的代码进行开发，这里前文已经详细介绍了，不再赘述。 将远程分支拉取到本地 设想这样一个场景：你的同事创建了一个新的远程分支并做了一个新的功能，而这个分支是你本地没有的，你现在的工作要基于这个新的功能才能进行下去，那么我们就需要把这个远程分支拉取到本地来。 在这个例子中，你的同事创建了远程分支origin/dev，而我们本地并没有与其对应的分支。 我们只需要右键该分支，点击签出，即可将该远程分支拉取到本地。 代码整合 在项目合作中，将其他人的代码整合到自己的代码中是经常用到的操作，这时就需要利用到Git的合并或变基功能。 一般来说代码整合会遇到以下几种情况： 当前分支正好比其他分支少了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，当前dev1中没有dev中的add README.md.的修改记录。首先确保我们当前分支在dev1分支上。 我们右键dev分支，选择变基或合并均可。 可以看到dev的提交记录整合到了dev1中。 当前分支和其他分支都修改了几条记录 如图，当前我们自己的代码在dev1分支中，我们要整合来自dev的代码，dev1中我写的md文件的内容较少。 在dev中我的队友写的md文件的内容较多。 现在一样要确保当前分支在dev1中，并且右键dev分支选择合并/变基来整合代码。这个时候就出现了冲突，大家应该很容易就想明白了，因为我当前分支和我队友的分支都同时存在文件名相同、内容不同的文件，这个问题不解决的话自然无法整合。 解决冲突的话就需要你和队友进行协商。比如这里我的分支dev1就比较少，队友的分支dev写的比我详细，所以我可以进行\"妥协\"，直接用队友的md文件即可，点击接受他们的这个按钮就可以使用队友的md文件了。 当然，我们也可以不进行妥协，IDEA提供了强大的修订功能，通过点击合并按钮，我们可以选择整合该文件的特定部分。具体如何操作，大家自行练习吧~ 删除提交 我们在实际开发中难免会提交一些无用的提交记录，这时需要利用删除提交的操作。 右键一个提交记录，选择删除提交选项，即可删除。 很简单吧？但要注意已经推送到远程分支的提交是不可删除的。 这里的绿色提交记录代表还未推送到远程分支的提交记录，棕色代表已经推送到远程分支的提交记录。 "},{"title":"【IDEA结合Git实现项目管理实战】二、基础篇","date":"2023-12-10T08:20:10.000Z","url":"/2023/12/10/%E4%BA%8C%E3%80%81Git%E7%9A%84%E5%9B%9B%E4%B8%AA%E5%8C%BA%E5%9F%9F/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的基础篇，将简要介绍使用Git所应该知道的最基本的知识，因此不算深究原理，哈哈。 注意：本文假设你已经成功在IDEA中配置了git Git的四个工作区域 基本概念 Git本地有三个工作区域，分别是工作区、暂存区、本地仓库 Git远程有一个工作区域，叫做远程仓库 工作区：开发者当前工作的目录，主要包含项目的实际文件，其中可能包括未进行版本管理的新文件和已修改的文件。 暂存区：作为一个临时存储区域，用于存放工作区中已经修改的文件的快照，以便将它们作为一个逻辑单元提交到本地仓库。 本地库：包含了项目的提交历史，开发者可以对其进行各种操作，如分支合并、变基、签出等。本地库是在开发者本地计算机上的存储库。 远程库： 存放在代码托管平台（如Gitee、GitHub）上的仓库，包含了已推送的代码版本、分支、标签等信息。开发者可以通过推送和拉取操作与远程库进行交互，以保持代码同步。 概念解释 工作区 一下子引入了这么多概念，可能大家会有点接受不了，那么这里对以上概念进行详细解释。 具体来说，工作区可以看做你在IDEA中打开的项目目录，如下图： 这里的项目目录就可以看作是工作区，但是在这个工作区中，既存在正常颜色（白色）的文件名，也存在红色、绿色和蓝色的文件名，那么这些文件分别代表什么含义？这里的文件的颜色，其实也就对应着上文所谓未进行版本管理的新文件和已修改的文件。如下表： 颜色 含义 白色 在当前提交历史未改动或者已经提交到本地库的文件 红色 未进行版本管理的新文件，也被称为未跟踪的文件 绿色 已经进行版本管理的新文件，该文件已被添加到暂存区中 蓝色 当前工作区中被修改的文件，该文件已被添加到暂存区中，与绿色相似 红色文件名、绿色文件名实例 大家可以自行测试下，如果你直接在项目目录中创建一个文件的话，IDEA会做出提醒： 这时如果你选择添加的话，该文件就会变成绿色文件从而被Git进行版本管理，如果选择取消的话，说明你不希望该文件被Git管理，那么该文件就是红色文件而不能被提交到暂存区。 总结： 当你创建一个新文件时，它就是红色的，表示这是一个未跟踪的文件。通过运行 git add 命令，将文件添加到暂存区，此时文件变为绿色，表示它已经被 Git 管理并准备提交。 如果你不想将一个未跟踪的文件或已修改的文件纳入版本管理，你可以选择不使用 git add 命令，或者使用 git reset 命令来取消已经添加到暂存区的文件。取消后，文件会回到红色状态，表示它未被跟踪或未被修改。 但是对于我们日常开发来说，我们创建一个文件肯定是有用意的，一般都希望该文件被提交上去，所以我们一般都选择添加文件，这样才能通过添加到暂存区，再到本地库最终推送到远程仓库中。 白色、蓝色文件名实例 比如这里我删除了md文件中的其中一行，md文件的文件名由白色转为了蓝色 该文件一般在该项目已经提交到本地库中后，我们准备开发新的功能时才会出现，也就是我们修改了当前已经存在于本地库中的文件，该文件就会被转化成蓝色文件。因为该文件已经被Git版本管理过了，所以可以直接提交到本地库。 总结： 蓝色文件可以直接提交到本地库中，因为其已经被Git版本管理了 蓝色文件可以简单认为就是白色文件被修改后的文件 暂存区 经过前面的内容，大家多少应该可以感觉到，暂存区其实是我们看不见、摸不着的存在，在暂存区中存在的文件一般是绿色和蓝色文件，也就是我们已经使用git add命令添加的文件。 当我们开发完毕一个新的功能后，我们就会准备将这次修改记录提交到本地仓库，这时候使用git commit命令就可以将暂存区中的文件提交到本地仓库中。 提交实例 在该实例中，我们删除了md文件的其中一行（可以看到changes一栏中是蓝色文件，如果你把一个新的文件也添加到暂存区的话就是前文提到的绿色文件），这里我们要在下方框中填写相关介绍，以便帮助其他团队成员或日后的自己知道这次的提交做了什么。 本地库 具体来说，本地库包含了开发者本地完整的提交历史和所有的本地分支，本地库中的提交记录是暂存区通过执行git commit后得来的。 这里要注意本地分支和本地库的区别，本地分支只是本地库众多分支的其中一个分支，当然本地库也可以只包含一个本地分支（但是一般来说开发者不会这么做）。 本地分支实例 这里展示了本地库中的master分支，在这个分支中存在着该分支的提交历史和各种信息。 远程库 远程库是存放在代码托管平台（如Gitee、GitHub）上的仓库，本地库执行git push命令可将本地的提交记录推送到远程仓库中。 远程分支和远程库的区别同本地分支与本地库的区别一样 远程分支实例 同上本地分支实例。 需要注意的是，由于我们在团队开发中涉及到多人的共同协作，因此每个人都可以向远程仓库中推送自己的代码，也就意味在当你在开发新的代码时，远程仓库可能已经发生了变动，那么远程分支也就不能实时和远程仓库的提交记录保持同步了，这时我们需要使用git fetch命令，将远程仓库的提交记录拉取到的远程分支中。 总结 通过对于工作区域的认识，大家想必已经了解了一个文件如何从工作区一步步提交到远程仓库中，当然这里存在着很多操作中的细节，如切换分支，合并来自其他分支的结果等，而我们是使用IDEA结合Git实现版本管理，同样存在着如何具体使用的问题。由于本篇只讲理论，就不过多介绍了，本系列将会继续更新下去，大家敬请期待~"},{"title":"【IDEA结合Git实现项目管理实战】一、配置篇","date":"2023-12-07T02:30:29.000Z","url":"/2023/12/07/%E4%B8%80%E3%80%81IDEA%E9%85%8D%E7%BD%AE/","tags":[["项目管理","/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"]],"categories":[["Git","/categories/Git/"]],"content":"前言 本系列将结合我个人参与团队协作开发项目的经验来介绍如何使用IDEA结合Git实现项目管理，因此可能与真正的企业开发协作存在差异，且文章所涉及的解析可能存在个人理解与实际的偏差。 本系列主讲如何具体操作，因此对于Git内部的原理将不会过多深究。 本文严禁任何形式的转载、搬运！ 本文作为该系列的配置篇，将介绍如何使用IDEA整合git，从而实现项目管理。 本文将使用Gitee作为项目管理工具。 注意：本文假定你已经拥有了一个Gitee账号并已经配置了密钥。 下载插件 打开设置 在插件中搜索Gitee并下载安装，安装完毕后IDEA会提醒重启IDE，重启后插件才会生效！ 添加账号 IDEA重启后，再次打开设置，在版本控制中可以看到Gitee这一栏，点击加号添加账号 点击加号后我们选择Log in via Gitee 授权完毕后点击应用。 项目管理 最后一个步骤就是正式的实现项目的版本控制了，实现这一步骤有两种操作。 第一种是将IDEA本地的项目上传到gitee中 第二种是从远程clone一个仓库到IDEA本地中 下面我们来逐个介绍这两种操作，并简述这两者之间的区别和使用场景。 1.将IDEA本地的项目上传到Gitee中 现在我们打开你想要托管给Gitee的项目，打开工具栏的VCS，点击Share Project on Gitee 这里我们可以设置仓库名（Repository name），Remote是远程分支名，可以不用修改，Description是这个仓库的描述，这里自己填写即可。 填写完毕后点击share，弹出这个窗口。 我们需要在本窗口中添加需要进行版本管理的文件以进行初始化，可以看到，本项目的所有文件都是理论篇提到的红色文件名的文件，这是因为这个项目还没有上传到远程仓库，也就不存在被Git跟踪的文件，所以都是红色文件名。 这里我们填写下提交信息并点击添加按钮，就可以上传成功了。 上传完毕后，我们可以观察到IDEA中出现了这三个功能块，至于这些功能块有何具体作用，我们将在基础篇详细介绍，这里不过多解释了。 这时候我们可以打开gitee的网站，点击顶部工具栏的头像，选择我的仓库，就可以看到刚刚创建的仓库了! 2.从远程clone一个仓库到IDEA本地中 这一步我们将从远程仓库中clone一个项目到本地中。 我们在gitee中选择一个想要clone的远程仓库，这里我使用的是我自己的远程仓库：  点击VCS，选择从版本控制中获取。 在仓库URL中，在URL中粘贴我们刚才复制的HTTPS地址，在目录中选择我们想要放置远程仓库代码的本地地址，填写完毕后点击克隆。 注意：目录必须是一个空目录 等待克隆完成后项目会自动跳转到你选择的目录。 总结 通过以上的介绍，我们了解到实现版本控制有两种操作： 第一种操作适用于以下场景： 本地你已经开发好了项目，需要将项目托管给远程仓库。 第二种操作适用于以下场景： 团队已经有了远程仓库（有人已经将远程仓库创建好了），这时候我们直接clone即可。 在gitee或github中看到了优质项目，我们clone本地进行学习 "},{"title":"Redis持久化问题排查","date":"2022-10-29T12:49:50.000Z","url":"/2022/10/29/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"Redis持久化问题排查 前言：在学习黑马的Redis持久化课程时因为老师用的MobaXterm软件与我使用的VMware存在差异，同时，老师上传的视频的redis配置与我大为不同而导致了一系列的问题，在进行了一天的排查（死杠）后终于解决了问题，感慨良多。在遇到问题解决不了的时候还是要多回头看，没准在出发点就能找到问题了。 下面就将展示问题的描述和解决方案 问题描述 老师在使用MobaXterm软件时使用redis-server redis.conf命令指定配置文件在前台启动Redis 然而我在使用redis-server命令却出现了下面的两种情况： 第一种：不指定配置文件启动Redis 发现监听6379端口失败 第二种：指定配置文件启动Redis 这里要先cd到自己的Redis安装路径下，然后使用命令redis-server redis.conf 发现没有产生日志，这里使用ps -ef | grep redis查看后端端口占用情况 显然，后台端口已经被占用了，这里是因为Redis配置了开机自启动和后台启动，导致端口占用而无法使用redis-server命令 问题解决 在执行以下操作前建议使用VMware创建一个快照，防止后续操作不当导致虚拟机设置出现问题 先使用以下命令关闭后台端口 此时cd到Redis的安装目录下，使用命令ps -ef | grep redis查看后台端口占用情况，发现后台端口关闭 然后在当前路径下使用vi redis.conf关闭Redis后台启动 将 daemonize 修改为 no 这样Redis就不会在后台启动了！ 接着可以开心的使用redis-server redis.conf指定配置文件启动redis了！ 然而，如果你使用该命令后又出现了如下情况 发现当前窗口确实是像前台启动一样阻塞的，但是却没有任何日志信息输出，这时我们打开RESP等图形化界面看一下连接是否成功 嗯，确实成功了，说明Redis服务确实启动了，但还是没有日志文件，那么这是怎么回事呢？ 哈哈，不用着急，我们先使用ctrl + c停止当前端口，然后再回到RESP上发现连接终止，这就说明我们之前做的操作没有问题！现在我们回到终端，使用vi redis.conf命令进入到配置文件中 找到logfile，发现logfile后面的引号内为一个日志文件 这个时候一切都真相大白了，因为在一开始配置Redis时，我们将在logfile的引号内写入了一个日志文件名称（一开始的logfile内的引号默认是空的），这就导致了我们使用redis-server命令输出的日志都进入到这个日志文件中，所以启动时当然看不到任何信息了！现在我们将引号内的文件命删除，保存并退出 再次使用redis-server redis.conf命令启动Redis 启动成功，大功告成！"},{"title":"VMWare演示Redis持久化","date":"2022-10-28T12:49:50.000Z","url":"/2022/10/28/VMWare%E6%BC%94%E7%A4%BARedis%E6%8C%81%E4%B9%85%E5%8C%96/","tags":[["学习笔记","/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"]],"categories":[["Redis","/categories/Redis/"]],"content":"VMware演示Redis持久化 Redis持久化的作用 Redis是内存存储，如果出现服务器宕机、服务重启等情况可能会丢失数据，利用Redis持久化可以将数据写入磁盘中，这样Redis就可以利用持久化的文件进行数据恢复，数据安全性得以大大提升。 RDB RDB全称为Redis Database Backup file(Redis数据备份文件)，也被叫做Redis数据快照。简单来说就是把内存重的所有数据都记录道磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。 RDB实现数据持久化有两种命令：save和bgsave save命令适合用于准备将Redis停机的情况。 在执行save命令时会占用Redis的主进程，阻塞所有命令，因此不适用于Redis正在运行的情况。 而bgsave命令会执行fork操作开启一个子进程，避免主进程收到影响。 此外，当Redis停机时会执行一次RDB操作进行数据备份。 下面将对该情况进行验证 RDB演示 使用redis-server redis.conf指令在任意路径下运行Redis 接下来创建一个新的终端创建连接，存入键值对 接着回到上一个终端使用ctrl + c关闭Redis，可以看到DB saved on disk表示文件正常保存 在当前路径下使用ll查看所有文件，看到dump.rdb快照文件已经生成 接着再次在当前终端使用redis-server redis.conf启动Redis，此时数据会自动恢复，回到另一个终端窗口 此时将使用ctrl + c关闭当前命令行客户端，重新使用redis-cli开启一个新的命令行 这时使用get num命令发现数据返回为789，（如果get num返回错误Error：Server closed the connection要将redis.conf的protected-mode设置为No关闭保护模式）表明数据备份成功 Redis停机自动执行RDB证明完毕！ RDB相关配置 这里需要 cd 到redis的安装目录下，我的安装目录在 /usr/local/src/redis-6.2.6下 这里老师用的MobaXterm可以直接使用第三方文件打开配置文件进行修改，因为我用的是VMware所以只能使用Linux命令进行修改配置 使用 vi redis.conf修改配置文件 这里有一些小技巧： 按下INSERT键进入修改状态，此时才可以对文件进行修改 按下ESC键可以使用一些命令：/xxx可以查找跳跃到当前文件中的xxx名称，:wq保存当前文件并退出，:q不保存并退出当前文件，:q!不保存并强制退出当前文件 下面就可以对配置文件进行修改了！ 修改为五秒内执行一次操作就出发RDB备份 接着 再查找rdbcompression后修改为yes即可 使用:wq保存并退出 rdb文件名修改后使用终端再次运行Redis，发现此时没有DB文件录入消息，之前的dump.rdb文件已不能被读取 到另一个终端重新使用redis-cli开启命令行，使用get num命令发现结果为空，再set 一个新的键值，回到Redis窗口发现出现Background saving started字段即表示成功！ AOF AOF全称为Append Only File (追加文件)。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 开启AOF需要在Redis的配置文件中修改配置，AOF记录命令的频率有三种:always 、everysec和no always配置胜在能记录每一次Redis执行的写命令，几乎不会丢失数据，但对性能会有很大的影响 everysec配置每隔一秒将缓冲区里的数据写入到AOF文件，避免直接对AOF文件操作而性能上有了提升，但是在数据间隔的一秒内如果出现服务宕机的情况会丢失最多一秒的数据 no配置执行频率最低，性能最好，但是可靠性很低，不推荐使用 AOF演示 先使用 cd 命令进入到Redis的安装路径下 使用vi redis.conf命令修改redis配置文件 将之前的save 5 1注释掉，写入save \"\"，表示禁用RDB 将appendonly修改为yes，开启AOF AOF执行频率的命令配置默认为appendfsync everysec，不需要修改 修改完毕后回到终端，使用rm -rf *.rdb删除之前的RDB文件 可以看到当前目录下rdb文件已删除 使用redis-server redis.conf命令重启Redis 发现日志中出现没有对RDB的读取，修改成功 在另一终端打开命令行，输入如下指令 返回空集合，说明所有数据已被清除 img set一个键值对 这时我们打开文件夹进入本机的Redis的安装路径下，发现AOF文件已经被创建成功，打开AOF文件，set命令已经被写入成功，说明AOF已经生效了 接下来验证AOF的重启恢复，先重启Redis服务，红框内的日志表示数据已经从AOF文件加载完毕 这时候我们回到命令行使用keys *命令，返回num，依然有数据，说明AOF重启能保证数据的恢复，证明完毕 RDB和AOF的比较 "},{"title":"VMware软件安装及问题解决","date":"2022-10-08T03:26:00.000Z","url":"/2022/10/08/VMware%E5%AE%89%E8%A3%85/","tags":[["问题解决","/tags/%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"]],"categories":[["Linux","/categories/Linux/"]],"content":"VMware软件安装 前言：最近学习Redis时需要使用Linux系统，导致不得不去安装一个虚拟机了，在准备安装VMware软件时遇到了一些问题，这里给出记录和解决方案，以做参考学习使用 准备工作 本文将在Windows系统下使用VMware软件配置Linux虚拟机，下文将会详解VMware软件的安装步骤 VMware软件下载地址： Linux CentOS 7版本下载映像文件地址： 按图片所示下载即可 这里我选择的是CentOS-7.0-x86_64-DVD-2009.iso 版本 在下载路径下右键以管理员身份运行 在这里要敲重点！！！如果你没有以管理员身份运行这个选项，那么就看接下来的问题解决部分，如果你有这个选项请点击跳转自行无视。 问题解决 按住快捷键Win+R打开运行窗口，输入“regedit”， 这样就打开了注册表编辑器 在编辑器左侧依次找到HKEY_CURRENT_USER 然后将RestrictToPermittedSnapins的值设置为0 上面的方法如果输入路径后发现MMC不存在，那么下面这个方法就派上用场了！ 按下Win + R键打开运行，输入gpedit.msc打开组策略编辑器，这里可能你又会惊讶的发现，gpedit.msc打不开了！别着急，下面还有解决方案。当然，如果你能打开组策略编辑器的话可以点击这里继续往下看 对于无法打开组策略编辑器的情况，win + r 键打开运行后，输入notepad打开记事本，复制粘贴以下内容 记事本左上角文件另存为到任意路径下 文件名写为gpedit.cmd，保存类型选择所有文件，编码选择ANSI 点击保存后在你保存的路径下右键以管理员身份运行 等待运行结束后发现win + R键打开的gpedit.msc可以正常打开了 接下来进入组策略编辑器，双击计算机配置 --&gt; Windows设置 --&gt; 安全设置 双击本地策略 进入安全选项找到下面两项选择已启用 大功告成！！！ 安装VMware 当解决上述问题后开始准备下载VM软件了，右键以管理员身份运行后等待片刻来到安装界面 一路下一步来到这个界面，安装路径自己选择，我选择安装在D盘下的目录里 勾选取消“启动时检查产品更新” 一路下一步，点击安装 点击许可证 这里要输入许可证密钥（这里大家自行网络上获取），然后点击输入，安装结束，注意不要误点到跳过了。 输入密钥确认安装后就可以使用VMware了！"}]